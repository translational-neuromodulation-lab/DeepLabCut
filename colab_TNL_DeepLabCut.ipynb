{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"colab_TNL_DeepLabCut.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RK255E7YoEIt"},"source":["# DeepLabCut network training and analysis on Google Colab GPUs\n","\n","This notebook assumes you already have a project folder with labeled data in your Google Drive. To do so with easy-to-use GUI, install DLC on your local computer by downloading DLC-CPU.yaml from http://www.mousemotorlab.org/deeplabcut. Then in terminal, execute `conda env create -f DLC-CPU.yaml`. To launch GUI:\n","- activate environment with `conda activate DLC-CPU`\n","- start python GUI with `pythonw -m deeplabcut`\n","\n","Follow GUI steps through \"Label frames\". Ensure project folder is in Google Drive. Then close GUI and run this notebook. Also convert videos to .mp4 format (.mov don't seem to work with this notebook)."]},{"cell_type":"markdown","metadata":{"id":"txoddlM8hLKm"},"source":["### Verify use of Python3 on virtual machine\n","Go to \"Runtime\" ->\"change runtime type\"-> select \"GPU\". You should see in top right that you are connected to \"Python3 Google Compute Engine backend (GPU)\"\n"]},{"cell_type":"markdown","metadata":{"id":"J0TPz0SHF1U2"},"source":["### Install DLC on virtual machine:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"q23BzhA6CXxu","executionInfo":{"status":"ok","timestamp":1610047231322,"user_tz":300,"elapsed":45430,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"}},"outputId":"8b3bf414-d1cb-4ad9-95d0-4cab63773502"},"source":["#(this will take a few minutes to install all the dependences!)\n","!pip install deeplabcut"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting deeplabcut\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/8f/1d098b2e43dd73a24a83cb6bbc73711d46794eadbddf453bc16220cecfb9/deeplabcut-2.1.9-py3-none-any.whl (665kB)\n","\r\u001b[K     |▌                               | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 30.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 15.7MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 11.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 348kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 358kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 368kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 378kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 389kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 399kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 409kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 419kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 430kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 440kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 450kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 460kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 471kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 481kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 491kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 501kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 512kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 522kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 532kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 542kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 552kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 563kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 573kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 583kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 593kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 604kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 614kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 624kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 634kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 645kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 655kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 665kB 9.1MB/s \n","\u001b[?25hRequirement already satisfied: intel-openmp in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2021.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2.23.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.16.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (4.41.1)\n","Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.9)\n","Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.2.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (3.13)\n","Collecting matplotlib==3.1.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/07/4b361d6d0f4e08942575f83a11d33f36897e1aae4279046606dd1808778a/matplotlib-3.1.3-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n","\u001b[K     |████████████████████████████████| 13.1MB 240kB/s \n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (7.1.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.36.2)\n","Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.4.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (5.5.0)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (1.1.5)\n","Collecting ruamel.yaml>=0.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/39/186f14f3836ac5d2a6a042c8de69988770e8b9abb537610edc429e4914aa/ruamel.yaml-0.16.12-py2.py3-none-any.whl (111kB)\n","\u001b[K     |████████████████████████████████| 112kB 58.5MB/s \n","\u001b[?25hCollecting bayesian-optimization\n","  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.22.2.post1)\n","Collecting statsmodels>=0.11\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/4c/9e2435ca6645d6bafa2b51bb11f0a365b28934a2ffe9d6e339d67130926d/statsmodels-0.12.1-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\n","\u001b[K     |████████████████████████████████| 9.5MB 54.7MB/s \n","\u001b[?25hRequirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (3.4.4)\n","Requirement already satisfied: moviepy<=1.0.1 in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.2.3.5)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2020.12.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.2.0)\n","Collecting numpy==1.16.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n","\u001b[K     |████████████████████████████████| 17.3MB 143kB/s \n","\u001b[?25hRequirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.5.1)\n","Collecting opencv-python-headless\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/fc/4da675cc522a749ebbcf85c5a63fba844b2d44c87e6f24e3fdb147df3270/opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6MB)\n","\u001b[K     |████████████████████████████████| 37.6MB 63kB/s \n","\u001b[?25hCollecting filterpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/ac8914360460fafa1990890259b7fa5ef7ba4cd59014e782e4ab3ab144d8/filterpy-1.4.5.zip (177kB)\n","\u001b[K     |████████████████████████████████| 184kB 55.9MB/s \n","\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (0.29.21)\n","Collecting numba==0.51.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/98/862d341b3c97c7fb028888f25d574f33a2550b82bde9859562cc840a22d6/numba-0.51.1-cp36-cp36m-manylinux2014_x86_64.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 48.9MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (51.1.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2.8.1)\n","Collecting tensorpack==0.9.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/cb/62dc9115722a0b4fbeca6275ffbe47118149171ffafa7d1db6e295453aae/tensorpack-0.9.8-py2.py3-none-any.whl (288kB)\n","\u001b[K     |████████████████████████████████| 296kB 57.7MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (2.10.0)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from deeplabcut) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deeplabcut) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deeplabcut) (2.10)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->deeplabcut) (2.4.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->deeplabcut) (1.1.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->deeplabcut) (2.5)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->deeplabcut) (7.0.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug->deeplabcut) (4.1.2.30)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug->deeplabcut) (1.7.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.3->deeplabcut) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.3->deeplabcut) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.3->deeplabcut) (0.10.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (0.7.5)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (0.8.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (1.0.18)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (4.4.2)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (4.3.3)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (2.6.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->deeplabcut) (4.8.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->deeplabcut) (2018.9)\n","Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/ff/ec25dc01ef04232a9e68ff18492e37dfa01f1f58172e702ad4f38536d41b/ruamel.yaml.clib-0.2.2-cp36-cp36m-manylinux1_x86_64.whl (549kB)\n","\u001b[K     |████████████████████████████████| 552kB 48.0MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->deeplabcut) (1.0.0)\n","Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->deeplabcut) (2.7.1)\n","Collecting llvmlite<0.35,>=0.34.0.dev0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/b7/8a91b513f165e0affdeb975c1fef307c39d1051ce71e8aec1da9dcb317ad/llvmlite-0.34.0-cp36-cp36m-manylinux2010_x86_64.whl (24.6MB)\n","\u001b[K     |████████████████████████████████| 24.6MB 125kB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->deeplabcut) (1.1.0)\n","Collecting msgpack-numpy>=0.4.4.2\n","  Downloading https://files.pythonhosted.org/packages/19/05/05b8d7c69c6abb36a34325cc3150089bdafc359f0a81fb998d93c5d5c737/msgpack_numpy-0.4.7.1-py2.py3-none-any.whl\n","Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->deeplabcut) (5.4.8)\n","Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->deeplabcut) (1.0.1)\n","Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->deeplabcut) (20.0.0)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->deeplabcut) (0.8.7)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deeplabcut) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->deeplabcut) (0.6.0)\n","Building wheels for collected packages: bayesian-optimization, filterpy\n","  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp36-none-any.whl size=11685 sha256=b8057d158f469fe89015f17da836bd0356ccdbfac01b6fb9bdd112840eb948b8\n","  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n","  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for filterpy: filename=filterpy-1.4.5-cp36-none-any.whl size=110453 sha256=e6745e1a74f0552b535e4a8b35600089e9515afd21e422c8e2d4fd9e65d285a7\n","  Stored in directory: /root/.cache/pip/wheels/c3/0c/dd/e92392c3f38a41371602d99fc77d6c1d42aadbf0c6afccdd02\n","Successfully built bayesian-optimization filterpy\n","\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.0 has requirement numpy~=1.19.2, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, matplotlib, ruamel.yaml.clib, ruamel.yaml, bayesian-optimization, statsmodels, opencv-python-headless, filterpy, llvmlite, numba, msgpack-numpy, tensorpack, deeplabcut\n","  Found existing installation: numpy 1.19.4\n","    Uninstalling numpy-1.19.4:\n","      Successfully uninstalled numpy-1.19.4\n","  Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","  Found existing installation: statsmodels 0.10.2\n","    Uninstalling statsmodels-0.10.2:\n","      Successfully uninstalled statsmodels-0.10.2\n","  Found existing installation: llvmlite 0.31.0\n","    Uninstalling llvmlite-0.31.0:\n","      Successfully uninstalled llvmlite-0.31.0\n","  Found existing installation: numba 0.48.0\n","    Uninstalling numba-0.48.0:\n","      Successfully uninstalled numba-0.48.0\n","Successfully installed bayesian-optimization-1.2.0 deeplabcut-2.1.9 filterpy-1.4.5 llvmlite-0.34.0 matplotlib-3.1.3 msgpack-numpy-0.4.7.1 numba-0.51.1 numpy-1.16.4 opencv-python-headless-4.5.1.48 ruamel.yaml-0.16.12 ruamel.yaml.clib-0.2.2 statsmodels-0.12.1 tensorpack-0.9.8\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits","numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"25wSj6TlVclR"},"source":["**(Be sure to click \"RESTART RUNTIME\" is it is displayed above above before moving on !)**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y36K4Eux3h-X","executionInfo":{"status":"ok","timestamp":1610047237145,"user_tz":300,"elapsed":563,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"}},"outputId":"b4bc3e64-d89e-43a8-822f-59708fee7e0e"},"source":["# Use TensorFlow 1.x:\n","%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cQ-nlTkri4HZ"},"source":["### Link your Google Drive:\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KS4Q4UkR9rgG","executionInfo":{"status":"ok","timestamp":1610047260424,"user_tz":300,"elapsed":20014,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"}},"outputId":"a6c4ee57-2867-4d2e-d498-648af9c7c394"},"source":["#Now, let's link to your Google Drive. Run this cell and follow the authorization instructions:\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vhENAlQnFENJ","executionInfo":{"status":"ok","timestamp":1610047265669,"user_tz":300,"elapsed":333,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"}},"outputId":"1e218a9e-e587-44f1-b01b-a750571df823"},"source":["#Setup your project variables (be sure project direction is in Google Drive and path is correct in config.yaml):\n","# PLEASE EDIT THESE:\n","  \n","ProjectFolderName = 'wirelessStim/wirelessStim-Drew-2021-01-01'\n","VideoType = 'mp4' \n","\n","#don't edit these:\n","videofile_path = ['/content/drive/My Drive/'+ProjectFolderName+'/videos/'] #Enter the list of videos or folder to analyze.\n","videofile_path"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"sXufoX6INe6w","executionInfo":{"status":"ok","timestamp":1610047272021,"user_tz":300,"elapsed":340,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"}}},"source":["#GUIs don't work on the cloud, so label your data locally on your computer! This will suppress the GUI support\n","import os\n","os.environ[\"DLClight\"]=\"True\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3K9Ndy1beyfG","executionInfo":{"status":"ok","timestamp":1610047290728,"user_tz":300,"elapsed":9408,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"}},"outputId":"aee28ef5-062c-4a3b-f7b6-11e17eaeeeba"},"source":["import deeplabcut"],"execution_count":5,"outputs":[{"output_type":"stream","text":["DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"o4orkg9QTHKK","executionInfo":{"elapsed":1079,"status":"ok","timestamp":1609768154070,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"},"user_tz":300},"outputId":"5dc4eb3b-f62b-4748-ab55-61fa1dccfab2"},"source":["deeplabcut.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.1.9'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Z7ZlDr3wV4D1","executionInfo":{"elapsed":527,"status":"ok","timestamp":1609768169136,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"},"user_tz":300},"outputId":"aa73157d-32bd-4d11-afd6-c0d4d2a74a33"},"source":["#This creates a path variable that links to your google drive copy\n","#No need to edit this, as you set it up before: \n","path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n","path_config_file"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/config.yaml'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"xNi9s1dboEJN"},"source":["### Create a training dataset:\n","You must do this step inside of Colab, not in GUI on local computer. After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'** This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n","**But if re-starting training, skip this step!**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eMeUwgxPoEJP","scrolled":true,"executionInfo":{"elapsed":48724,"status":"ok","timestamp":1609551306970,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"},"user_tz":300},"outputId":"e53436e2-8daf-48ea-acb6-a42b5bd97dae"},"source":["# Note: if re-running this on project, first delete the folder called dlc-models.\n","# There are many more functions you can set here, including which network to use!\n","# For function details, see https://github.com/DeepLabCut/DeepLabCut/blob/master/docs/functionDetails.md\n","deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n","The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[(0.95,\n","  1,\n","  (array([92, 49,  5, 90,  3, 10, 36, 22, 68, 30, 13, 29, 74, 45, 55, 50, 17,\n","          70, 21, 73,  1, 19, 18,  6, 93, 76, 44, 39, 25, 71, 15,  8, 99, 37,\n","          69, 65, 16,  0, 88, 43, 63, 24, 42, 82, 96, 85, 59,  4, 84,  9, 12,\n","          28, 35, 54, 62, 57, 31, 64, 97, 79, 95, 78, 34, 58, 94, 66, 72, 83,\n","          51, 14, 86, 26, 53,  7, 11, 23, 46, 56, 98, 33, 47, 41, 20, 60, 32,\n","          40, 27, 67, 75, 77, 87, 81, 61, 52, 89]),\n","   array([48, 38, 80, 91,  2])))]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"c4FczXGDoEJU"},"source":["## Start training:\n","This function trains the network for a specific shuffle of the training dataset. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pOvDq_2oEJW","outputId":"ee3e42c6-15b7-4fbb-e65a-73c06530021f"},"source":["# Typically, you want to train to 200,000 + iterations.\n","# Again there are more options to set here if desired. Check docstring.\n","deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=500)\n","\n","# This will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations). "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Selecting single-animal trainer\n"],"name":"stdout"},{"output_type":"stream","text":["Config:\n","{'all_joints': [[0], [1], [2], [3]],\n"," 'all_joints_names': ['nose', 'leftear', 'rightear', 'tailbase'],\n"," 'batch_size': 1,\n"," 'crop_pad': 0,\n"," 'cropratio': 0.4,\n"," 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_wirelessStimJan1/wirelessStim_Drew95shuffle1.mat',\n"," 'dataset_type': 'imgaug',\n"," 'deterministic': False,\n"," 'display_iters': 1000,\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 0.8,\n"," 'init_weights': '/content/drive/My '\n","                 'Drive/wirelessStim/wirelessStim-Drew-2021-01-01/dlc-models/iteration-0/wirelessStimJan1-trainset95shuffle1/train/snapshot-109000',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 0.05,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'max_input_size': 1500,\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_wirelessStimJan1/Documentation_data-wirelessStim_95shuffle1.pickle',\n"," 'min_input_size': 64,\n"," 'mirror': False,\n"," 'multi_step': [[0.005, 10000],\n","                [0.02, 430000],\n","                [0.002, 730000],\n","                [0.001, 1030000]],\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 4,\n"," 'optimizer': 'sgd',\n"," 'pairwise_huber_loss': False,\n"," 'pairwise_predict': False,\n"," 'partaffinityfield_predict': False,\n"," 'pos_dist_thresh': 17,\n"," 'project_path': '/content/drive/My '\n","                 'Drive/wirelessStim/wirelessStim-Drew-2021-01-01',\n"," 'regularize': False,\n"," 'rotation': 25,\n"," 'rotratio': 0.4,\n"," 'save_iters': 50000,\n"," 'scale_jitter_lo': 0.5,\n"," 'scale_jitter_up': 1.25,\n"," 'scoremap_dir': 'test',\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/My '\n","                    'Drive/wirelessStim/wirelessStim-Drew-2021-01-01/dlc-models/iteration-0/wirelessStimJan1-trainset95shuffle1/train/snapshot',\n"," 'stride': 8.0,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n"],"name":"stderr"},{"output_type":"stream","text":["Starting with imgaug pose-dataset loader (=default).\n","Batch Size is 1\n","Initializing ResNet\n","Loading already trained DLC with backbone: resnet_50\n","Display_iters overwritten as 10\n","Save_iters overwritten as 500\n","Training parameter:\n","{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/dlc-models/iteration-0/wirelessStimJan1-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['nose', 'leftear', 'rightear', 'tailbase'], 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_wirelessStimJan1/wirelessStim_Drew95shuffle1.mat', 'display_iters': 1000, 'init_weights': '/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/dlc-models/iteration-0/wirelessStimJan1-trainset95shuffle1/train/snapshot-109000', 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_wirelessStimJan1/Documentation_data-wirelessStim_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': [-90, 90]}}\n","Starting training....\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","iteration: 210 loss: 0.0011 lr: 0.005\n","iteration: 220 loss: 0.0010 lr: 0.005\n","iteration: 230 loss: 0.0012 lr: 0.005\n","iteration: 240 loss: 0.0012 lr: 0.005\n","iteration: 250 loss: 0.0013 lr: 0.005\n","iteration: 260 loss: 0.0013 lr: 0.005\n","iteration: 270 loss: 0.0012 lr: 0.005\n","iteration: 280 loss: 0.0012 lr: 0.005\n","iteration: 290 loss: 0.0013 lr: 0.005\n","iteration: 300 loss: 0.0012 lr: 0.005\n","iteration: 310 loss: 0.0011 lr: 0.005\n","iteration: 320 loss: 0.0013 lr: 0.005\n","iteration: 330 loss: 0.0010 lr: 0.005\n","iteration: 340 loss: 0.0011 lr: 0.005\n","iteration: 350 loss: 0.0015 lr: 0.005\n","iteration: 360 loss: 0.0010 lr: 0.005\n","iteration: 370 loss: 0.0011 lr: 0.005\n","iteration: 380 loss: 0.0011 lr: 0.005\n","iteration: 390 loss: 0.0015 lr: 0.005\n","iteration: 400 loss: 0.0010 lr: 0.005\n","iteration: 410 loss: 0.0011 lr: 0.005\n","iteration: 420 loss: 0.0011 lr: 0.005\n","iteration: 430 loss: 0.0008 lr: 0.005\n","iteration: 440 loss: 0.0011 lr: 0.005\n","iteration: 450 loss: 0.0011 lr: 0.005\n","iteration: 460 loss: 0.0011 lr: 0.005\n","iteration: 470 loss: 0.0012 lr: 0.005\n","iteration: 480 loss: 0.0010 lr: 0.005\n","iteration: 490 loss: 0.0009 lr: 0.005\n","iteration: 500 loss: 0.0014 lr: 0.005\n","iteration: 510 loss: 0.0011 lr: 0.005\n","iteration: 520 loss: 0.0009 lr: 0.005\n","iteration: 530 loss: 0.0011 lr: 0.005\n","iteration: 540 loss: 0.0009 lr: 0.005\n","iteration: 550 loss: 0.0013 lr: 0.005\n","iteration: 560 loss: 0.0011 lr: 0.005\n","iteration: 570 loss: 0.0010 lr: 0.005\n","iteration: 580 loss: 0.0012 lr: 0.005\n","iteration: 590 loss: 0.0010 lr: 0.005\n","iteration: 600 loss: 0.0009 lr: 0.005\n","iteration: 610 loss: 0.0012 lr: 0.005\n","iteration: 620 loss: 0.0012 lr: 0.005\n","iteration: 630 loss: 0.0010 lr: 0.005\n","iteration: 640 loss: 0.0012 lr: 0.005\n","iteration: 650 loss: 0.0012 lr: 0.005\n","iteration: 660 loss: 0.0014 lr: 0.005\n","iteration: 670 loss: 0.0011 lr: 0.005\n","iteration: 680 loss: 0.0011 lr: 0.005\n","iteration: 690 loss: 0.0011 lr: 0.005\n","iteration: 700 loss: 0.0010 lr: 0.005\n","iteration: 710 loss: 0.0011 lr: 0.005\n","iteration: 720 loss: 0.0010 lr: 0.005\n","iteration: 730 loss: 0.0010 lr: 0.005\n","iteration: 740 loss: 0.0010 lr: 0.005\n","iteration: 750 loss: 0.0011 lr: 0.005\n","iteration: 760 loss: 0.0012 lr: 0.005\n","iteration: 770 loss: 0.0011 lr: 0.005\n","iteration: 780 loss: 0.0012 lr: 0.005\n","iteration: 790 loss: 0.0011 lr: 0.005\n","iteration: 800 loss: 0.0011 lr: 0.005\n","iteration: 810 loss: 0.0010 lr: 0.005\n","iteration: 820 loss: 0.0011 lr: 0.005\n","iteration: 830 loss: 0.0010 lr: 0.005\n","iteration: 840 loss: 0.0012 lr: 0.005\n","iteration: 850 loss: 0.0009 lr: 0.005\n","iteration: 860 loss: 0.0010 lr: 0.005\n","iteration: 870 loss: 0.0010 lr: 0.005\n","iteration: 880 loss: 0.0011 lr: 0.005\n","iteration: 890 loss: 0.0013 lr: 0.005\n","iteration: 900 loss: 0.0011 lr: 0.005\n","iteration: 910 loss: 0.0011 lr: 0.005\n","iteration: 920 loss: 0.0012 lr: 0.005\n","iteration: 930 loss: 0.0012 lr: 0.005\n","iteration: 940 loss: 0.0012 lr: 0.005\n","iteration: 950 loss: 0.0010 lr: 0.005\n","iteration: 960 loss: 0.0010 lr: 0.005\n","iteration: 970 loss: 0.0012 lr: 0.005\n","iteration: 980 loss: 0.0011 lr: 0.005\n","iteration: 990 loss: 0.0013 lr: 0.005\n","iteration: 1000 loss: 0.0011 lr: 0.005\n","iteration: 1010 loss: 0.0010 lr: 0.005\n","iteration: 1020 loss: 0.0011 lr: 0.005\n","iteration: 1030 loss: 0.0010 lr: 0.005\n","iteration: 1040 loss: 0.0011 lr: 0.005\n","iteration: 1050 loss: 0.0016 lr: 0.005\n","iteration: 1060 loss: 0.0012 lr: 0.005\n","iteration: 1070 loss: 0.0019 lr: 0.005\n","iteration: 1080 loss: 0.0010 lr: 0.005\n","iteration: 1090 loss: 0.0010 lr: 0.005\n","iteration: 1100 loss: 0.0009 lr: 0.005\n","iteration: 1110 loss: 0.0013 lr: 0.005\n","iteration: 1120 loss: 0.0010 lr: 0.005\n","iteration: 1130 loss: 0.0009 lr: 0.005\n","iteration: 1140 loss: 0.0010 lr: 0.005\n","iteration: 1150 loss: 0.0013 lr: 0.005\n","iteration: 1160 loss: 0.0009 lr: 0.005\n","iteration: 1170 loss: 0.0009 lr: 0.005\n","iteration: 1180 loss: 0.0011 lr: 0.005\n","iteration: 1190 loss: 0.0012 lr: 0.005\n","iteration: 1200 loss: 0.0010 lr: 0.005\n","iteration: 1210 loss: 0.0009 lr: 0.005\n","iteration: 1220 loss: 0.0009 lr: 0.005\n","iteration: 1230 loss: 0.0010 lr: 0.005\n","iteration: 1240 loss: 0.0010 lr: 0.005\n","iteration: 1250 loss: 0.0012 lr: 0.005\n","iteration: 1260 loss: 0.0009 lr: 0.005\n","iteration: 1270 loss: 0.0011 lr: 0.005\n","iteration: 1280 loss: 0.0011 lr: 0.005\n","iteration: 1290 loss: 0.0012 lr: 0.005\n","iteration: 1300 loss: 0.0009 lr: 0.005\n","iteration: 1310 loss: 0.0011 lr: 0.005\n","iteration: 1320 loss: 0.0014 lr: 0.005\n","iteration: 1330 loss: 0.0013 lr: 0.005\n","iteration: 1340 loss: 0.0010 lr: 0.005\n","iteration: 1350 loss: 0.0011 lr: 0.005\n","iteration: 1360 loss: 0.0011 lr: 0.005\n","iteration: 1370 loss: 0.0013 lr: 0.005\n","iteration: 1380 loss: 0.0011 lr: 0.005\n","iteration: 1390 loss: 0.0011 lr: 0.005\n","iteration: 1400 loss: 0.0012 lr: 0.005\n","iteration: 1410 loss: 0.0013 lr: 0.005\n","iteration: 1420 loss: 0.0012 lr: 0.005\n","iteration: 1430 loss: 0.0008 lr: 0.005\n","iteration: 1440 loss: 0.0016 lr: 0.005\n","iteration: 1450 loss: 0.0010 lr: 0.005\n","iteration: 1460 loss: 0.0011 lr: 0.005\n","iteration: 1470 loss: 0.0009 lr: 0.005\n","iteration: 1480 loss: 0.0011 lr: 0.005\n","iteration: 1490 loss: 0.0013 lr: 0.005\n","iteration: 1500 loss: 0.0012 lr: 0.005\n","iteration: 1510 loss: 0.0011 lr: 0.005\n","iteration: 1520 loss: 0.0011 lr: 0.005\n","iteration: 1530 loss: 0.0009 lr: 0.005\n","iteration: 1540 loss: 0.0013 lr: 0.005\n","iteration: 1550 loss: 0.0012 lr: 0.005\n","iteration: 1560 loss: 0.0012 lr: 0.005\n","iteration: 1570 loss: 0.0011 lr: 0.005\n","iteration: 1580 loss: 0.0010 lr: 0.005\n","iteration: 1590 loss: 0.0011 lr: 0.005\n","iteration: 1600 loss: 0.0009 lr: 0.005\n","iteration: 1610 loss: 0.0012 lr: 0.005\n","iteration: 1620 loss: 0.0012 lr: 0.005\n","iteration: 1630 loss: 0.0012 lr: 0.005\n","iteration: 1640 loss: 0.0011 lr: 0.005\n","iteration: 1650 loss: 0.0008 lr: 0.005\n","iteration: 1660 loss: 0.0010 lr: 0.005\n","iteration: 1670 loss: 0.0010 lr: 0.005\n","iteration: 1680 loss: 0.0012 lr: 0.005\n","iteration: 1690 loss: 0.0008 lr: 0.005\n","iteration: 1700 loss: 0.0012 lr: 0.005\n","iteration: 1710 loss: 0.0011 lr: 0.005\n","iteration: 1720 loss: 0.0012 lr: 0.005\n","iteration: 1730 loss: 0.0010 lr: 0.005\n","iteration: 1740 loss: 0.0012 lr: 0.005\n","iteration: 1750 loss: 0.0009 lr: 0.005\n","iteration: 1760 loss: 0.0011 lr: 0.005\n","iteration: 1770 loss: 0.0010 lr: 0.005\n","iteration: 1780 loss: 0.0011 lr: 0.005\n","iteration: 1790 loss: 0.0009 lr: 0.005\n","iteration: 1800 loss: 0.0010 lr: 0.005\n","iteration: 1810 loss: 0.0014 lr: 0.005\n","iteration: 1820 loss: 0.0009 lr: 0.005\n","iteration: 1830 loss: 0.0016 lr: 0.005\n","iteration: 1840 loss: 0.0010 lr: 0.005\n","iteration: 1850 loss: 0.0010 lr: 0.005\n","iteration: 1860 loss: 0.0012 lr: 0.005\n","iteration: 1870 loss: 0.0010 lr: 0.005\n","iteration: 1880 loss: 0.0010 lr: 0.005\n","iteration: 1890 loss: 0.0011 lr: 0.005\n","iteration: 1900 loss: 0.0009 lr: 0.005\n","iteration: 1910 loss: 0.0012 lr: 0.005\n","iteration: 1920 loss: 0.0011 lr: 0.005\n","iteration: 1930 loss: 0.0014 lr: 0.005\n","iteration: 1940 loss: 0.0011 lr: 0.005\n","iteration: 1950 loss: 0.0009 lr: 0.005\n","iteration: 1960 loss: 0.0009 lr: 0.005\n","iteration: 1970 loss: 0.0010 lr: 0.005\n","iteration: 1980 loss: 0.0013 lr: 0.005\n","iteration: 1990 loss: 0.0008 lr: 0.005\n","iteration: 2000 loss: 0.0009 lr: 0.005\n","iteration: 2010 loss: 0.0010 lr: 0.005\n","iteration: 2020 loss: 0.0013 lr: 0.005\n","iteration: 2030 loss: 0.0013 lr: 0.005\n","iteration: 2040 loss: 0.0009 lr: 0.005\n","iteration: 2050 loss: 0.0010 lr: 0.005\n","iteration: 2060 loss: 0.0011 lr: 0.005\n","iteration: 2070 loss: 0.0010 lr: 0.005\n","iteration: 2080 loss: 0.0010 lr: 0.005\n","iteration: 2090 loss: 0.0013 lr: 0.005\n","iteration: 2100 loss: 0.0013 lr: 0.005\n","iteration: 2110 loss: 0.0011 lr: 0.005\n","iteration: 2120 loss: 0.0008 lr: 0.005\n","iteration: 2130 loss: 0.0011 lr: 0.005\n","iteration: 2140 loss: 0.0010 lr: 0.005\n","iteration: 2150 loss: 0.0012 lr: 0.005\n","iteration: 2160 loss: 0.0011 lr: 0.005\n","iteration: 2170 loss: 0.0012 lr: 0.005\n","iteration: 2180 loss: 0.0012 lr: 0.005\n","iteration: 2190 loss: 0.0010 lr: 0.005\n","iteration: 2200 loss: 0.0012 lr: 0.005\n","iteration: 2210 loss: 0.0009 lr: 0.005\n","iteration: 2220 loss: 0.0015 lr: 0.005\n","iteration: 2230 loss: 0.0010 lr: 0.005\n","iteration: 2240 loss: 0.0009 lr: 0.005\n","iteration: 2250 loss: 0.0011 lr: 0.005\n","iteration: 2260 loss: 0.0012 lr: 0.005\n","iteration: 2270 loss: 0.0008 lr: 0.005\n","iteration: 2280 loss: 0.0010 lr: 0.005\n","iteration: 2290 loss: 0.0010 lr: 0.005\n","iteration: 2300 loss: 0.0010 lr: 0.005\n","iteration: 2310 loss: 0.0010 lr: 0.005\n","iteration: 2320 loss: 0.0010 lr: 0.005\n","iteration: 2330 loss: 0.0012 lr: 0.005\n","iteration: 2340 loss: 0.0011 lr: 0.005\n","iteration: 2350 loss: 0.0012 lr: 0.005\n","iteration: 2360 loss: 0.0011 lr: 0.005\n","iteration: 2370 loss: 0.0010 lr: 0.005\n","iteration: 2380 loss: 0.0012 lr: 0.005\n","iteration: 2390 loss: 0.0009 lr: 0.005\n","iteration: 2400 loss: 0.0012 lr: 0.005\n","iteration: 2410 loss: 0.0011 lr: 0.005\n","iteration: 2420 loss: 0.0010 lr: 0.005\n","iteration: 2430 loss: 0.0010 lr: 0.005\n","iteration: 2440 loss: 0.0011 lr: 0.005\n","iteration: 2450 loss: 0.0010 lr: 0.005\n","iteration: 2460 loss: 0.0011 lr: 0.005\n","iteration: 2470 loss: 0.0012 lr: 0.005\n","iteration: 2480 loss: 0.0010 lr: 0.005\n","iteration: 2490 loss: 0.0011 lr: 0.005\n","iteration: 2500 loss: 0.0012 lr: 0.005\n","iteration: 2510 loss: 0.0011 lr: 0.005\n","iteration: 2520 loss: 0.0011 lr: 0.005\n","iteration: 2530 loss: 0.0012 lr: 0.005\n","iteration: 2540 loss: 0.0011 lr: 0.005\n","iteration: 2550 loss: 0.0009 lr: 0.005\n","iteration: 2560 loss: 0.0012 lr: 0.005\n","iteration: 2570 loss: 0.0011 lr: 0.005\n","iteration: 2580 loss: 0.0009 lr: 0.005\n","iteration: 2590 loss: 0.0011 lr: 0.005\n","iteration: 2600 loss: 0.0008 lr: 0.005\n","iteration: 2610 loss: 0.0010 lr: 0.005\n","iteration: 2620 loss: 0.0010 lr: 0.005\n","iteration: 2630 loss: 0.0009 lr: 0.005\n","iteration: 2640 loss: 0.0010 lr: 0.005\n","iteration: 2650 loss: 0.0009 lr: 0.005\n","iteration: 2660 loss: 0.0014 lr: 0.005\n","iteration: 2670 loss: 0.0008 lr: 0.005\n","iteration: 2680 loss: 0.0013 lr: 0.005\n","iteration: 2690 loss: 0.0010 lr: 0.005\n","iteration: 2700 loss: 0.0009 lr: 0.005\n","iteration: 2710 loss: 0.0012 lr: 0.005\n","iteration: 2720 loss: 0.0009 lr: 0.005\n","iteration: 2730 loss: 0.0010 lr: 0.005\n","iteration: 2740 loss: 0.0010 lr: 0.005\n","iteration: 2750 loss: 0.0012 lr: 0.005\n","iteration: 2760 loss: 0.0009 lr: 0.005\n","iteration: 2770 loss: 0.0013 lr: 0.005\n","iteration: 2780 loss: 0.0011 lr: 0.005\n","iteration: 2790 loss: 0.0011 lr: 0.005\n","iteration: 2800 loss: 0.0010 lr: 0.005\n","iteration: 2810 loss: 0.0011 lr: 0.005\n","iteration: 2820 loss: 0.0011 lr: 0.005\n","iteration: 2830 loss: 0.0010 lr: 0.005\n","iteration: 2840 loss: 0.0010 lr: 0.005\n","iteration: 2850 loss: 0.0010 lr: 0.005\n","iteration: 2860 loss: 0.0009 lr: 0.005\n","iteration: 2870 loss: 0.0010 lr: 0.005\n","iteration: 2880 loss: 0.0009 lr: 0.005\n","iteration: 2890 loss: 0.0011 lr: 0.005\n","iteration: 2900 loss: 0.0009 lr: 0.005\n","iteration: 2910 loss: 0.0016 lr: 0.005\n","iteration: 2920 loss: 0.0011 lr: 0.005\n","iteration: 2930 loss: 0.0011 lr: 0.005\n","iteration: 2940 loss: 0.0011 lr: 0.005\n","iteration: 2950 loss: 0.0011 lr: 0.005\n","iteration: 2960 loss: 0.0012 lr: 0.005\n","iteration: 2970 loss: 0.0009 lr: 0.005\n","iteration: 2980 loss: 0.0011 lr: 0.005\n","iteration: 2990 loss: 0.0010 lr: 0.005\n","iteration: 3000 loss: 0.0010 lr: 0.005\n","iteration: 3010 loss: 0.0010 lr: 0.005\n","iteration: 3020 loss: 0.0011 lr: 0.005\n","iteration: 3030 loss: 0.0010 lr: 0.005\n","iteration: 3040 loss: 0.0009 lr: 0.005\n","iteration: 3050 loss: 0.0011 lr: 0.005\n","iteration: 3060 loss: 0.0012 lr: 0.005\n","iteration: 3070 loss: 0.0010 lr: 0.005\n","iteration: 3080 loss: 0.0009 lr: 0.005\n","iteration: 3090 loss: 0.0010 lr: 0.005\n","iteration: 3100 loss: 0.0009 lr: 0.005\n","iteration: 3110 loss: 0.0010 lr: 0.005\n","iteration: 3120 loss: 0.0012 lr: 0.005\n","iteration: 3130 loss: 0.0011 lr: 0.005\n","iteration: 3140 loss: 0.0011 lr: 0.005\n","iteration: 3150 loss: 0.0009 lr: 0.005\n","iteration: 3160 loss: 0.0013 lr: 0.005\n","iteration: 3170 loss: 0.0013 lr: 0.005\n","iteration: 3180 loss: 0.0008 lr: 0.005\n","iteration: 3190 loss: 0.0010 lr: 0.005\n","iteration: 3200 loss: 0.0010 lr: 0.005\n","iteration: 3210 loss: 0.0012 lr: 0.005\n","iteration: 3220 loss: 0.0010 lr: 0.005\n","iteration: 3230 loss: 0.0009 lr: 0.005\n","iteration: 3240 loss: 0.0012 lr: 0.005\n","iteration: 3250 loss: 0.0010 lr: 0.005\n","iteration: 3260 loss: 0.0010 lr: 0.005\n","iteration: 3270 loss: 0.0010 lr: 0.005\n","iteration: 3280 loss: 0.0009 lr: 0.005\n","iteration: 3290 loss: 0.0013 lr: 0.005\n","iteration: 3300 loss: 0.0010 lr: 0.005\n","iteration: 3310 loss: 0.0010 lr: 0.005\n","iteration: 3320 loss: 0.0011 lr: 0.005\n","iteration: 3330 loss: 0.0010 lr: 0.005\n","iteration: 3340 loss: 0.0012 lr: 0.005\n","iteration: 3350 loss: 0.0009 lr: 0.005\n","iteration: 3360 loss: 0.0008 lr: 0.005\n","iteration: 3370 loss: 0.0012 lr: 0.005\n","iteration: 3380 loss: 0.0012 lr: 0.005\n","iteration: 3390 loss: 0.0010 lr: 0.005\n","iteration: 3400 loss: 0.0010 lr: 0.005\n","iteration: 3410 loss: 0.0009 lr: 0.005\n","iteration: 3420 loss: 0.0011 lr: 0.005\n","iteration: 3430 loss: 0.0011 lr: 0.005\n","iteration: 3440 loss: 0.0010 lr: 0.005\n","iteration: 3450 loss: 0.0010 lr: 0.005\n","iteration: 3460 loss: 0.0012 lr: 0.005\n","iteration: 3470 loss: 0.0012 lr: 0.005\n","iteration: 3480 loss: 0.0009 lr: 0.005\n","iteration: 3490 loss: 0.0014 lr: 0.005\n","iteration: 3500 loss: 0.0011 lr: 0.005\n","iteration: 3510 loss: 0.0010 lr: 0.005\n","iteration: 3520 loss: 0.0010 lr: 0.005\n","iteration: 3530 loss: 0.0010 lr: 0.005\n","iteration: 3540 loss: 0.0010 lr: 0.005\n","iteration: 3550 loss: 0.0012 lr: 0.005\n","iteration: 3560 loss: 0.0014 lr: 0.005\n","iteration: 3570 loss: 0.0008 lr: 0.005\n","iteration: 3580 loss: 0.0009 lr: 0.005\n","iteration: 3590 loss: 0.0010 lr: 0.005\n","iteration: 3600 loss: 0.0009 lr: 0.005\n","iteration: 3610 loss: 0.0010 lr: 0.005\n","iteration: 3620 loss: 0.0012 lr: 0.005\n","iteration: 3630 loss: 0.0009 lr: 0.005\n","iteration: 3640 loss: 0.0010 lr: 0.005\n","iteration: 3650 loss: 0.0010 lr: 0.005\n","iteration: 3660 loss: 0.0009 lr: 0.005\n","iteration: 3670 loss: 0.0011 lr: 0.005\n","iteration: 3680 loss: 0.0009 lr: 0.005\n","iteration: 3690 loss: 0.0010 lr: 0.005\n","iteration: 3700 loss: 0.0011 lr: 0.005\n","iteration: 3710 loss: 0.0011 lr: 0.005\n","iteration: 3720 loss: 0.0010 lr: 0.005\n","iteration: 3730 loss: 0.0010 lr: 0.005\n","iteration: 3740 loss: 0.0013 lr: 0.005\n","iteration: 3750 loss: 0.0010 lr: 0.005\n","iteration: 3760 loss: 0.0012 lr: 0.005\n","iteration: 3770 loss: 0.0009 lr: 0.005\n","iteration: 3780 loss: 0.0010 lr: 0.005\n","iteration: 3790 loss: 0.0008 lr: 0.005\n","iteration: 3800 loss: 0.0009 lr: 0.005\n","iteration: 3810 loss: 0.0012 lr: 0.005\n","iteration: 3820 loss: 0.0009 lr: 0.005\n","iteration: 3830 loss: 0.0013 lr: 0.005\n","iteration: 3840 loss: 0.0012 lr: 0.005\n","iteration: 3850 loss: 0.0010 lr: 0.005\n","iteration: 3860 loss: 0.0012 lr: 0.005\n","iteration: 3870 loss: 0.0010 lr: 0.005\n","iteration: 3880 loss: 0.0012 lr: 0.005\n","iteration: 3890 loss: 0.0009 lr: 0.005\n","iteration: 3900 loss: 0.0010 lr: 0.005\n","iteration: 3910 loss: 0.0009 lr: 0.005\n","iteration: 3920 loss: 0.0012 lr: 0.005\n","iteration: 3930 loss: 0.0011 lr: 0.005\n","iteration: 3940 loss: 0.0009 lr: 0.005\n","iteration: 3950 loss: 0.0010 lr: 0.005\n","iteration: 3960 loss: 0.0009 lr: 0.005\n","iteration: 3970 loss: 0.0013 lr: 0.005\n","iteration: 3980 loss: 0.0011 lr: 0.005\n","iteration: 3990 loss: 0.0011 lr: 0.005\n","iteration: 4000 loss: 0.0011 lr: 0.005\n","iteration: 4010 loss: 0.0013 lr: 0.005\n","iteration: 4020 loss: 0.0009 lr: 0.005\n","iteration: 4030 loss: 0.0010 lr: 0.005\n","iteration: 4040 loss: 0.0010 lr: 0.005\n","iteration: 4050 loss: 0.0012 lr: 0.005\n","iteration: 4060 loss: 0.0011 lr: 0.005\n","iteration: 4070 loss: 0.0011 lr: 0.005\n","iteration: 4080 loss: 0.0011 lr: 0.005\n","iteration: 4090 loss: 0.0011 lr: 0.005\n","iteration: 4100 loss: 0.0009 lr: 0.005\n","iteration: 4110 loss: 0.0012 lr: 0.005\n","iteration: 4120 loss: 0.0013 lr: 0.005\n","iteration: 4130 loss: 0.0011 lr: 0.005\n","iteration: 4140 loss: 0.0011 lr: 0.005\n","iteration: 4150 loss: 0.0010 lr: 0.005\n","iteration: 4160 loss: 0.0010 lr: 0.005\n","iteration: 4170 loss: 0.0011 lr: 0.005\n","iteration: 4180 loss: 0.0011 lr: 0.005\n","iteration: 4190 loss: 0.0010 lr: 0.005\n","iteration: 4200 loss: 0.0012 lr: 0.005\n","iteration: 4210 loss: 0.0010 lr: 0.005\n","iteration: 4220 loss: 0.0011 lr: 0.005\n","iteration: 4230 loss: 0.0010 lr: 0.005\n","iteration: 4240 loss: 0.0012 lr: 0.005\n","iteration: 4250 loss: 0.0012 lr: 0.005\n","iteration: 4260 loss: 0.0009 lr: 0.005\n","iteration: 4270 loss: 0.0008 lr: 0.005\n","iteration: 4280 loss: 0.0010 lr: 0.005\n","iteration: 4290 loss: 0.0011 lr: 0.005\n","iteration: 4300 loss: 0.0010 lr: 0.005\n","iteration: 4310 loss: 0.0011 lr: 0.005\n","iteration: 4320 loss: 0.0008 lr: 0.005\n","iteration: 4330 loss: 0.0010 lr: 0.005\n","iteration: 4340 loss: 0.0009 lr: 0.005\n","iteration: 4350 loss: 0.0009 lr: 0.005\n","iteration: 4360 loss: 0.0010 lr: 0.005\n","iteration: 4370 loss: 0.0012 lr: 0.005\n","iteration: 4380 loss: 0.0010 lr: 0.005\n","iteration: 4390 loss: 0.0008 lr: 0.005\n","iteration: 4400 loss: 0.0010 lr: 0.005\n","iteration: 4410 loss: 0.0011 lr: 0.005\n","iteration: 4420 loss: 0.0010 lr: 0.005\n","iteration: 4430 loss: 0.0010 lr: 0.005\n","iteration: 4440 loss: 0.0010 lr: 0.005\n","iteration: 4450 loss: 0.0009 lr: 0.005\n","iteration: 4460 loss: 0.0009 lr: 0.005\n","iteration: 4470 loss: 0.0010 lr: 0.005\n","iteration: 4480 loss: 0.0010 lr: 0.005\n","iteration: 4490 loss: 0.0013 lr: 0.005\n","iteration: 4500 loss: 0.0011 lr: 0.005\n","iteration: 4510 loss: 0.0010 lr: 0.005\n","iteration: 4520 loss: 0.0008 lr: 0.005\n","iteration: 4530 loss: 0.0011 lr: 0.005\n","iteration: 4540 loss: 0.0010 lr: 0.005\n","iteration: 4550 loss: 0.0008 lr: 0.005\n","iteration: 4560 loss: 0.0009 lr: 0.005\n","iteration: 4570 loss: 0.0009 lr: 0.005\n","iteration: 4580 loss: 0.0010 lr: 0.005\n","iteration: 4590 loss: 0.0010 lr: 0.005\n","iteration: 4600 loss: 0.0009 lr: 0.005\n","iteration: 4610 loss: 0.0009 lr: 0.005\n","iteration: 4620 loss: 0.0012 lr: 0.005\n","iteration: 4630 loss: 0.0012 lr: 0.005\n","iteration: 4640 loss: 0.0009 lr: 0.005\n","iteration: 4650 loss: 0.0010 lr: 0.005\n","iteration: 4660 loss: 0.0011 lr: 0.005\n","iteration: 4670 loss: 0.0009 lr: 0.005\n","iteration: 4680 loss: 0.0009 lr: 0.005\n","iteration: 4690 loss: 0.0011 lr: 0.005\n","iteration: 4700 loss: 0.0013 lr: 0.005\n","iteration: 4710 loss: 0.0009 lr: 0.005\n","iteration: 4720 loss: 0.0013 lr: 0.005\n","iteration: 4730 loss: 0.0010 lr: 0.005\n","iteration: 4740 loss: 0.0010 lr: 0.005\n","iteration: 4750 loss: 0.0010 lr: 0.005\n","iteration: 4760 loss: 0.0009 lr: 0.005\n","iteration: 4770 loss: 0.0013 lr: 0.005\n","iteration: 4780 loss: 0.0013 lr: 0.005\n","iteration: 4790 loss: 0.0012 lr: 0.005\n","iteration: 4800 loss: 0.0013 lr: 0.005\n","iteration: 4810 loss: 0.0009 lr: 0.005\n","iteration: 4820 loss: 0.0011 lr: 0.005\n","iteration: 4830 loss: 0.0011 lr: 0.005\n","iteration: 4840 loss: 0.0009 lr: 0.005\n","iteration: 4850 loss: 0.0009 lr: 0.005\n","iteration: 4860 loss: 0.0010 lr: 0.005\n","iteration: 4870 loss: 0.0011 lr: 0.005\n","iteration: 4880 loss: 0.0011 lr: 0.005\n","iteration: 4890 loss: 0.0012 lr: 0.005\n","iteration: 4900 loss: 0.0009 lr: 0.005\n","iteration: 4910 loss: 0.0009 lr: 0.005\n","iteration: 4920 loss: 0.0010 lr: 0.005\n","iteration: 4930 loss: 0.0011 lr: 0.005\n","iteration: 4940 loss: 0.0012 lr: 0.005\n","iteration: 4950 loss: 0.0010 lr: 0.005\n","iteration: 4960 loss: 0.0010 lr: 0.005\n","iteration: 4970 loss: 0.0010 lr: 0.005\n","iteration: 4980 loss: 0.0011 lr: 0.005\n","iteration: 4990 loss: 0.0013 lr: 0.005\n","iteration: 5000 loss: 0.0012 lr: 0.005\n","iteration: 5010 loss: 0.0010 lr: 0.005\n","iteration: 5020 loss: 0.0009 lr: 0.005\n","iteration: 5030 loss: 0.0012 lr: 0.005\n","iteration: 5040 loss: 0.0009 lr: 0.005\n","iteration: 5050 loss: 0.0010 lr: 0.005\n","iteration: 5060 loss: 0.0014 lr: 0.005\n","iteration: 5070 loss: 0.0010 lr: 0.005\n","iteration: 5080 loss: 0.0012 lr: 0.005\n","iteration: 5090 loss: 0.0011 lr: 0.005\n","iteration: 5100 loss: 0.0012 lr: 0.005\n","iteration: 5110 loss: 0.0014 lr: 0.005\n","iteration: 5120 loss: 0.0011 lr: 0.005\n","iteration: 5130 loss: 0.0010 lr: 0.005\n","iteration: 5140 loss: 0.0010 lr: 0.005\n","iteration: 5150 loss: 0.0009 lr: 0.005\n","iteration: 5160 loss: 0.0015 lr: 0.005\n","iteration: 5170 loss: 0.0010 lr: 0.005\n","iteration: 5180 loss: 0.0011 lr: 0.005\n","iteration: 5190 loss: 0.0009 lr: 0.005\n","iteration: 5200 loss: 0.0009 lr: 0.005\n","iteration: 5210 loss: 0.0010 lr: 0.005\n","iteration: 5220 loss: 0.0011 lr: 0.005\n","iteration: 5230 loss: 0.0011 lr: 0.005\n","iteration: 5240 loss: 0.0013 lr: 0.005\n","iteration: 5250 loss: 0.0013 lr: 0.005\n","iteration: 5260 loss: 0.0011 lr: 0.005\n","iteration: 5270 loss: 0.0011 lr: 0.005\n","iteration: 5280 loss: 0.0011 lr: 0.005\n","iteration: 5290 loss: 0.0008 lr: 0.005\n","iteration: 5300 loss: 0.0013 lr: 0.005\n","iteration: 5310 loss: 0.0012 lr: 0.005\n","iteration: 5320 loss: 0.0012 lr: 0.005\n","iteration: 5330 loss: 0.0011 lr: 0.005\n","iteration: 5340 loss: 0.0010 lr: 0.005\n","iteration: 5350 loss: 0.0014 lr: 0.005\n","iteration: 5360 loss: 0.0009 lr: 0.005\n","iteration: 5370 loss: 0.0010 lr: 0.005\n","iteration: 5380 loss: 0.0010 lr: 0.005\n","iteration: 5390 loss: 0.0013 lr: 0.005\n","iteration: 5400 loss: 0.0013 lr: 0.005\n","iteration: 5410 loss: 0.0012 lr: 0.005\n","iteration: 5420 loss: 0.0010 lr: 0.005\n","iteration: 5430 loss: 0.0019 lr: 0.005\n","iteration: 5440 loss: 0.0011 lr: 0.005\n","iteration: 5450 loss: 0.0024 lr: 0.005\n","iteration: 5460 loss: 0.0011 lr: 0.005\n","iteration: 5470 loss: 0.0009 lr: 0.005\n","iteration: 5480 loss: 0.0016 lr: 0.005\n","iteration: 5490 loss: 0.0011 lr: 0.005\n","iteration: 5500 loss: 0.0010 lr: 0.005\n","iteration: 5510 loss: 0.0011 lr: 0.005\n","iteration: 5520 loss: 0.0012 lr: 0.005\n","iteration: 5530 loss: 0.0009 lr: 0.005\n","iteration: 5540 loss: 0.0010 lr: 0.005\n","iteration: 5550 loss: 0.0010 lr: 0.005\n","iteration: 5560 loss: 0.0010 lr: 0.005\n","iteration: 5570 loss: 0.0009 lr: 0.005\n","iteration: 5580 loss: 0.0011 lr: 0.005\n","iteration: 5590 loss: 0.0011 lr: 0.005\n","iteration: 5600 loss: 0.0010 lr: 0.005\n","iteration: 5610 loss: 0.0009 lr: 0.005\n","iteration: 5620 loss: 0.0011 lr: 0.005\n","iteration: 5630 loss: 0.0012 lr: 0.005\n","iteration: 5640 loss: 0.0012 lr: 0.005\n","iteration: 5650 loss: 0.0009 lr: 0.005\n","iteration: 5660 loss: 0.0014 lr: 0.005\n","iteration: 5670 loss: 0.0010 lr: 0.005\n","iteration: 5680 loss: 0.0010 lr: 0.005\n","iteration: 5690 loss: 0.0010 lr: 0.005\n","iteration: 5700 loss: 0.0012 lr: 0.005\n","iteration: 5710 loss: 0.0010 lr: 0.005\n","iteration: 5720 loss: 0.0008 lr: 0.005\n","iteration: 5730 loss: 0.0013 lr: 0.005\n","iteration: 5740 loss: 0.0012 lr: 0.005\n","iteration: 5750 loss: 0.0012 lr: 0.005\n","iteration: 5760 loss: 0.0010 lr: 0.005\n","iteration: 5770 loss: 0.0013 lr: 0.005\n","iteration: 5780 loss: 0.0010 lr: 0.005\n","iteration: 5790 loss: 0.0008 lr: 0.005\n","iteration: 5800 loss: 0.0010 lr: 0.005\n","iteration: 5810 loss: 0.0012 lr: 0.005\n","iteration: 5820 loss: 0.0011 lr: 0.005\n","iteration: 5830 loss: 0.0011 lr: 0.005\n","iteration: 5840 loss: 0.0010 lr: 0.005\n","iteration: 5850 loss: 0.0012 lr: 0.005\n","iteration: 5860 loss: 0.0015 lr: 0.005\n","iteration: 5870 loss: 0.0012 lr: 0.005\n","iteration: 5880 loss: 0.0011 lr: 0.005\n","iteration: 5890 loss: 0.0012 lr: 0.005\n","iteration: 5900 loss: 0.0011 lr: 0.005\n","iteration: 5910 loss: 0.0012 lr: 0.005\n","iteration: 5920 loss: 0.0011 lr: 0.005\n","iteration: 5930 loss: 0.0010 lr: 0.005\n","iteration: 5940 loss: 0.0012 lr: 0.005\n","iteration: 5950 loss: 0.0012 lr: 0.005\n","iteration: 5960 loss: 0.0011 lr: 0.005\n","iteration: 5970 loss: 0.0010 lr: 0.005\n","iteration: 5980 loss: 0.0011 lr: 0.005\n","iteration: 5990 loss: 0.0010 lr: 0.005\n","iteration: 6000 loss: 0.0010 lr: 0.005\n","iteration: 6010 loss: 0.0010 lr: 0.005\n","iteration: 6020 loss: 0.0011 lr: 0.005\n","iteration: 6030 loss: 0.0009 lr: 0.005\n","iteration: 6040 loss: 0.0010 lr: 0.005\n","iteration: 6050 loss: 0.0008 lr: 0.005\n","iteration: 6060 loss: 0.0010 lr: 0.005\n","iteration: 6070 loss: 0.0010 lr: 0.005\n","iteration: 6080 loss: 0.0015 lr: 0.005\n","iteration: 6090 loss: 0.0010 lr: 0.005\n","iteration: 6100 loss: 0.0011 lr: 0.005\n","iteration: 6110 loss: 0.0012 lr: 0.005\n","iteration: 6120 loss: 0.0014 lr: 0.005\n","iteration: 6130 loss: 0.0009 lr: 0.005\n","iteration: 6140 loss: 0.0011 lr: 0.005\n","iteration: 6150 loss: 0.0009 lr: 0.005\n","iteration: 6160 loss: 0.0009 lr: 0.005\n","iteration: 6170 loss: 0.0012 lr: 0.005\n","iteration: 6180 loss: 0.0007 lr: 0.005\n","iteration: 6190 loss: 0.0010 lr: 0.005\n","iteration: 6200 loss: 0.0012 lr: 0.005\n","iteration: 6210 loss: 0.0009 lr: 0.005\n","iteration: 6220 loss: 0.0011 lr: 0.005\n","iteration: 6230 loss: 0.0011 lr: 0.005\n","iteration: 6240 loss: 0.0011 lr: 0.005\n","iteration: 6250 loss: 0.0010 lr: 0.005\n","iteration: 6260 loss: 0.0010 lr: 0.005\n","iteration: 6270 loss: 0.0012 lr: 0.005\n","iteration: 6280 loss: 0.0009 lr: 0.005\n","iteration: 6290 loss: 0.0013 lr: 0.005\n","iteration: 6300 loss: 0.0009 lr: 0.005\n","iteration: 6310 loss: 0.0009 lr: 0.005\n","iteration: 6320 loss: 0.0010 lr: 0.005\n","iteration: 6330 loss: 0.0011 lr: 0.005\n","iteration: 6340 loss: 0.0009 lr: 0.005\n","iteration: 6350 loss: 0.0016 lr: 0.005\n","iteration: 6360 loss: 0.0010 lr: 0.005\n","iteration: 6370 loss: 0.0010 lr: 0.005\n","iteration: 6380 loss: 0.0011 lr: 0.005\n","iteration: 6390 loss: 0.0011 lr: 0.005\n","iteration: 6400 loss: 0.0010 lr: 0.005\n","iteration: 6410 loss: 0.0014 lr: 0.005\n","iteration: 6420 loss: 0.0011 lr: 0.005\n","iteration: 6430 loss: 0.0010 lr: 0.005\n","iteration: 6440 loss: 0.0010 lr: 0.005\n","iteration: 6450 loss: 0.0010 lr: 0.005\n","iteration: 6460 loss: 0.0009 lr: 0.005\n","iteration: 6470 loss: 0.0010 lr: 0.005\n","iteration: 6480 loss: 0.0011 lr: 0.005\n","iteration: 6490 loss: 0.0010 lr: 0.005\n","iteration: 6500 loss: 0.0011 lr: 0.005\n","iteration: 6510 loss: 0.0009 lr: 0.005\n","iteration: 6520 loss: 0.0012 lr: 0.005\n","iteration: 6530 loss: 0.0012 lr: 0.005\n","iteration: 6540 loss: 0.0012 lr: 0.005\n","iteration: 6550 loss: 0.0011 lr: 0.005\n","iteration: 6560 loss: 0.0010 lr: 0.005\n","iteration: 6570 loss: 0.0010 lr: 0.005\n","iteration: 6580 loss: 0.0012 lr: 0.005\n","iteration: 6590 loss: 0.0012 lr: 0.005\n","iteration: 6600 loss: 0.0010 lr: 0.005\n","iteration: 6610 loss: 0.0010 lr: 0.005\n","iteration: 6620 loss: 0.0012 lr: 0.005\n","iteration: 6630 loss: 0.0009 lr: 0.005\n","iteration: 6640 loss: 0.0009 lr: 0.005\n","iteration: 6650 loss: 0.0011 lr: 0.005\n","iteration: 6660 loss: 0.0012 lr: 0.005\n","iteration: 6670 loss: 0.0010 lr: 0.005\n","iteration: 6680 loss: 0.0011 lr: 0.005\n","iteration: 6690 loss: 0.0009 lr: 0.005\n","iteration: 6700 loss: 0.0009 lr: 0.005\n","iteration: 6710 loss: 0.0009 lr: 0.005\n","iteration: 6720 loss: 0.0011 lr: 0.005\n","iteration: 6730 loss: 0.0009 lr: 0.005\n","iteration: 6740 loss: 0.0009 lr: 0.005\n","iteration: 6750 loss: 0.0012 lr: 0.005\n","iteration: 6760 loss: 0.0009 lr: 0.005\n","iteration: 6770 loss: 0.0010 lr: 0.005\n","iteration: 6780 loss: 0.0009 lr: 0.005\n","iteration: 6790 loss: 0.0010 lr: 0.005\n","iteration: 6800 loss: 0.0013 lr: 0.005\n","iteration: 6810 loss: 0.0009 lr: 0.005\n","iteration: 6820 loss: 0.0011 lr: 0.005\n","iteration: 6830 loss: 0.0010 lr: 0.005\n","iteration: 6840 loss: 0.0011 lr: 0.005\n","iteration: 6850 loss: 0.0009 lr: 0.005\n","iteration: 6860 loss: 0.0014 lr: 0.005\n","iteration: 6870 loss: 0.0011 lr: 0.005\n","iteration: 6880 loss: 0.0010 lr: 0.005\n","iteration: 6890 loss: 0.0011 lr: 0.005\n","iteration: 6900 loss: 0.0011 lr: 0.005\n","iteration: 6910 loss: 0.0011 lr: 0.005\n","iteration: 6920 loss: 0.0010 lr: 0.005\n","iteration: 6930 loss: 0.0014 lr: 0.005\n","iteration: 6940 loss: 0.0010 lr: 0.005\n","iteration: 6950 loss: 0.0010 lr: 0.005\n","iteration: 6960 loss: 0.0011 lr: 0.005\n","iteration: 6970 loss: 0.0011 lr: 0.005\n","iteration: 6980 loss: 0.0011 lr: 0.005\n","iteration: 6990 loss: 0.0011 lr: 0.005\n","iteration: 7000 loss: 0.0010 lr: 0.005\n","iteration: 7010 loss: 0.0010 lr: 0.005\n","iteration: 7020 loss: 0.0013 lr: 0.005\n","iteration: 7030 loss: 0.0009 lr: 0.005\n","iteration: 7040 loss: 0.0008 lr: 0.005\n","iteration: 7050 loss: 0.0012 lr: 0.005\n","iteration: 7060 loss: 0.0012 lr: 0.005\n","iteration: 7070 loss: 0.0008 lr: 0.005\n","iteration: 7080 loss: 0.0009 lr: 0.005\n","iteration: 7090 loss: 0.0010 lr: 0.005\n","iteration: 7100 loss: 0.0011 lr: 0.005\n","iteration: 7110 loss: 0.0009 lr: 0.005\n","iteration: 7120 loss: 0.0009 lr: 0.005\n","iteration: 7130 loss: 0.0007 lr: 0.005\n","iteration: 7140 loss: 0.0009 lr: 0.005\n","iteration: 7150 loss: 0.0007 lr: 0.005\n","iteration: 7160 loss: 0.0011 lr: 0.005\n","iteration: 7170 loss: 0.0010 lr: 0.005\n","iteration: 7180 loss: 0.0010 lr: 0.005\n","iteration: 7190 loss: 0.0009 lr: 0.005\n","iteration: 7200 loss: 0.0009 lr: 0.005\n","iteration: 7210 loss: 0.0010 lr: 0.005\n","iteration: 7220 loss: 0.0011 lr: 0.005\n","iteration: 7230 loss: 0.0010 lr: 0.005\n","iteration: 7240 loss: 0.0010 lr: 0.005\n","iteration: 7250 loss: 0.0011 lr: 0.005\n","iteration: 7260 loss: 0.0012 lr: 0.005\n","iteration: 7270 loss: 0.0010 lr: 0.005\n","iteration: 7280 loss: 0.0011 lr: 0.005\n","iteration: 7290 loss: 0.0014 lr: 0.005\n","iteration: 7300 loss: 0.0012 lr: 0.005\n","iteration: 7310 loss: 0.0010 lr: 0.005\n","iteration: 7320 loss: 0.0009 lr: 0.005\n","iteration: 7330 loss: 0.0010 lr: 0.005\n","iteration: 7340 loss: 0.0010 lr: 0.005\n","iteration: 7350 loss: 0.0010 lr: 0.005\n","iteration: 7360 loss: 0.0011 lr: 0.005\n","iteration: 7370 loss: 0.0010 lr: 0.005\n","iteration: 7380 loss: 0.0011 lr: 0.005\n","iteration: 7390 loss: 0.0012 lr: 0.005\n","iteration: 7400 loss: 0.0011 lr: 0.005\n","iteration: 7410 loss: 0.0010 lr: 0.005\n","iteration: 7420 loss: 0.0010 lr: 0.005\n","iteration: 7430 loss: 0.0011 lr: 0.005\n","iteration: 7440 loss: 0.0010 lr: 0.005\n","iteration: 7450 loss: 0.0011 lr: 0.005\n","iteration: 7460 loss: 0.0010 lr: 0.005\n","iteration: 7470 loss: 0.0011 lr: 0.005\n","iteration: 7480 loss: 0.0011 lr: 0.005\n","iteration: 7490 loss: 0.0012 lr: 0.005\n","iteration: 7500 loss: 0.0013 lr: 0.005\n","iteration: 7510 loss: 0.0010 lr: 0.005\n","iteration: 7520 loss: 0.0011 lr: 0.005\n","iteration: 7530 loss: 0.0009 lr: 0.005\n","iteration: 7540 loss: 0.0010 lr: 0.005\n","iteration: 7550 loss: 0.0011 lr: 0.005\n","iteration: 7560 loss: 0.0010 lr: 0.005\n","iteration: 7570 loss: 0.0011 lr: 0.005\n","iteration: 7580 loss: 0.0009 lr: 0.005\n","iteration: 7590 loss: 0.0012 lr: 0.005\n","iteration: 7600 loss: 0.0011 lr: 0.005\n","iteration: 7610 loss: 0.0014 lr: 0.005\n","iteration: 7620 loss: 0.0011 lr: 0.005\n","iteration: 7630 loss: 0.0009 lr: 0.005\n","iteration: 7640 loss: 0.0011 lr: 0.005\n","iteration: 7650 loss: 0.0008 lr: 0.005\n","iteration: 7660 loss: 0.0011 lr: 0.005\n","iteration: 7670 loss: 0.0011 lr: 0.005\n","iteration: 7680 loss: 0.0011 lr: 0.005\n","iteration: 7690 loss: 0.0009 lr: 0.005\n","iteration: 7700 loss: 0.0010 lr: 0.005\n","iteration: 7710 loss: 0.0010 lr: 0.005\n","iteration: 7720 loss: 0.0010 lr: 0.005\n","iteration: 7730 loss: 0.0011 lr: 0.005\n","iteration: 7740 loss: 0.0011 lr: 0.005\n","iteration: 7750 loss: 0.0009 lr: 0.005\n","iteration: 7760 loss: 0.0010 lr: 0.005\n","iteration: 7770 loss: 0.0011 lr: 0.005\n","iteration: 7780 loss: 0.0010 lr: 0.005\n","iteration: 7790 loss: 0.0010 lr: 0.005\n","iteration: 7800 loss: 0.0013 lr: 0.005\n","iteration: 7810 loss: 0.0010 lr: 0.005\n","iteration: 7820 loss: 0.0010 lr: 0.005\n","iteration: 7830 loss: 0.0008 lr: 0.005\n","iteration: 7840 loss: 0.0010 lr: 0.005\n","iteration: 7850 loss: 0.0013 lr: 0.005\n","iteration: 7860 loss: 0.0009 lr: 0.005\n","iteration: 7870 loss: 0.0010 lr: 0.005\n","iteration: 7880 loss: 0.0009 lr: 0.005\n","iteration: 7890 loss: 0.0009 lr: 0.005\n","iteration: 7900 loss: 0.0010 lr: 0.005\n","iteration: 7910 loss: 0.0010 lr: 0.005\n","iteration: 7920 loss: 0.0010 lr: 0.005\n","iteration: 7930 loss: 0.0010 lr: 0.005\n","iteration: 7940 loss: 0.0009 lr: 0.005\n","iteration: 7950 loss: 0.0011 lr: 0.005\n","iteration: 7960 loss: 0.0009 lr: 0.005\n","iteration: 7970 loss: 0.0013 lr: 0.005\n","iteration: 7980 loss: 0.0009 lr: 0.005\n","iteration: 7990 loss: 0.0008 lr: 0.005\n","iteration: 8000 loss: 0.0012 lr: 0.005\n","iteration: 8010 loss: 0.0010 lr: 0.005\n","iteration: 8020 loss: 0.0011 lr: 0.005\n","iteration: 8030 loss: 0.0011 lr: 0.005\n","iteration: 8040 loss: 0.0011 lr: 0.005\n","iteration: 8050 loss: 0.0012 lr: 0.005\n","iteration: 8060 loss: 0.0012 lr: 0.005\n","iteration: 8070 loss: 0.0008 lr: 0.005\n","iteration: 8080 loss: 0.0014 lr: 0.005\n","iteration: 8090 loss: 0.0008 lr: 0.005\n","iteration: 8100 loss: 0.0011 lr: 0.005\n","iteration: 8110 loss: 0.0011 lr: 0.005\n","iteration: 8120 loss: 0.0011 lr: 0.005\n","iteration: 8130 loss: 0.0009 lr: 0.005\n","iteration: 8140 loss: 0.0010 lr: 0.005\n","iteration: 8150 loss: 0.0009 lr: 0.005\n","iteration: 8160 loss: 0.0010 lr: 0.005\n","iteration: 8170 loss: 0.0010 lr: 0.005\n","iteration: 8180 loss: 0.0011 lr: 0.005\n","iteration: 8190 loss: 0.0012 lr: 0.005\n","iteration: 8200 loss: 0.0010 lr: 0.005\n","iteration: 8210 loss: 0.0010 lr: 0.005\n","iteration: 8220 loss: 0.0012 lr: 0.005\n","iteration: 8230 loss: 0.0010 lr: 0.005\n","iteration: 8240 loss: 0.0010 lr: 0.005\n","iteration: 8250 loss: 0.0013 lr: 0.005\n","iteration: 8260 loss: 0.0010 lr: 0.005\n","iteration: 8270 loss: 0.0013 lr: 0.005\n","iteration: 8280 loss: 0.0012 lr: 0.005\n","iteration: 8290 loss: 0.0010 lr: 0.005\n","iteration: 8300 loss: 0.0010 lr: 0.005\n","iteration: 8310 loss: 0.0009 lr: 0.005\n","iteration: 8320 loss: 0.0010 lr: 0.005\n","iteration: 8330 loss: 0.0010 lr: 0.005\n","iteration: 8340 loss: 0.0010 lr: 0.005\n","iteration: 8350 loss: 0.0011 lr: 0.005\n","iteration: 8360 loss: 0.0011 lr: 0.005\n","iteration: 8370 loss: 0.0014 lr: 0.005\n","iteration: 8380 loss: 0.0009 lr: 0.005\n","iteration: 8390 loss: 0.0010 lr: 0.005\n","iteration: 8400 loss: 0.0008 lr: 0.005\n","iteration: 8410 loss: 0.0008 lr: 0.005\n","iteration: 8420 loss: 0.0010 lr: 0.005\n","iteration: 8430 loss: 0.0011 lr: 0.005\n","iteration: 8440 loss: 0.0009 lr: 0.005\n","iteration: 8450 loss: 0.0009 lr: 0.005\n","iteration: 8460 loss: 0.0010 lr: 0.005\n","iteration: 8470 loss: 0.0010 lr: 0.005\n","iteration: 8480 loss: 0.0013 lr: 0.005\n","iteration: 8490 loss: 0.0011 lr: 0.005\n","iteration: 8500 loss: 0.0011 lr: 0.005\n","iteration: 8510 loss: 0.0013 lr: 0.005\n","iteration: 8520 loss: 0.0015 lr: 0.005\n","iteration: 8530 loss: 0.0010 lr: 0.005\n","iteration: 8540 loss: 0.0010 lr: 0.005\n","iteration: 8550 loss: 0.0012 lr: 0.005\n","iteration: 8560 loss: 0.0009 lr: 0.005\n","iteration: 8570 loss: 0.0016 lr: 0.005\n","iteration: 8580 loss: 0.0009 lr: 0.005\n","iteration: 8590 loss: 0.0010 lr: 0.005\n","iteration: 8600 loss: 0.0009 lr: 0.005\n","iteration: 8610 loss: 0.0008 lr: 0.005\n","iteration: 8620 loss: 0.0011 lr: 0.005\n","iteration: 8630 loss: 0.0010 lr: 0.005\n","iteration: 8640 loss: 0.0011 lr: 0.005\n","iteration: 8650 loss: 0.0009 lr: 0.005\n","iteration: 8660 loss: 0.0011 lr: 0.005\n","iteration: 8670 loss: 0.0009 lr: 0.005\n","iteration: 8680 loss: 0.0011 lr: 0.005\n","iteration: 8690 loss: 0.0012 lr: 0.005\n","iteration: 8700 loss: 0.0013 lr: 0.005\n","iteration: 8710 loss: 0.0010 lr: 0.005\n","iteration: 8720 loss: 0.0011 lr: 0.005\n","iteration: 8730 loss: 0.0012 lr: 0.005\n","iteration: 8740 loss: 0.0012 lr: 0.005\n","iteration: 8750 loss: 0.0008 lr: 0.005\n","iteration: 8760 loss: 0.0012 lr: 0.005\n","iteration: 8770 loss: 0.0012 lr: 0.005\n","iteration: 8780 loss: 0.0012 lr: 0.005\n","iteration: 8790 loss: 0.0009 lr: 0.005\n","iteration: 8800 loss: 0.0011 lr: 0.005\n","iteration: 8810 loss: 0.0011 lr: 0.005\n","iteration: 8820 loss: 0.0010 lr: 0.005\n","iteration: 8830 loss: 0.0010 lr: 0.005\n","iteration: 8840 loss: 0.0011 lr: 0.005\n","iteration: 8850 loss: 0.0011 lr: 0.005\n","iteration: 8860 loss: 0.0009 lr: 0.005\n","iteration: 8870 loss: 0.0010 lr: 0.005\n","iteration: 8880 loss: 0.0011 lr: 0.005\n","iteration: 8890 loss: 0.0010 lr: 0.005\n","iteration: 8900 loss: 0.0011 lr: 0.005\n","iteration: 8910 loss: 0.0009 lr: 0.005\n","iteration: 8920 loss: 0.0010 lr: 0.005\n","iteration: 8930 loss: 0.0011 lr: 0.005\n","iteration: 8940 loss: 0.0009 lr: 0.005\n","iteration: 8950 loss: 0.0009 lr: 0.005\n","iteration: 8960 loss: 0.0013 lr: 0.005\n","iteration: 8970 loss: 0.0014 lr: 0.005\n","iteration: 8980 loss: 0.0011 lr: 0.005\n","iteration: 8990 loss: 0.0010 lr: 0.005\n","iteration: 9000 loss: 0.0008 lr: 0.005\n","iteration: 9010 loss: 0.0011 lr: 0.005\n","iteration: 9020 loss: 0.0009 lr: 0.005\n","iteration: 9030 loss: 0.0009 lr: 0.005\n","iteration: 9040 loss: 0.0012 lr: 0.005\n","iteration: 9050 loss: 0.0012 lr: 0.005\n","iteration: 9060 loss: 0.0010 lr: 0.005\n","iteration: 9070 loss: 0.0011 lr: 0.005\n","iteration: 9080 loss: 0.0009 lr: 0.005\n","iteration: 9090 loss: 0.0010 lr: 0.005\n","iteration: 9100 loss: 0.0010 lr: 0.005\n","iteration: 9110 loss: 0.0010 lr: 0.005\n","iteration: 9120 loss: 0.0013 lr: 0.005\n","iteration: 9130 loss: 0.0010 lr: 0.005\n","iteration: 9140 loss: 0.0010 lr: 0.005\n","iteration: 9150 loss: 0.0011 lr: 0.005\n","iteration: 9160 loss: 0.0010 lr: 0.005\n","iteration: 9170 loss: 0.0010 lr: 0.005\n","iteration: 9180 loss: 0.0010 lr: 0.005\n","iteration: 9190 loss: 0.0011 lr: 0.005\n","iteration: 9200 loss: 0.0009 lr: 0.005\n","iteration: 9210 loss: 0.0010 lr: 0.005\n","iteration: 9220 loss: 0.0011 lr: 0.005\n","iteration: 9230 loss: 0.0009 lr: 0.005\n","iteration: 9240 loss: 0.0008 lr: 0.005\n","iteration: 9250 loss: 0.0011 lr: 0.005\n","iteration: 9260 loss: 0.0009 lr: 0.005\n","iteration: 9270 loss: 0.0013 lr: 0.005\n","iteration: 9280 loss: 0.0010 lr: 0.005\n","iteration: 9290 loss: 0.0011 lr: 0.005\n","iteration: 9300 loss: 0.0010 lr: 0.005\n","iteration: 9310 loss: 0.0011 lr: 0.005\n","iteration: 9320 loss: 0.0009 lr: 0.005\n","iteration: 9330 loss: 0.0012 lr: 0.005\n","iteration: 9340 loss: 0.0009 lr: 0.005\n","iteration: 9350 loss: 0.0011 lr: 0.005\n","iteration: 9360 loss: 0.0010 lr: 0.005\n","iteration: 9370 loss: 0.0013 lr: 0.005\n","iteration: 9380 loss: 0.0013 lr: 0.005\n","iteration: 9390 loss: 0.0009 lr: 0.005\n","iteration: 9400 loss: 0.0010 lr: 0.005\n","iteration: 9410 loss: 0.0010 lr: 0.005\n","iteration: 9420 loss: 0.0012 lr: 0.005\n","iteration: 9430 loss: 0.0011 lr: 0.005\n","iteration: 9440 loss: 0.0010 lr: 0.005\n","iteration: 9450 loss: 0.0014 lr: 0.005\n","iteration: 9460 loss: 0.0010 lr: 0.005\n","iteration: 9470 loss: 0.0009 lr: 0.005\n","iteration: 9480 loss: 0.0012 lr: 0.005\n","iteration: 9490 loss: 0.0012 lr: 0.005\n","iteration: 9500 loss: 0.0010 lr: 0.005\n","iteration: 9510 loss: 0.0011 lr: 0.005\n","iteration: 9520 loss: 0.0010 lr: 0.005\n","iteration: 9530 loss: 0.0011 lr: 0.005\n","iteration: 9540 loss: 0.0008 lr: 0.005\n","iteration: 9550 loss: 0.0012 lr: 0.005\n","iteration: 9560 loss: 0.0010 lr: 0.005\n","iteration: 9570 loss: 0.0010 lr: 0.005\n","iteration: 9580 loss: 0.0012 lr: 0.005\n","iteration: 9590 loss: 0.0009 lr: 0.005\n","iteration: 9600 loss: 0.0013 lr: 0.005\n","iteration: 9610 loss: 0.0012 lr: 0.005\n","iteration: 9620 loss: 0.0012 lr: 0.005\n","iteration: 9630 loss: 0.0010 lr: 0.005\n","iteration: 9640 loss: 0.0010 lr: 0.005\n","iteration: 9650 loss: 0.0009 lr: 0.005\n","iteration: 9660 loss: 0.0011 lr: 0.005\n","iteration: 9670 loss: 0.0011 lr: 0.005\n","iteration: 9680 loss: 0.0010 lr: 0.005\n","iteration: 9690 loss: 0.0013 lr: 0.005\n","iteration: 9700 loss: 0.0011 lr: 0.005\n","iteration: 9710 loss: 0.0011 lr: 0.005\n","iteration: 9720 loss: 0.0010 lr: 0.005\n","iteration: 9730 loss: 0.0009 lr: 0.005\n","iteration: 9740 loss: 0.0012 lr: 0.005\n","iteration: 9750 loss: 0.0010 lr: 0.005\n","iteration: 9760 loss: 0.0008 lr: 0.005\n","iteration: 9770 loss: 0.0010 lr: 0.005\n","iteration: 9780 loss: 0.0010 lr: 0.005\n","iteration: 9790 loss: 0.0010 lr: 0.005\n","iteration: 9800 loss: 0.0010 lr: 0.005\n","iteration: 9810 loss: 0.0008 lr: 0.005\n","iteration: 9820 loss: 0.0010 lr: 0.005\n","iteration: 9830 loss: 0.0010 lr: 0.005\n","iteration: 9840 loss: 0.0009 lr: 0.005\n","iteration: 9850 loss: 0.0010 lr: 0.005\n","iteration: 9860 loss: 0.0012 lr: 0.005\n","iteration: 9870 loss: 0.0011 lr: 0.005\n","iteration: 9880 loss: 0.0011 lr: 0.005\n","iteration: 9890 loss: 0.0011 lr: 0.005\n","iteration: 9900 loss: 0.0012 lr: 0.005\n","iteration: 9910 loss: 0.0009 lr: 0.005\n","iteration: 9920 loss: 0.0012 lr: 0.005\n","iteration: 9930 loss: 0.0009 lr: 0.005\n","iteration: 9940 loss: 0.0010 lr: 0.005\n","iteration: 9950 loss: 0.0008 lr: 0.005\n","iteration: 9960 loss: 0.0010 lr: 0.005\n","iteration: 9970 loss: 0.0010 lr: 0.005\n","iteration: 9980 loss: 0.0011 lr: 0.005\n","iteration: 9990 loss: 0.0012 lr: 0.005\n","iteration: 10000 loss: 0.0010 lr: 0.005\n","iteration: 10010 loss: 0.0011 lr: 0.02\n","iteration: 10020 loss: 0.0010 lr: 0.02\n","iteration: 10030 loss: 0.0011 lr: 0.02\n","iteration: 10040 loss: 0.0012 lr: 0.02\n","iteration: 10050 loss: 0.0012 lr: 0.02\n","iteration: 10060 loss: 0.0011 lr: 0.02\n","iteration: 10070 loss: 0.0010 lr: 0.02\n","iteration: 10080 loss: 0.0012 lr: 0.02\n","iteration: 10090 loss: 0.0012 lr: 0.02\n","iteration: 10100 loss: 0.0013 lr: 0.02\n","iteration: 10110 loss: 0.0012 lr: 0.02\n","iteration: 10120 loss: 0.0011 lr: 0.02\n","iteration: 10130 loss: 0.0015 lr: 0.02\n","iteration: 10140 loss: 0.0012 lr: 0.02\n","iteration: 10150 loss: 0.0012 lr: 0.02\n","iteration: 10160 loss: 0.0014 lr: 0.02\n","iteration: 10170 loss: 0.0012 lr: 0.02\n","iteration: 10180 loss: 0.0012 lr: 0.02\n","iteration: 10190 loss: 0.0016 lr: 0.02\n","iteration: 10200 loss: 0.0014 lr: 0.02\n","iteration: 10210 loss: 0.0012 lr: 0.02\n","iteration: 10220 loss: 0.0014 lr: 0.02\n","iteration: 10230 loss: 0.0010 lr: 0.02\n","iteration: 10240 loss: 0.0012 lr: 0.02\n","iteration: 10250 loss: 0.0013 lr: 0.02\n","iteration: 10260 loss: 0.0012 lr: 0.02\n","iteration: 10270 loss: 0.0013 lr: 0.02\n","iteration: 10280 loss: 0.0013 lr: 0.02\n","iteration: 10290 loss: 0.0013 lr: 0.02\n","iteration: 10300 loss: 0.0013 lr: 0.02\n","iteration: 10310 loss: 0.0010 lr: 0.02\n","iteration: 10320 loss: 0.0015 lr: 0.02\n","iteration: 10330 loss: 0.0011 lr: 0.02\n","iteration: 10340 loss: 0.0013 lr: 0.02\n","iteration: 10350 loss: 0.0013 lr: 0.02\n","iteration: 10360 loss: 0.0012 lr: 0.02\n","iteration: 10370 loss: 0.0013 lr: 0.02\n","iteration: 10380 loss: 0.0010 lr: 0.02\n","iteration: 10390 loss: 0.0011 lr: 0.02\n","iteration: 10400 loss: 0.0013 lr: 0.02\n","iteration: 10410 loss: 0.0013 lr: 0.02\n","iteration: 10420 loss: 0.0017 lr: 0.02\n","iteration: 10430 loss: 0.0012 lr: 0.02\n","iteration: 10440 loss: 0.0013 lr: 0.02\n","iteration: 10450 loss: 0.0015 lr: 0.02\n","iteration: 10460 loss: 0.0012 lr: 0.02\n","iteration: 10470 loss: 0.0014 lr: 0.02\n","iteration: 10480 loss: 0.0013 lr: 0.02\n","iteration: 10490 loss: 0.0014 lr: 0.02\n","iteration: 10500 loss: 0.0013 lr: 0.02\n","iteration: 10510 loss: 0.0014 lr: 0.02\n","iteration: 10520 loss: 0.0012 lr: 0.02\n","iteration: 10530 loss: 0.0014 lr: 0.02\n","iteration: 10540 loss: 0.0011 lr: 0.02\n","iteration: 10550 loss: 0.0012 lr: 0.02\n","iteration: 10560 loss: 0.0013 lr: 0.02\n","iteration: 10570 loss: 0.0014 lr: 0.02\n","iteration: 10580 loss: 0.0012 lr: 0.02\n","iteration: 10590 loss: 0.0015 lr: 0.02\n","iteration: 10600 loss: 0.0010 lr: 0.02\n","iteration: 10610 loss: 0.0013 lr: 0.02\n","iteration: 10620 loss: 0.0012 lr: 0.02\n","iteration: 10630 loss: 0.0013 lr: 0.02\n","iteration: 10640 loss: 0.0014 lr: 0.02\n","iteration: 10650 loss: 0.0012 lr: 0.02\n","iteration: 10660 loss: 0.0009 lr: 0.02\n","iteration: 10670 loss: 0.0017 lr: 0.02\n","iteration: 10680 loss: 0.0014 lr: 0.02\n","iteration: 10690 loss: 0.0012 lr: 0.02\n","iteration: 10700 loss: 0.0011 lr: 0.02\n","iteration: 10710 loss: 0.0014 lr: 0.02\n","iteration: 10720 loss: 0.0011 lr: 0.02\n","iteration: 10730 loss: 0.0012 lr: 0.02\n","iteration: 10740 loss: 0.0012 lr: 0.02\n","iteration: 10750 loss: 0.0012 lr: 0.02\n","iteration: 10760 loss: 0.0010 lr: 0.02\n","iteration: 10770 loss: 0.0012 lr: 0.02\n","iteration: 10780 loss: 0.0013 lr: 0.02\n","iteration: 10790 loss: 0.0022 lr: 0.02\n","iteration: 10800 loss: 0.0014 lr: 0.02\n","iteration: 10810 loss: 0.0012 lr: 0.02\n","iteration: 10820 loss: 0.0013 lr: 0.02\n","iteration: 10830 loss: 0.0014 lr: 0.02\n","iteration: 10840 loss: 0.0013 lr: 0.02\n","iteration: 10850 loss: 0.0011 lr: 0.02\n","iteration: 10860 loss: 0.0013 lr: 0.02\n","iteration: 10870 loss: 0.0014 lr: 0.02\n","iteration: 10880 loss: 0.0011 lr: 0.02\n","iteration: 10890 loss: 0.0014 lr: 0.02\n","iteration: 10900 loss: 0.0012 lr: 0.02\n","iteration: 10910 loss: 0.0012 lr: 0.02\n","iteration: 10920 loss: 0.0010 lr: 0.02\n","iteration: 10930 loss: 0.0014 lr: 0.02\n","iteration: 10940 loss: 0.0018 lr: 0.02\n","iteration: 10950 loss: 0.0011 lr: 0.02\n","iteration: 10960 loss: 0.0011 lr: 0.02\n","iteration: 10970 loss: 0.0016 lr: 0.02\n","iteration: 10980 loss: 0.0014 lr: 0.02\n","iteration: 10990 loss: 0.0011 lr: 0.02\n","iteration: 11000 loss: 0.0014 lr: 0.02\n","iteration: 11010 loss: 0.0018 lr: 0.02\n","iteration: 11020 loss: 0.0013 lr: 0.02\n","iteration: 11030 loss: 0.0011 lr: 0.02\n","iteration: 11040 loss: 0.0018 lr: 0.02\n","iteration: 11050 loss: 0.0010 lr: 0.02\n","iteration: 11060 loss: 0.0014 lr: 0.02\n","iteration: 11070 loss: 0.0013 lr: 0.02\n","iteration: 11080 loss: 0.0016 lr: 0.02\n","iteration: 11090 loss: 0.0012 lr: 0.02\n","iteration: 11100 loss: 0.0011 lr: 0.02\n","iteration: 11110 loss: 0.0012 lr: 0.02\n","iteration: 11120 loss: 0.0012 lr: 0.02\n","iteration: 11130 loss: 0.0014 lr: 0.02\n","iteration: 11140 loss: 0.0013 lr: 0.02\n","iteration: 11150 loss: 0.0013 lr: 0.02\n","iteration: 11160 loss: 0.0010 lr: 0.02\n","iteration: 11170 loss: 0.0013 lr: 0.02\n","iteration: 11180 loss: 0.0012 lr: 0.02\n","iteration: 11190 loss: 0.0012 lr: 0.02\n","iteration: 11200 loss: 0.0010 lr: 0.02\n","iteration: 11210 loss: 0.0012 lr: 0.02\n","iteration: 11220 loss: 0.0017 lr: 0.02\n","iteration: 11230 loss: 0.0012 lr: 0.02\n","iteration: 11240 loss: 0.0014 lr: 0.02\n","iteration: 11250 loss: 0.0015 lr: 0.02\n","iteration: 11260 loss: 0.0011 lr: 0.02\n","iteration: 11270 loss: 0.0011 lr: 0.02\n","iteration: 11280 loss: 0.0010 lr: 0.02\n","iteration: 11290 loss: 0.0011 lr: 0.02\n","iteration: 11300 loss: 0.0012 lr: 0.02\n","iteration: 11310 loss: 0.0014 lr: 0.02\n","iteration: 11320 loss: 0.0010 lr: 0.02\n","iteration: 11330 loss: 0.0011 lr: 0.02\n","iteration: 11340 loss: 0.0013 lr: 0.02\n","iteration: 11350 loss: 0.0011 lr: 0.02\n","iteration: 11360 loss: 0.0011 lr: 0.02\n","iteration: 11370 loss: 0.0013 lr: 0.02\n","iteration: 11380 loss: 0.0014 lr: 0.02\n","iteration: 11390 loss: 0.0011 lr: 0.02\n","iteration: 11400 loss: 0.0012 lr: 0.02\n","iteration: 11410 loss: 0.0014 lr: 0.02\n","iteration: 11420 loss: 0.0013 lr: 0.02\n","iteration: 11430 loss: 0.0014 lr: 0.02\n","iteration: 11440 loss: 0.0012 lr: 0.02\n","iteration: 11450 loss: 0.0011 lr: 0.02\n","iteration: 11460 loss: 0.0012 lr: 0.02\n","iteration: 11470 loss: 0.0013 lr: 0.02\n","iteration: 11480 loss: 0.0011 lr: 0.02\n","iteration: 11490 loss: 0.0016 lr: 0.02\n","iteration: 11500 loss: 0.0015 lr: 0.02\n","iteration: 11510 loss: 0.0012 lr: 0.02\n","iteration: 11520 loss: 0.0011 lr: 0.02\n","iteration: 11530 loss: 0.0013 lr: 0.02\n","iteration: 11540 loss: 0.0013 lr: 0.02\n","iteration: 11550 loss: 0.0011 lr: 0.02\n","iteration: 11560 loss: 0.0007 lr: 0.02\n","iteration: 11570 loss: 0.0011 lr: 0.02\n","iteration: 11580 loss: 0.0014 lr: 0.02\n","iteration: 11590 loss: 0.0012 lr: 0.02\n","iteration: 11600 loss: 0.0012 lr: 0.02\n","iteration: 11610 loss: 0.0011 lr: 0.02\n","iteration: 11620 loss: 0.0014 lr: 0.02\n","iteration: 11630 loss: 0.0011 lr: 0.02\n","iteration: 11640 loss: 0.0011 lr: 0.02\n","iteration: 11650 loss: 0.0012 lr: 0.02\n","iteration: 11660 loss: 0.0011 lr: 0.02\n","iteration: 11670 loss: 0.0012 lr: 0.02\n","iteration: 11680 loss: 0.0013 lr: 0.02\n","iteration: 11690 loss: 0.0011 lr: 0.02\n","iteration: 11700 loss: 0.0013 lr: 0.02\n","iteration: 11710 loss: 0.0012 lr: 0.02\n","iteration: 11720 loss: 0.0016 lr: 0.02\n","iteration: 11730 loss: 0.0012 lr: 0.02\n","iteration: 11740 loss: 0.0012 lr: 0.02\n","iteration: 11750 loss: 0.0009 lr: 0.02\n","iteration: 11760 loss: 0.0017 lr: 0.02\n","iteration: 11770 loss: 0.0014 lr: 0.02\n","iteration: 11780 loss: 0.0015 lr: 0.02\n","iteration: 11790 loss: 0.0012 lr: 0.02\n","iteration: 11800 loss: 0.0012 lr: 0.02\n","iteration: 11810 loss: 0.0012 lr: 0.02\n","iteration: 11820 loss: 0.0011 lr: 0.02\n","iteration: 11830 loss: 0.0014 lr: 0.02\n","iteration: 11840 loss: 0.0012 lr: 0.02\n","iteration: 11850 loss: 0.0012 lr: 0.02\n","iteration: 11860 loss: 0.0011 lr: 0.02\n","iteration: 11870 loss: 0.0011 lr: 0.02\n","iteration: 11880 loss: 0.0010 lr: 0.02\n","iteration: 11890 loss: 0.0014 lr: 0.02\n","iteration: 11900 loss: 0.0010 lr: 0.02\n","iteration: 11910 loss: 0.0015 lr: 0.02\n","iteration: 11920 loss: 0.0018 lr: 0.02\n","iteration: 11930 loss: 0.0014 lr: 0.02\n","iteration: 11940 loss: 0.0013 lr: 0.02\n","iteration: 11950 loss: 0.0010 lr: 0.02\n","iteration: 11960 loss: 0.0013 lr: 0.02\n","iteration: 11970 loss: 0.0014 lr: 0.02\n","iteration: 11980 loss: 0.0014 lr: 0.02\n","iteration: 11990 loss: 0.0011 lr: 0.02\n","iteration: 12000 loss: 0.0011 lr: 0.02\n","iteration: 12010 loss: 0.0013 lr: 0.02\n","iteration: 12020 loss: 0.0012 lr: 0.02\n","iteration: 12030 loss: 0.0014 lr: 0.02\n","iteration: 12040 loss: 0.0015 lr: 0.02\n","iteration: 12050 loss: 0.0011 lr: 0.02\n","iteration: 12060 loss: 0.0013 lr: 0.02\n","iteration: 12070 loss: 0.0012 lr: 0.02\n","iteration: 12080 loss: 0.0013 lr: 0.02\n","iteration: 12090 loss: 0.0013 lr: 0.02\n","iteration: 12100 loss: 0.0015 lr: 0.02\n","iteration: 12110 loss: 0.0010 lr: 0.02\n","iteration: 12120 loss: 0.0015 lr: 0.02\n","iteration: 12130 loss: 0.0011 lr: 0.02\n","iteration: 12140 loss: 0.0016 lr: 0.02\n","iteration: 12150 loss: 0.0015 lr: 0.02\n","iteration: 12160 loss: 0.0016 lr: 0.02\n","iteration: 12170 loss: 0.0013 lr: 0.02\n","iteration: 12180 loss: 0.0018 lr: 0.02\n","iteration: 12190 loss: 0.0012 lr: 0.02\n","iteration: 12200 loss: 0.0012 lr: 0.02\n","iteration: 12210 loss: 0.0012 lr: 0.02\n","iteration: 12220 loss: 0.0013 lr: 0.02\n","iteration: 12230 loss: 0.0012 lr: 0.02\n","iteration: 12240 loss: 0.0012 lr: 0.02\n","iteration: 12250 loss: 0.0013 lr: 0.02\n","iteration: 12260 loss: 0.0012 lr: 0.02\n","iteration: 12270 loss: 0.0016 lr: 0.02\n","iteration: 12280 loss: 0.0015 lr: 0.02\n","iteration: 12290 loss: 0.0012 lr: 0.02\n","iteration: 12300 loss: 0.0016 lr: 0.02\n","iteration: 12310 loss: 0.0013 lr: 0.02\n","iteration: 12320 loss: 0.0021 lr: 0.02\n","iteration: 12330 loss: 0.0012 lr: 0.02\n","iteration: 12340 loss: 0.0012 lr: 0.02\n","iteration: 12350 loss: 0.0012 lr: 0.02\n","iteration: 12360 loss: 0.0014 lr: 0.02\n","iteration: 12370 loss: 0.0016 lr: 0.02\n","iteration: 12380 loss: 0.0016 lr: 0.02\n","iteration: 12390 loss: 0.0027 lr: 0.02\n","iteration: 12400 loss: 0.0013 lr: 0.02\n","iteration: 12410 loss: 0.0014 lr: 0.02\n","iteration: 12420 loss: 0.0015 lr: 0.02\n","iteration: 12430 loss: 0.0011 lr: 0.02\n","iteration: 12440 loss: 0.0012 lr: 0.02\n","iteration: 12450 loss: 0.0011 lr: 0.02\n","iteration: 12460 loss: 0.0011 lr: 0.02\n","iteration: 12470 loss: 0.0012 lr: 0.02\n","iteration: 12480 loss: 0.0016 lr: 0.02\n","iteration: 12490 loss: 0.0012 lr: 0.02\n","iteration: 12500 loss: 0.0013 lr: 0.02\n","iteration: 12510 loss: 0.0011 lr: 0.02\n","iteration: 12520 loss: 0.0013 lr: 0.02\n","iteration: 12530 loss: 0.0011 lr: 0.02\n","iteration: 12540 loss: 0.0016 lr: 0.02\n","iteration: 12550 loss: 0.0011 lr: 0.02\n","iteration: 12560 loss: 0.0011 lr: 0.02\n","iteration: 12570 loss: 0.0012 lr: 0.02\n","iteration: 12580 loss: 0.0011 lr: 0.02\n","iteration: 12590 loss: 0.0013 lr: 0.02\n","iteration: 12600 loss: 0.0013 lr: 0.02\n","iteration: 12610 loss: 0.0013 lr: 0.02\n","iteration: 12620 loss: 0.0012 lr: 0.02\n","iteration: 12630 loss: 0.0012 lr: 0.02\n","iteration: 12640 loss: 0.0013 lr: 0.02\n","iteration: 12650 loss: 0.0014 lr: 0.02\n","iteration: 12660 loss: 0.0014 lr: 0.02\n","iteration: 12670 loss: 0.0014 lr: 0.02\n","iteration: 12680 loss: 0.0013 lr: 0.02\n","iteration: 12690 loss: 0.0009 lr: 0.02\n","iteration: 12700 loss: 0.0011 lr: 0.02\n","iteration: 12710 loss: 0.0012 lr: 0.02\n","iteration: 12720 loss: 0.0010 lr: 0.02\n","iteration: 12730 loss: 0.0012 lr: 0.02\n","iteration: 12740 loss: 0.0012 lr: 0.02\n","iteration: 12750 loss: 0.0011 lr: 0.02\n","iteration: 12760 loss: 0.0016 lr: 0.02\n","iteration: 12770 loss: 0.0009 lr: 0.02\n","iteration: 12780 loss: 0.0014 lr: 0.02\n","iteration: 12790 loss: 0.0014 lr: 0.02\n","iteration: 12800 loss: 0.0010 lr: 0.02\n","iteration: 12810 loss: 0.0014 lr: 0.02\n","iteration: 12820 loss: 0.0014 lr: 0.02\n","iteration: 12830 loss: 0.0010 lr: 0.02\n","iteration: 12840 loss: 0.0013 lr: 0.02\n","iteration: 12850 loss: 0.0011 lr: 0.02\n","iteration: 12860 loss: 0.0014 lr: 0.02\n","iteration: 12870 loss: 0.0015 lr: 0.02\n","iteration: 12880 loss: 0.0012 lr: 0.02\n","iteration: 12890 loss: 0.0013 lr: 0.02\n","iteration: 12900 loss: 0.0011 lr: 0.02\n","iteration: 12910 loss: 0.0011 lr: 0.02\n","iteration: 12920 loss: 0.0013 lr: 0.02\n","iteration: 12930 loss: 0.0014 lr: 0.02\n","iteration: 12940 loss: 0.0015 lr: 0.02\n","iteration: 12950 loss: 0.0011 lr: 0.02\n","iteration: 12960 loss: 0.0016 lr: 0.02\n","iteration: 12970 loss: 0.0015 lr: 0.02\n","iteration: 12980 loss: 0.0013 lr: 0.02\n","iteration: 12990 loss: 0.0013 lr: 0.02\n","iteration: 13000 loss: 0.0015 lr: 0.02\n","iteration: 13010 loss: 0.0011 lr: 0.02\n","iteration: 13020 loss: 0.0009 lr: 0.02\n","iteration: 13030 loss: 0.0013 lr: 0.02\n","iteration: 13040 loss: 0.0011 lr: 0.02\n","iteration: 13050 loss: 0.0014 lr: 0.02\n","iteration: 13060 loss: 0.0015 lr: 0.02\n","iteration: 13070 loss: 0.0014 lr: 0.02\n","iteration: 13080 loss: 0.0013 lr: 0.02\n","iteration: 13090 loss: 0.0013 lr: 0.02\n","iteration: 13100 loss: 0.0015 lr: 0.02\n","iteration: 13110 loss: 0.0015 lr: 0.02\n","iteration: 13120 loss: 0.0014 lr: 0.02\n","iteration: 13130 loss: 0.0013 lr: 0.02\n","iteration: 13140 loss: 0.0012 lr: 0.02\n","iteration: 13150 loss: 0.0011 lr: 0.02\n","iteration: 13160 loss: 0.0012 lr: 0.02\n","iteration: 13170 loss: 0.0012 lr: 0.02\n","iteration: 13180 loss: 0.0013 lr: 0.02\n","iteration: 13190 loss: 0.0012 lr: 0.02\n","iteration: 13200 loss: 0.0011 lr: 0.02\n","iteration: 13210 loss: 0.0010 lr: 0.02\n","iteration: 13220 loss: 0.0011 lr: 0.02\n","iteration: 13230 loss: 0.0011 lr: 0.02\n","iteration: 13240 loss: 0.0012 lr: 0.02\n","iteration: 13250 loss: 0.0012 lr: 0.02\n","iteration: 13260 loss: 0.0013 lr: 0.02\n","iteration: 13270 loss: 0.0011 lr: 0.02\n","iteration: 13280 loss: 0.0010 lr: 0.02\n","iteration: 13290 loss: 0.0011 lr: 0.02\n","iteration: 13300 loss: 0.0016 lr: 0.02\n","iteration: 13310 loss: 0.0012 lr: 0.02\n","iteration: 13320 loss: 0.0012 lr: 0.02\n","iteration: 13330 loss: 0.0012 lr: 0.02\n","iteration: 13340 loss: 0.0010 lr: 0.02\n","iteration: 13350 loss: 0.0013 lr: 0.02\n","iteration: 13360 loss: 0.0016 lr: 0.02\n","iteration: 13370 loss: 0.0009 lr: 0.02\n","iteration: 13380 loss: 0.0014 lr: 0.02\n","iteration: 13390 loss: 0.0013 lr: 0.02\n","iteration: 13400 loss: 0.0013 lr: 0.02\n","iteration: 13410 loss: 0.0016 lr: 0.02\n","iteration: 13420 loss: 0.0015 lr: 0.02\n","iteration: 13430 loss: 0.0012 lr: 0.02\n","iteration: 13440 loss: 0.0014 lr: 0.02\n","iteration: 13450 loss: 0.0014 lr: 0.02\n","iteration: 13460 loss: 0.0014 lr: 0.02\n","iteration: 13470 loss: 0.0013 lr: 0.02\n","iteration: 13480 loss: 0.0014 lr: 0.02\n","iteration: 13490 loss: 0.0010 lr: 0.02\n","iteration: 13500 loss: 0.0014 lr: 0.02\n","iteration: 13510 loss: 0.0013 lr: 0.02\n","iteration: 13520 loss: 0.0019 lr: 0.02\n","iteration: 13530 loss: 0.0015 lr: 0.02\n","iteration: 13540 loss: 0.0012 lr: 0.02\n","iteration: 13550 loss: 0.0012 lr: 0.02\n","iteration: 13560 loss: 0.0012 lr: 0.02\n","iteration: 13570 loss: 0.0015 lr: 0.02\n","iteration: 13580 loss: 0.0014 lr: 0.02\n","iteration: 13590 loss: 0.0013 lr: 0.02\n","iteration: 13600 loss: 0.0014 lr: 0.02\n","iteration: 13610 loss: 0.0014 lr: 0.02\n","iteration: 13620 loss: 0.0013 lr: 0.02\n","iteration: 13630 loss: 0.0013 lr: 0.02\n","iteration: 13640 loss: 0.0012 lr: 0.02\n","iteration: 13650 loss: 0.0012 lr: 0.02\n","iteration: 13660 loss: 0.0012 lr: 0.02\n","iteration: 13670 loss: 0.0010 lr: 0.02\n","iteration: 13680 loss: 0.0016 lr: 0.02\n","iteration: 13690 loss: 0.0011 lr: 0.02\n","iteration: 13700 loss: 0.0011 lr: 0.02\n","iteration: 13710 loss: 0.0010 lr: 0.02\n","iteration: 13720 loss: 0.0013 lr: 0.02\n","iteration: 13730 loss: 0.0012 lr: 0.02\n","iteration: 13740 loss: 0.0012 lr: 0.02\n","iteration: 13750 loss: 0.0011 lr: 0.02\n","iteration: 13760 loss: 0.0012 lr: 0.02\n","iteration: 13770 loss: 0.0011 lr: 0.02\n","iteration: 13780 loss: 0.0012 lr: 0.02\n","iteration: 13790 loss: 0.0011 lr: 0.02\n","iteration: 13800 loss: 0.0013 lr: 0.02\n","iteration: 13810 loss: 0.0013 lr: 0.02\n","iteration: 13820 loss: 0.0015 lr: 0.02\n","iteration: 13830 loss: 0.0011 lr: 0.02\n","iteration: 13840 loss: 0.0017 lr: 0.02\n","iteration: 13850 loss: 0.0012 lr: 0.02\n","iteration: 13860 loss: 0.0011 lr: 0.02\n","iteration: 13870 loss: 0.0012 lr: 0.02\n","iteration: 13880 loss: 0.0014 lr: 0.02\n","iteration: 13890 loss: 0.0013 lr: 0.02\n","iteration: 13900 loss: 0.0009 lr: 0.02\n","iteration: 13910 loss: 0.0019 lr: 0.02\n","iteration: 13920 loss: 0.0018 lr: 0.02\n","iteration: 13930 loss: 0.0011 lr: 0.02\n","iteration: 13940 loss: 0.0018 lr: 0.02\n","iteration: 13950 loss: 0.0012 lr: 0.02\n","iteration: 13960 loss: 0.0012 lr: 0.02\n","iteration: 13970 loss: 0.0013 lr: 0.02\n","iteration: 13980 loss: 0.0012 lr: 0.02\n","iteration: 13990 loss: 0.0012 lr: 0.02\n","iteration: 14000 loss: 0.0015 lr: 0.02\n","iteration: 14010 loss: 0.0011 lr: 0.02\n","iteration: 14020 loss: 0.0010 lr: 0.02\n","iteration: 14030 loss: 0.0012 lr: 0.02\n","iteration: 14040 loss: 0.0009 lr: 0.02\n","iteration: 14050 loss: 0.0015 lr: 0.02\n","iteration: 14060 loss: 0.0009 lr: 0.02\n","iteration: 14070 loss: 0.0014 lr: 0.02\n","iteration: 14080 loss: 0.0019 lr: 0.02\n","iteration: 14090 loss: 0.0013 lr: 0.02\n","iteration: 14100 loss: 0.0014 lr: 0.02\n","iteration: 14110 loss: 0.0010 lr: 0.02\n","iteration: 14120 loss: 0.0014 lr: 0.02\n","iteration: 14130 loss: 0.0012 lr: 0.02\n","iteration: 14140 loss: 0.0011 lr: 0.02\n","iteration: 14150 loss: 0.0015 lr: 0.02\n","iteration: 14160 loss: 0.0015 lr: 0.02\n","iteration: 14170 loss: 0.0012 lr: 0.02\n","iteration: 14180 loss: 0.0012 lr: 0.02\n","iteration: 14190 loss: 0.0013 lr: 0.02\n","iteration: 14200 loss: 0.0013 lr: 0.02\n","iteration: 14210 loss: 0.0012 lr: 0.02\n","iteration: 14220 loss: 0.0013 lr: 0.02\n","iteration: 14230 loss: 0.0013 lr: 0.02\n","iteration: 14240 loss: 0.0013 lr: 0.02\n","iteration: 14250 loss: 0.0015 lr: 0.02\n","iteration: 14260 loss: 0.0013 lr: 0.02\n","iteration: 14270 loss: 0.0011 lr: 0.02\n","iteration: 14280 loss: 0.0016 lr: 0.02\n","iteration: 14290 loss: 0.0017 lr: 0.02\n","iteration: 14300 loss: 0.0013 lr: 0.02\n","iteration: 14310 loss: 0.0011 lr: 0.02\n","iteration: 14320 loss: 0.0015 lr: 0.02\n","iteration: 14330 loss: 0.0011 lr: 0.02\n","iteration: 14340 loss: 0.0012 lr: 0.02\n","iteration: 14350 loss: 0.0012 lr: 0.02\n","iteration: 14360 loss: 0.0012 lr: 0.02\n","iteration: 14370 loss: 0.0013 lr: 0.02\n","iteration: 14380 loss: 0.0014 lr: 0.02\n","iteration: 14390 loss: 0.0013 lr: 0.02\n","iteration: 14400 loss: 0.0013 lr: 0.02\n","iteration: 14410 loss: 0.0013 lr: 0.02\n","iteration: 14420 loss: 0.0013 lr: 0.02\n","iteration: 14430 loss: 0.0014 lr: 0.02\n","iteration: 14440 loss: 0.0014 lr: 0.02\n","iteration: 14450 loss: 0.0013 lr: 0.02\n","iteration: 14460 loss: 0.0013 lr: 0.02\n","iteration: 14470 loss: 0.0014 lr: 0.02\n","iteration: 14480 loss: 0.0011 lr: 0.02\n","iteration: 14490 loss: 0.0015 lr: 0.02\n","iteration: 14500 loss: 0.0012 lr: 0.02\n","iteration: 14510 loss: 0.0016 lr: 0.02\n","iteration: 14520 loss: 0.0011 lr: 0.02\n","iteration: 14530 loss: 0.0013 lr: 0.02\n","iteration: 14540 loss: 0.0011 lr: 0.02\n","iteration: 14550 loss: 0.0013 lr: 0.02\n","iteration: 14560 loss: 0.0014 lr: 0.02\n","iteration: 14570 loss: 0.0011 lr: 0.02\n","iteration: 14580 loss: 0.0016 lr: 0.02\n","iteration: 14590 loss: 0.0016 lr: 0.02\n","iteration: 14600 loss: 0.0014 lr: 0.02\n","iteration: 14610 loss: 0.0016 lr: 0.02\n","iteration: 14620 loss: 0.0013 lr: 0.02\n","iteration: 14630 loss: 0.0016 lr: 0.02\n","iteration: 14640 loss: 0.0015 lr: 0.02\n","iteration: 14650 loss: 0.0013 lr: 0.02\n","iteration: 14660 loss: 0.0016 lr: 0.02\n","iteration: 14670 loss: 0.0012 lr: 0.02\n","iteration: 14680 loss: 0.0011 lr: 0.02\n","iteration: 14690 loss: 0.0010 lr: 0.02\n","iteration: 14700 loss: 0.0015 lr: 0.02\n","iteration: 14710 loss: 0.0012 lr: 0.02\n","iteration: 14720 loss: 0.0016 lr: 0.02\n","iteration: 14730 loss: 0.0011 lr: 0.02\n","iteration: 14740 loss: 0.0012 lr: 0.02\n","iteration: 14750 loss: 0.0011 lr: 0.02\n","iteration: 14760 loss: 0.0012 lr: 0.02\n","iteration: 14770 loss: 0.0016 lr: 0.02\n","iteration: 14780 loss: 0.0018 lr: 0.02\n","iteration: 14790 loss: 0.0013 lr: 0.02\n","iteration: 14800 loss: 0.0014 lr: 0.02\n","iteration: 14810 loss: 0.0010 lr: 0.02\n","iteration: 14820 loss: 0.0013 lr: 0.02\n","iteration: 14830 loss: 0.0014 lr: 0.02\n","iteration: 14840 loss: 0.0013 lr: 0.02\n","iteration: 14850 loss: 0.0013 lr: 0.02\n","iteration: 14860 loss: 0.0012 lr: 0.02\n","iteration: 14870 loss: 0.0013 lr: 0.02\n","iteration: 14880 loss: 0.0013 lr: 0.02\n","iteration: 14890 loss: 0.0013 lr: 0.02\n","iteration: 14900 loss: 0.0012 lr: 0.02\n","iteration: 14910 loss: 0.0012 lr: 0.02\n","iteration: 14920 loss: 0.0010 lr: 0.02\n","iteration: 14930 loss: 0.0014 lr: 0.02\n","iteration: 14940 loss: 0.0011 lr: 0.02\n","iteration: 14950 loss: 0.0012 lr: 0.02\n","iteration: 14960 loss: 0.0014 lr: 0.02\n","iteration: 14970 loss: 0.0011 lr: 0.02\n","iteration: 14980 loss: 0.0014 lr: 0.02\n","iteration: 14990 loss: 0.0014 lr: 0.02\n","iteration: 15000 loss: 0.0016 lr: 0.02\n","iteration: 15010 loss: 0.0017 lr: 0.02\n","iteration: 15020 loss: 0.0014 lr: 0.02\n","iteration: 15030 loss: 0.0015 lr: 0.02\n","iteration: 15040 loss: 0.0014 lr: 0.02\n","iteration: 15050 loss: 0.0011 lr: 0.02\n","iteration: 15060 loss: 0.0017 lr: 0.02\n","iteration: 15070 loss: 0.0012 lr: 0.02\n","iteration: 15080 loss: 0.0010 lr: 0.02\n","iteration: 15090 loss: 0.0012 lr: 0.02\n","iteration: 15100 loss: 0.0010 lr: 0.02\n","iteration: 15110 loss: 0.0009 lr: 0.02\n","iteration: 15120 loss: 0.0013 lr: 0.02\n","iteration: 15130 loss: 0.0011 lr: 0.02\n","iteration: 15140 loss: 0.0015 lr: 0.02\n","iteration: 15150 loss: 0.0013 lr: 0.02\n","iteration: 15160 loss: 0.0010 lr: 0.02\n","iteration: 15170 loss: 0.0013 lr: 0.02\n","iteration: 15180 loss: 0.0012 lr: 0.02\n","iteration: 15190 loss: 0.0011 lr: 0.02\n","iteration: 15200 loss: 0.0011 lr: 0.02\n","iteration: 15210 loss: 0.0013 lr: 0.02\n","iteration: 15220 loss: 0.0011 lr: 0.02\n","iteration: 15230 loss: 0.0013 lr: 0.02\n","iteration: 15240 loss: 0.0013 lr: 0.02\n","iteration: 15250 loss: 0.0010 lr: 0.02\n","iteration: 15260 loss: 0.0013 lr: 0.02\n","iteration: 15270 loss: 0.0012 lr: 0.02\n","iteration: 15280 loss: 0.0014 lr: 0.02\n","iteration: 15290 loss: 0.0010 lr: 0.02\n","iteration: 15300 loss: 0.0014 lr: 0.02\n","iteration: 15310 loss: 0.0014 lr: 0.02\n","iteration: 15320 loss: 0.0012 lr: 0.02\n","iteration: 15330 loss: 0.0012 lr: 0.02\n","iteration: 15340 loss: 0.0009 lr: 0.02\n","iteration: 15350 loss: 0.0016 lr: 0.02\n","iteration: 15360 loss: 0.0012 lr: 0.02\n","iteration: 15370 loss: 0.0015 lr: 0.02\n","iteration: 15380 loss: 0.0014 lr: 0.02\n","iteration: 15390 loss: 0.0011 lr: 0.02\n","iteration: 15400 loss: 0.0011 lr: 0.02\n","iteration: 15410 loss: 0.0013 lr: 0.02\n","iteration: 15420 loss: 0.0015 lr: 0.02\n","iteration: 15430 loss: 0.0009 lr: 0.02\n","iteration: 15440 loss: 0.0012 lr: 0.02\n","iteration: 15450 loss: 0.0014 lr: 0.02\n","iteration: 15460 loss: 0.0011 lr: 0.02\n","iteration: 15470 loss: 0.0013 lr: 0.02\n","iteration: 15480 loss: 0.0017 lr: 0.02\n","iteration: 15490 loss: 0.0012 lr: 0.02\n","iteration: 15500 loss: 0.0012 lr: 0.02\n","iteration: 15510 loss: 0.0011 lr: 0.02\n","iteration: 15520 loss: 0.0013 lr: 0.02\n","iteration: 15530 loss: 0.0015 lr: 0.02\n","iteration: 15540 loss: 0.0013 lr: 0.02\n","iteration: 15550 loss: 0.0012 lr: 0.02\n","iteration: 15560 loss: 0.0015 lr: 0.02\n","iteration: 15570 loss: 0.0013 lr: 0.02\n","iteration: 15580 loss: 0.0013 lr: 0.02\n","iteration: 15590 loss: 0.0014 lr: 0.02\n","iteration: 15600 loss: 0.0016 lr: 0.02\n","iteration: 15610 loss: 0.0015 lr: 0.02\n","iteration: 15620 loss: 0.0012 lr: 0.02\n","iteration: 15630 loss: 0.0016 lr: 0.02\n","iteration: 15640 loss: 0.0014 lr: 0.02\n","iteration: 15650 loss: 0.0013 lr: 0.02\n","iteration: 15660 loss: 0.0015 lr: 0.02\n","iteration: 15670 loss: 0.0021 lr: 0.02\n","iteration: 15680 loss: 0.0016 lr: 0.02\n","iteration: 15690 loss: 0.0018 lr: 0.02\n","iteration: 15700 loss: 0.0014 lr: 0.02\n","iteration: 15710 loss: 0.0014 lr: 0.02\n","iteration: 15720 loss: 0.0016 lr: 0.02\n","iteration: 15730 loss: 0.0012 lr: 0.02\n","iteration: 15740 loss: 0.0017 lr: 0.02\n","iteration: 15750 loss: 0.0011 lr: 0.02\n","iteration: 15760 loss: 0.0012 lr: 0.02\n","iteration: 15770 loss: 0.0012 lr: 0.02\n","iteration: 15780 loss: 0.0013 lr: 0.02\n","iteration: 15790 loss: 0.0014 lr: 0.02\n","iteration: 15800 loss: 0.0013 lr: 0.02\n","iteration: 15810 loss: 0.0012 lr: 0.02\n","iteration: 15820 loss: 0.0011 lr: 0.02\n","iteration: 15830 loss: 0.0014 lr: 0.02\n","iteration: 15840 loss: 0.0011 lr: 0.02\n","iteration: 15850 loss: 0.0012 lr: 0.02\n","iteration: 15860 loss: 0.0013 lr: 0.02\n","iteration: 15870 loss: 0.0014 lr: 0.02\n","iteration: 15880 loss: 0.0018 lr: 0.02\n","iteration: 15890 loss: 0.0013 lr: 0.02\n","iteration: 15900 loss: 0.0012 lr: 0.02\n","iteration: 15910 loss: 0.0010 lr: 0.02\n","iteration: 15920 loss: 0.0013 lr: 0.02\n","iteration: 15930 loss: 0.0014 lr: 0.02\n","iteration: 15940 loss: 0.0013 lr: 0.02\n","iteration: 15950 loss: 0.0011 lr: 0.02\n","iteration: 15960 loss: 0.0009 lr: 0.02\n","iteration: 15970 loss: 0.0012 lr: 0.02\n","iteration: 15980 loss: 0.0012 lr: 0.02\n","iteration: 15990 loss: 0.0012 lr: 0.02\n","iteration: 16000 loss: 0.0015 lr: 0.02\n","iteration: 16010 loss: 0.0015 lr: 0.02\n","iteration: 16020 loss: 0.0013 lr: 0.02\n","iteration: 16030 loss: 0.0011 lr: 0.02\n","iteration: 16040 loss: 0.0013 lr: 0.02\n","iteration: 16050 loss: 0.0012 lr: 0.02\n","iteration: 16060 loss: 0.0018 lr: 0.02\n","iteration: 16070 loss: 0.0010 lr: 0.02\n","iteration: 16080 loss: 0.0013 lr: 0.02\n","iteration: 16090 loss: 0.0011 lr: 0.02\n","iteration: 16100 loss: 0.0012 lr: 0.02\n","iteration: 16110 loss: 0.0012 lr: 0.02\n","iteration: 16120 loss: 0.0013 lr: 0.02\n","iteration: 16130 loss: 0.0015 lr: 0.02\n","iteration: 16140 loss: 0.0012 lr: 0.02\n","iteration: 16150 loss: 0.0015 lr: 0.02\n","iteration: 16160 loss: 0.0012 lr: 0.02\n","iteration: 16170 loss: 0.0014 lr: 0.02\n","iteration: 16180 loss: 0.0011 lr: 0.02\n","iteration: 16190 loss: 0.0010 lr: 0.02\n","iteration: 16200 loss: 0.0013 lr: 0.02\n","iteration: 16210 loss: 0.0012 lr: 0.02\n","iteration: 16220 loss: 0.0014 lr: 0.02\n","iteration: 16230 loss: 0.0014 lr: 0.02\n","iteration: 16240 loss: 0.0015 lr: 0.02\n","iteration: 16250 loss: 0.0012 lr: 0.02\n","iteration: 16260 loss: 0.0012 lr: 0.02\n","iteration: 16270 loss: 0.0014 lr: 0.02\n","iteration: 16280 loss: 0.0013 lr: 0.02\n","iteration: 16290 loss: 0.0012 lr: 0.02\n","iteration: 16300 loss: 0.0011 lr: 0.02\n","iteration: 16310 loss: 0.0014 lr: 0.02\n","iteration: 16320 loss: 0.0011 lr: 0.02\n","iteration: 16330 loss: 0.0015 lr: 0.02\n","iteration: 16340 loss: 0.0013 lr: 0.02\n","iteration: 16350 loss: 0.0012 lr: 0.02\n","iteration: 16360 loss: 0.0014 lr: 0.02\n","iteration: 16370 loss: 0.0014 lr: 0.02\n","iteration: 16380 loss: 0.0015 lr: 0.02\n","iteration: 16390 loss: 0.0015 lr: 0.02\n","iteration: 16400 loss: 0.0010 lr: 0.02\n","iteration: 16410 loss: 0.0012 lr: 0.02\n","iteration: 16420 loss: 0.0013 lr: 0.02\n","iteration: 16430 loss: 0.0014 lr: 0.02\n","iteration: 16440 loss: 0.0013 lr: 0.02\n","iteration: 16450 loss: 0.0013 lr: 0.02\n","iteration: 16460 loss: 0.0014 lr: 0.02\n","iteration: 16470 loss: 0.0011 lr: 0.02\n","iteration: 16480 loss: 0.0016 lr: 0.02\n","iteration: 16490 loss: 0.0011 lr: 0.02\n","iteration: 16500 loss: 0.0014 lr: 0.02\n","iteration: 16510 loss: 0.0011 lr: 0.02\n","iteration: 16520 loss: 0.0013 lr: 0.02\n","iteration: 16530 loss: 0.0012 lr: 0.02\n","iteration: 16540 loss: 0.0012 lr: 0.02\n","iteration: 16550 loss: 0.0010 lr: 0.02\n","iteration: 16560 loss: 0.0014 lr: 0.02\n","iteration: 16570 loss: 0.0014 lr: 0.02\n","iteration: 16580 loss: 0.0013 lr: 0.02\n","iteration: 16590 loss: 0.0013 lr: 0.02\n","iteration: 16600 loss: 0.0013 lr: 0.02\n","iteration: 16610 loss: 0.0012 lr: 0.02\n","iteration: 16620 loss: 0.0012 lr: 0.02\n","iteration: 16630 loss: 0.0016 lr: 0.02\n","iteration: 16640 loss: 0.0011 lr: 0.02\n","iteration: 16650 loss: 0.0011 lr: 0.02\n","iteration: 16660 loss: 0.0012 lr: 0.02\n","iteration: 16670 loss: 0.0015 lr: 0.02\n","iteration: 16680 loss: 0.0011 lr: 0.02\n","iteration: 16690 loss: 0.0012 lr: 0.02\n","iteration: 16700 loss: 0.0013 lr: 0.02\n","iteration: 16710 loss: 0.0013 lr: 0.02\n","iteration: 16720 loss: 0.0012 lr: 0.02\n","iteration: 16730 loss: 0.0011 lr: 0.02\n","iteration: 16740 loss: 0.0013 lr: 0.02\n","iteration: 16750 loss: 0.0012 lr: 0.02\n","iteration: 16760 loss: 0.0011 lr: 0.02\n","iteration: 16770 loss: 0.0014 lr: 0.02\n","iteration: 16780 loss: 0.0011 lr: 0.02\n","iteration: 16790 loss: 0.0013 lr: 0.02\n","iteration: 16800 loss: 0.0012 lr: 0.02\n","iteration: 16810 loss: 0.0013 lr: 0.02\n","iteration: 16820 loss: 0.0013 lr: 0.02\n","iteration: 16830 loss: 0.0015 lr: 0.02\n","iteration: 16840 loss: 0.0011 lr: 0.02\n","iteration: 16850 loss: 0.0011 lr: 0.02\n","iteration: 16860 loss: 0.0011 lr: 0.02\n","iteration: 16870 loss: 0.0014 lr: 0.02\n","iteration: 16880 loss: 0.0018 lr: 0.02\n","iteration: 16890 loss: 0.0012 lr: 0.02\n","iteration: 16900 loss: 0.0018 lr: 0.02\n","iteration: 16910 loss: 0.0014 lr: 0.02\n","iteration: 16920 loss: 0.0012 lr: 0.02\n","iteration: 16930 loss: 0.0012 lr: 0.02\n","iteration: 16940 loss: 0.0012 lr: 0.02\n","iteration: 16950 loss: 0.0011 lr: 0.02\n","iteration: 16960 loss: 0.0011 lr: 0.02\n","iteration: 16970 loss: 0.0016 lr: 0.02\n","iteration: 16980 loss: 0.0012 lr: 0.02\n","iteration: 16990 loss: 0.0010 lr: 0.02\n","iteration: 17000 loss: 0.0015 lr: 0.02\n","iteration: 17010 loss: 0.0016 lr: 0.02\n","iteration: 17020 loss: 0.0014 lr: 0.02\n","iteration: 17030 loss: 0.0012 lr: 0.02\n","iteration: 17040 loss: 0.0018 lr: 0.02\n","iteration: 17050 loss: 0.0012 lr: 0.02\n","iteration: 17060 loss: 0.0015 lr: 0.02\n","iteration: 17070 loss: 0.0012 lr: 0.02\n","iteration: 17080 loss: 0.0012 lr: 0.02\n","iteration: 17090 loss: 0.0013 lr: 0.02\n","iteration: 17100 loss: 0.0016 lr: 0.02\n","iteration: 17110 loss: 0.0014 lr: 0.02\n","iteration: 17120 loss: 0.0014 lr: 0.02\n","iteration: 17130 loss: 0.0016 lr: 0.02\n","iteration: 17140 loss: 0.0013 lr: 0.02\n","iteration: 17150 loss: 0.0012 lr: 0.02\n","iteration: 17160 loss: 0.0011 lr: 0.02\n","iteration: 17170 loss: 0.0011 lr: 0.02\n","iteration: 17180 loss: 0.0011 lr: 0.02\n","iteration: 17190 loss: 0.0014 lr: 0.02\n","iteration: 17200 loss: 0.0012 lr: 0.02\n","iteration: 17210 loss: 0.0012 lr: 0.02\n","iteration: 17220 loss: 0.0019 lr: 0.02\n","iteration: 17230 loss: 0.0014 lr: 0.02\n","iteration: 17240 loss: 0.0015 lr: 0.02\n","iteration: 17250 loss: 0.0016 lr: 0.02\n","iteration: 17260 loss: 0.0013 lr: 0.02\n","iteration: 17270 loss: 0.0012 lr: 0.02\n","iteration: 17280 loss: 0.0016 lr: 0.02\n","iteration: 17290 loss: 0.0019 lr: 0.02\n","iteration: 17300 loss: 0.0013 lr: 0.02\n","iteration: 17310 loss: 0.0012 lr: 0.02\n","iteration: 17320 loss: 0.0012 lr: 0.02\n","iteration: 17330 loss: 0.0013 lr: 0.02\n","iteration: 17340 loss: 0.0015 lr: 0.02\n","iteration: 17350 loss: 0.0015 lr: 0.02\n","iteration: 17360 loss: 0.0013 lr: 0.02\n","iteration: 17370 loss: 0.0013 lr: 0.02\n","iteration: 17380 loss: 0.0013 lr: 0.02\n","iteration: 17390 loss: 0.0012 lr: 0.02\n","iteration: 17400 loss: 0.0013 lr: 0.02\n","iteration: 17410 loss: 0.0011 lr: 0.02\n","iteration: 17420 loss: 0.0012 lr: 0.02\n","iteration: 17430 loss: 0.0010 lr: 0.02\n","iteration: 17440 loss: 0.0013 lr: 0.02\n","iteration: 17450 loss: 0.0014 lr: 0.02\n","iteration: 17460 loss: 0.0016 lr: 0.02\n","iteration: 17470 loss: 0.0013 lr: 0.02\n","iteration: 17480 loss: 0.0013 lr: 0.02\n","iteration: 17490 loss: 0.0012 lr: 0.02\n","iteration: 17500 loss: 0.0013 lr: 0.02\n","iteration: 17510 loss: 0.0009 lr: 0.02\n","iteration: 17520 loss: 0.0013 lr: 0.02\n","iteration: 17530 loss: 0.0012 lr: 0.02\n","iteration: 17540 loss: 0.0012 lr: 0.02\n","iteration: 17550 loss: 0.0013 lr: 0.02\n","iteration: 17560 loss: 0.0013 lr: 0.02\n","iteration: 17570 loss: 0.0013 lr: 0.02\n","iteration: 17580 loss: 0.0012 lr: 0.02\n","iteration: 17590 loss: 0.0011 lr: 0.02\n","iteration: 17600 loss: 0.0010 lr: 0.02\n","iteration: 17610 loss: 0.0013 lr: 0.02\n","iteration: 17620 loss: 0.0016 lr: 0.02\n","iteration: 17630 loss: 0.0015 lr: 0.02\n","iteration: 17640 loss: 0.0017 lr: 0.02\n","iteration: 17650 loss: 0.0012 lr: 0.02\n","iteration: 17660 loss: 0.0016 lr: 0.02\n","iteration: 17670 loss: 0.0012 lr: 0.02\n","iteration: 17680 loss: 0.0011 lr: 0.02\n","iteration: 17690 loss: 0.0015 lr: 0.02\n","iteration: 17700 loss: 0.0011 lr: 0.02\n","iteration: 17710 loss: 0.0014 lr: 0.02\n","iteration: 17720 loss: 0.0012 lr: 0.02\n","iteration: 17730 loss: 0.0013 lr: 0.02\n","iteration: 17740 loss: 0.0016 lr: 0.02\n","iteration: 17750 loss: 0.0016 lr: 0.02\n","iteration: 17760 loss: 0.0014 lr: 0.02\n","iteration: 17770 loss: 0.0014 lr: 0.02\n","iteration: 17780 loss: 0.0012 lr: 0.02\n","iteration: 17790 loss: 0.0013 lr: 0.02\n","iteration: 17800 loss: 0.0012 lr: 0.02\n","iteration: 17810 loss: 0.0015 lr: 0.02\n","iteration: 17820 loss: 0.0011 lr: 0.02\n","iteration: 17830 loss: 0.0014 lr: 0.02\n","iteration: 17840 loss: 0.0017 lr: 0.02\n","iteration: 17850 loss: 0.0014 lr: 0.02\n","iteration: 17860 loss: 0.0013 lr: 0.02\n","iteration: 17870 loss: 0.0012 lr: 0.02\n","iteration: 17880 loss: 0.0012 lr: 0.02\n","iteration: 17890 loss: 0.0013 lr: 0.02\n","iteration: 17900 loss: 0.0013 lr: 0.02\n","iteration: 17910 loss: 0.0014 lr: 0.02\n","iteration: 17920 loss: 0.0013 lr: 0.02\n","iteration: 17930 loss: 0.0012 lr: 0.02\n","iteration: 17940 loss: 0.0011 lr: 0.02\n","iteration: 17950 loss: 0.0011 lr: 0.02\n","iteration: 17960 loss: 0.0010 lr: 0.02\n","iteration: 17970 loss: 0.0012 lr: 0.02\n","iteration: 17980 loss: 0.0013 lr: 0.02\n","iteration: 17990 loss: 0.0011 lr: 0.02\n","iteration: 18000 loss: 0.0011 lr: 0.02\n","iteration: 18010 loss: 0.0012 lr: 0.02\n","iteration: 18020 loss: 0.0018 lr: 0.02\n","iteration: 18030 loss: 0.0010 lr: 0.02\n","iteration: 18040 loss: 0.0009 lr: 0.02\n","iteration: 18050 loss: 0.0012 lr: 0.02\n","iteration: 18060 loss: 0.0013 lr: 0.02\n","iteration: 18070 loss: 0.0014 lr: 0.02\n","iteration: 18080 loss: 0.0013 lr: 0.02\n","iteration: 18090 loss: 0.0012 lr: 0.02\n","iteration: 18100 loss: 0.0013 lr: 0.02\n","iteration: 18110 loss: 0.0012 lr: 0.02\n","iteration: 18120 loss: 0.0017 lr: 0.02\n","iteration: 18130 loss: 0.0013 lr: 0.02\n","iteration: 18140 loss: 0.0011 lr: 0.02\n","iteration: 18150 loss: 0.0014 lr: 0.02\n","iteration: 18160 loss: 0.0012 lr: 0.02\n","iteration: 18170 loss: 0.0013 lr: 0.02\n","iteration: 18180 loss: 0.0015 lr: 0.02\n","iteration: 18190 loss: 0.0013 lr: 0.02\n","iteration: 18200 loss: 0.0011 lr: 0.02\n","iteration: 18210 loss: 0.0011 lr: 0.02\n","iteration: 18220 loss: 0.0012 lr: 0.02\n","iteration: 18230 loss: 0.0011 lr: 0.02\n","iteration: 18240 loss: 0.0011 lr: 0.02\n","iteration: 18250 loss: 0.0016 lr: 0.02\n","iteration: 18260 loss: 0.0010 lr: 0.02\n","iteration: 18270 loss: 0.0013 lr: 0.02\n","iteration: 18280 loss: 0.0012 lr: 0.02\n","iteration: 18290 loss: 0.0011 lr: 0.02\n","iteration: 18300 loss: 0.0014 lr: 0.02\n","iteration: 18310 loss: 0.0014 lr: 0.02\n","iteration: 18320 loss: 0.0011 lr: 0.02\n","iteration: 18330 loss: 0.0013 lr: 0.02\n","iteration: 18340 loss: 0.0014 lr: 0.02\n","iteration: 18350 loss: 0.0015 lr: 0.02\n","iteration: 18360 loss: 0.0013 lr: 0.02\n","iteration: 18370 loss: 0.0014 lr: 0.02\n","iteration: 18380 loss: 0.0012 lr: 0.02\n","iteration: 18390 loss: 0.0013 lr: 0.02\n","iteration: 18400 loss: 0.0013 lr: 0.02\n","iteration: 18410 loss: 0.0011 lr: 0.02\n","iteration: 18420 loss: 0.0014 lr: 0.02\n","iteration: 18430 loss: 0.0011 lr: 0.02\n","iteration: 18440 loss: 0.0012 lr: 0.02\n","iteration: 18450 loss: 0.0009 lr: 0.02\n","iteration: 18460 loss: 0.0016 lr: 0.02\n","iteration: 18470 loss: 0.0011 lr: 0.02\n","iteration: 18480 loss: 0.0009 lr: 0.02\n","iteration: 18490 loss: 0.0013 lr: 0.02\n","iteration: 18500 loss: 0.0010 lr: 0.02\n","iteration: 18510 loss: 0.0018 lr: 0.02\n","iteration: 18520 loss: 0.0014 lr: 0.02\n","iteration: 18530 loss: 0.0013 lr: 0.02\n","iteration: 18540 loss: 0.0015 lr: 0.02\n","iteration: 18550 loss: 0.0013 lr: 0.02\n","iteration: 18560 loss: 0.0013 lr: 0.02\n","iteration: 18570 loss: 0.0015 lr: 0.02\n","iteration: 18580 loss: 0.0011 lr: 0.02\n","iteration: 18590 loss: 0.0011 lr: 0.02\n","iteration: 18600 loss: 0.0014 lr: 0.02\n","iteration: 18610 loss: 0.0013 lr: 0.02\n","iteration: 18620 loss: 0.0016 lr: 0.02\n","iteration: 18630 loss: 0.0010 lr: 0.02\n","iteration: 18640 loss: 0.0012 lr: 0.02\n","iteration: 18650 loss: 0.0010 lr: 0.02\n","iteration: 18660 loss: 0.0017 lr: 0.02\n","iteration: 18670 loss: 0.0010 lr: 0.02\n","iteration: 18680 loss: 0.0011 lr: 0.02\n","iteration: 18690 loss: 0.0015 lr: 0.02\n","iteration: 18700 loss: 0.0014 lr: 0.02\n","iteration: 18710 loss: 0.0014 lr: 0.02\n","iteration: 18720 loss: 0.0012 lr: 0.02\n","iteration: 18730 loss: 0.0012 lr: 0.02\n","iteration: 18740 loss: 0.0012 lr: 0.02\n","iteration: 18750 loss: 0.0016 lr: 0.02\n","iteration: 18760 loss: 0.0013 lr: 0.02\n","iteration: 18770 loss: 0.0009 lr: 0.02\n","iteration: 18780 loss: 0.0012 lr: 0.02\n","iteration: 18790 loss: 0.0013 lr: 0.02\n","iteration: 18800 loss: 0.0014 lr: 0.02\n","iteration: 18810 loss: 0.0012 lr: 0.02\n","iteration: 18820 loss: 0.0011 lr: 0.02\n","iteration: 18830 loss: 0.0013 lr: 0.02\n","iteration: 18840 loss: 0.0011 lr: 0.02\n","iteration: 18850 loss: 0.0014 lr: 0.02\n","iteration: 18860 loss: 0.0012 lr: 0.02\n","iteration: 18870 loss: 0.0011 lr: 0.02\n","iteration: 18880 loss: 0.0014 lr: 0.02\n","iteration: 18890 loss: 0.0012 lr: 0.02\n","iteration: 18900 loss: 0.0012 lr: 0.02\n","iteration: 18910 loss: 0.0012 lr: 0.02\n","iteration: 18920 loss: 0.0015 lr: 0.02\n","iteration: 18930 loss: 0.0011 lr: 0.02\n","iteration: 18940 loss: 0.0015 lr: 0.02\n","iteration: 18950 loss: 0.0010 lr: 0.02\n","iteration: 18960 loss: 0.0015 lr: 0.02\n","iteration: 18970 loss: 0.0015 lr: 0.02\n","iteration: 18980 loss: 0.0016 lr: 0.02\n","iteration: 18990 loss: 0.0011 lr: 0.02\n","iteration: 19000 loss: 0.0015 lr: 0.02\n","iteration: 19010 loss: 0.0015 lr: 0.02\n","iteration: 19020 loss: 0.0023 lr: 0.02\n","iteration: 19030 loss: 0.0012 lr: 0.02\n","iteration: 19040 loss: 0.0014 lr: 0.02\n","iteration: 19050 loss: 0.0010 lr: 0.02\n","iteration: 19060 loss: 0.0014 lr: 0.02\n","iteration: 19070 loss: 0.0010 lr: 0.02\n","iteration: 19080 loss: 0.0018 lr: 0.02\n","iteration: 19090 loss: 0.0011 lr: 0.02\n","iteration: 19100 loss: 0.0012 lr: 0.02\n","iteration: 19110 loss: 0.0011 lr: 0.02\n","iteration: 19120 loss: 0.0012 lr: 0.02\n","iteration: 19130 loss: 0.0015 lr: 0.02\n","iteration: 19140 loss: 0.0016 lr: 0.02\n","iteration: 19150 loss: 0.0014 lr: 0.02\n","iteration: 19160 loss: 0.0012 lr: 0.02\n","iteration: 19170 loss: 0.0012 lr: 0.02\n","iteration: 19180 loss: 0.0011 lr: 0.02\n","iteration: 19190 loss: 0.0011 lr: 0.02\n","iteration: 19200 loss: 0.0010 lr: 0.02\n","iteration: 19210 loss: 0.0012 lr: 0.02\n","iteration: 19220 loss: 0.0012 lr: 0.02\n","iteration: 19230 loss: 0.0014 lr: 0.02\n","iteration: 19240 loss: 0.0012 lr: 0.02\n","iteration: 19250 loss: 0.0011 lr: 0.02\n","iteration: 19260 loss: 0.0011 lr: 0.02\n","iteration: 19270 loss: 0.0009 lr: 0.02\n","iteration: 19280 loss: 0.0011 lr: 0.02\n","iteration: 19290 loss: 0.0013 lr: 0.02\n","iteration: 19300 loss: 0.0011 lr: 0.02\n","iteration: 19310 loss: 0.0012 lr: 0.02\n","iteration: 19320 loss: 0.0014 lr: 0.02\n","iteration: 19330 loss: 0.0013 lr: 0.02\n","iteration: 19340 loss: 0.0016 lr: 0.02\n","iteration: 19350 loss: 0.0015 lr: 0.02\n","iteration: 19360 loss: 0.0012 lr: 0.02\n","iteration: 19370 loss: 0.0013 lr: 0.02\n","iteration: 19380 loss: 0.0013 lr: 0.02\n","iteration: 19390 loss: 0.0013 lr: 0.02\n","iteration: 19400 loss: 0.0013 lr: 0.02\n","iteration: 19410 loss: 0.0012 lr: 0.02\n","iteration: 19420 loss: 0.0012 lr: 0.02\n","iteration: 19430 loss: 0.0012 lr: 0.02\n","iteration: 19440 loss: 0.0012 lr: 0.02\n","iteration: 19450 loss: 0.0013 lr: 0.02\n","iteration: 19460 loss: 0.0012 lr: 0.02\n","iteration: 19470 loss: 0.0011 lr: 0.02\n","iteration: 19480 loss: 0.0010 lr: 0.02\n","iteration: 19490 loss: 0.0018 lr: 0.02\n","iteration: 19500 loss: 0.0013 lr: 0.02\n","iteration: 19510 loss: 0.0010 lr: 0.02\n","iteration: 19520 loss: 0.0011 lr: 0.02\n","iteration: 19530 loss: 0.0009 lr: 0.02\n","iteration: 19540 loss: 0.0017 lr: 0.02\n","iteration: 19550 loss: 0.0011 lr: 0.02\n","iteration: 19560 loss: 0.0012 lr: 0.02\n","iteration: 19570 loss: 0.0013 lr: 0.02\n","iteration: 19580 loss: 0.0013 lr: 0.02\n","iteration: 19590 loss: 0.0011 lr: 0.02\n","iteration: 19600 loss: 0.0013 lr: 0.02\n","iteration: 19610 loss: 0.0010 lr: 0.02\n","iteration: 19620 loss: 0.0016 lr: 0.02\n","iteration: 19630 loss: 0.0013 lr: 0.02\n","iteration: 19640 loss: 0.0015 lr: 0.02\n","iteration: 19650 loss: 0.0018 lr: 0.02\n","iteration: 19660 loss: 0.0015 lr: 0.02\n","iteration: 19670 loss: 0.0017 lr: 0.02\n","iteration: 19680 loss: 0.0013 lr: 0.02\n","iteration: 19690 loss: 0.0016 lr: 0.02\n","iteration: 19700 loss: 0.0012 lr: 0.02\n","iteration: 19710 loss: 0.0013 lr: 0.02\n","iteration: 19720 loss: 0.0013 lr: 0.02\n","iteration: 19730 loss: 0.0012 lr: 0.02\n","iteration: 19740 loss: 0.0013 lr: 0.02\n","iteration: 19750 loss: 0.0009 lr: 0.02\n","iteration: 19760 loss: 0.0015 lr: 0.02\n","iteration: 19770 loss: 0.0012 lr: 0.02\n","iteration: 19780 loss: 0.0017 lr: 0.02\n","iteration: 19790 loss: 0.0013 lr: 0.02\n","iteration: 19800 loss: 0.0015 lr: 0.02\n","iteration: 19810 loss: 0.0010 lr: 0.02\n","iteration: 19820 loss: 0.0011 lr: 0.02\n","iteration: 19830 loss: 0.0016 lr: 0.02\n","iteration: 19840 loss: 0.0012 lr: 0.02\n","iteration: 19850 loss: 0.0012 lr: 0.02\n","iteration: 19860 loss: 0.0013 lr: 0.02\n","iteration: 19870 loss: 0.0013 lr: 0.02\n","iteration: 19880 loss: 0.0011 lr: 0.02\n","iteration: 19890 loss: 0.0013 lr: 0.02\n","iteration: 19900 loss: 0.0013 lr: 0.02\n","iteration: 19910 loss: 0.0015 lr: 0.02\n","iteration: 19920 loss: 0.0011 lr: 0.02\n","iteration: 19930 loss: 0.0013 lr: 0.02\n","iteration: 19940 loss: 0.0016 lr: 0.02\n","iteration: 19950 loss: 0.0015 lr: 0.02\n","iteration: 19960 loss: 0.0013 lr: 0.02\n","iteration: 19970 loss: 0.0013 lr: 0.02\n","iteration: 19980 loss: 0.0013 lr: 0.02\n","iteration: 19990 loss: 0.0011 lr: 0.02\n","iteration: 20000 loss: 0.0012 lr: 0.02\n","iteration: 20010 loss: 0.0012 lr: 0.02\n","iteration: 20020 loss: 0.0011 lr: 0.02\n","iteration: 20030 loss: 0.0012 lr: 0.02\n","iteration: 20040 loss: 0.0012 lr: 0.02\n","iteration: 20050 loss: 0.0013 lr: 0.02\n","iteration: 20060 loss: 0.0011 lr: 0.02\n","iteration: 20070 loss: 0.0013 lr: 0.02\n","iteration: 20080 loss: 0.0011 lr: 0.02\n","iteration: 20090 loss: 0.0013 lr: 0.02\n","iteration: 20100 loss: 0.0014 lr: 0.02\n","iteration: 20110 loss: 0.0011 lr: 0.02\n","iteration: 20120 loss: 0.0011 lr: 0.02\n","iteration: 20130 loss: 0.0013 lr: 0.02\n","iteration: 20140 loss: 0.0016 lr: 0.02\n","iteration: 20150 loss: 0.0011 lr: 0.02\n","iteration: 20160 loss: 0.0018 lr: 0.02\n","iteration: 20170 loss: 0.0016 lr: 0.02\n","iteration: 20180 loss: 0.0014 lr: 0.02\n","iteration: 20190 loss: 0.0015 lr: 0.02\n","iteration: 20200 loss: 0.0011 lr: 0.02\n","iteration: 20210 loss: 0.0014 lr: 0.02\n","iteration: 20220 loss: 0.0012 lr: 0.02\n","iteration: 20230 loss: 0.0012 lr: 0.02\n","iteration: 20240 loss: 0.0013 lr: 0.02\n","iteration: 20250 loss: 0.0012 lr: 0.02\n","iteration: 20260 loss: 0.0012 lr: 0.02\n","iteration: 20270 loss: 0.0013 lr: 0.02\n","iteration: 20280 loss: 0.0010 lr: 0.02\n","iteration: 20290 loss: 0.0011 lr: 0.02\n","iteration: 20300 loss: 0.0015 lr: 0.02\n","iteration: 20310 loss: 0.0011 lr: 0.02\n","iteration: 20320 loss: 0.0012 lr: 0.02\n","iteration: 20330 loss: 0.0016 lr: 0.02\n","iteration: 20340 loss: 0.0012 lr: 0.02\n","iteration: 20350 loss: 0.0011 lr: 0.02\n","iteration: 20360 loss: 0.0011 lr: 0.02\n","iteration: 20370 loss: 0.0014 lr: 0.02\n","iteration: 20380 loss: 0.0014 lr: 0.02\n","iteration: 20390 loss: 0.0011 lr: 0.02\n","iteration: 20400 loss: 0.0016 lr: 0.02\n","iteration: 20410 loss: 0.0011 lr: 0.02\n","iteration: 20420 loss: 0.0013 lr: 0.02\n","iteration: 20430 loss: 0.0015 lr: 0.02\n","iteration: 20440 loss: 0.0014 lr: 0.02\n","iteration: 20450 loss: 0.0014 lr: 0.02\n","iteration: 20460 loss: 0.0012 lr: 0.02\n","iteration: 20470 loss: 0.0012 lr: 0.02\n","iteration: 20480 loss: 0.0010 lr: 0.02\n","iteration: 20490 loss: 0.0013 lr: 0.02\n","iteration: 20500 loss: 0.0016 lr: 0.02\n","iteration: 20510 loss: 0.0013 lr: 0.02\n","iteration: 20520 loss: 0.0009 lr: 0.02\n","iteration: 20530 loss: 0.0011 lr: 0.02\n","iteration: 20540 loss: 0.0011 lr: 0.02\n","iteration: 20550 loss: 0.0013 lr: 0.02\n","iteration: 20560 loss: 0.0011 lr: 0.02\n","iteration: 20570 loss: 0.0013 lr: 0.02\n","iteration: 20580 loss: 0.0010 lr: 0.02\n","iteration: 20590 loss: 0.0012 lr: 0.02\n","iteration: 20600 loss: 0.0009 lr: 0.02\n","iteration: 20610 loss: 0.0016 lr: 0.02\n","iteration: 20620 loss: 0.0013 lr: 0.02\n","iteration: 20630 loss: 0.0014 lr: 0.02\n","iteration: 20640 loss: 0.0014 lr: 0.02\n","iteration: 20650 loss: 0.0012 lr: 0.02\n","iteration: 20660 loss: 0.0012 lr: 0.02\n","iteration: 20670 loss: 0.0012 lr: 0.02\n","iteration: 20680 loss: 0.0013 lr: 0.02\n","iteration: 20690 loss: 0.0013 lr: 0.02\n","iteration: 20700 loss: 0.0012 lr: 0.02\n","iteration: 20710 loss: 0.0015 lr: 0.02\n","iteration: 20720 loss: 0.0013 lr: 0.02\n","iteration: 20730 loss: 0.0011 lr: 0.02\n","iteration: 20740 loss: 0.0009 lr: 0.02\n","iteration: 20750 loss: 0.0013 lr: 0.02\n","iteration: 20760 loss: 0.0013 lr: 0.02\n","iteration: 20770 loss: 0.0015 lr: 0.02\n","iteration: 20780 loss: 0.0014 lr: 0.02\n","iteration: 20790 loss: 0.0013 lr: 0.02\n","iteration: 20800 loss: 0.0012 lr: 0.02\n","iteration: 20810 loss: 0.0011 lr: 0.02\n","iteration: 20820 loss: 0.0021 lr: 0.02\n","iteration: 20830 loss: 0.0012 lr: 0.02\n","iteration: 20840 loss: 0.0016 lr: 0.02\n","iteration: 20850 loss: 0.0014 lr: 0.02\n","iteration: 20860 loss: 0.0014 lr: 0.02\n","iteration: 20870 loss: 0.0017 lr: 0.02\n","iteration: 20880 loss: 0.0015 lr: 0.02\n","iteration: 20890 loss: 0.0014 lr: 0.02\n","iteration: 20900 loss: 0.0012 lr: 0.02\n","iteration: 20910 loss: 0.0014 lr: 0.02\n","iteration: 20920 loss: 0.0017 lr: 0.02\n","iteration: 20930 loss: 0.0015 lr: 0.02\n","iteration: 20940 loss: 0.0014 lr: 0.02\n","iteration: 20950 loss: 0.0014 lr: 0.02\n","iteration: 20960 loss: 0.0014 lr: 0.02\n","iteration: 20970 loss: 0.0011 lr: 0.02\n","iteration: 20980 loss: 0.0015 lr: 0.02\n","iteration: 20990 loss: 0.0015 lr: 0.02\n","iteration: 21000 loss: 0.0012 lr: 0.02\n","iteration: 21010 loss: 0.0012 lr: 0.02\n","iteration: 21020 loss: 0.0011 lr: 0.02\n","iteration: 21030 loss: 0.0014 lr: 0.02\n","iteration: 21040 loss: 0.0014 lr: 0.02\n","iteration: 21050 loss: 0.0019 lr: 0.02\n","iteration: 21060 loss: 0.0014 lr: 0.02\n","iteration: 21070 loss: 0.0011 lr: 0.02\n","iteration: 21080 loss: 0.0012 lr: 0.02\n","iteration: 21090 loss: 0.0014 lr: 0.02\n","iteration: 21100 loss: 0.0013 lr: 0.02\n","iteration: 21110 loss: 0.0010 lr: 0.02\n","iteration: 21120 loss: 0.0013 lr: 0.02\n","iteration: 21130 loss: 0.0013 lr: 0.02\n","iteration: 21140 loss: 0.0010 lr: 0.02\n","iteration: 21150 loss: 0.0014 lr: 0.02\n","iteration: 21160 loss: 0.0013 lr: 0.02\n","iteration: 21170 loss: 0.0012 lr: 0.02\n","iteration: 21180 loss: 0.0015 lr: 0.02\n","iteration: 21190 loss: 0.0013 lr: 0.02\n","iteration: 21200 loss: 0.0013 lr: 0.02\n","iteration: 21210 loss: 0.0014 lr: 0.02\n","iteration: 21220 loss: 0.0016 lr: 0.02\n","iteration: 21230 loss: 0.0016 lr: 0.02\n","iteration: 21240 loss: 0.0013 lr: 0.02\n","iteration: 21250 loss: 0.0018 lr: 0.02\n","iteration: 21260 loss: 0.0013 lr: 0.02\n","iteration: 21270 loss: 0.0011 lr: 0.02\n","iteration: 21280 loss: 0.0014 lr: 0.02\n","iteration: 21290 loss: 0.0011 lr: 0.02\n","iteration: 21300 loss: 0.0012 lr: 0.02\n","iteration: 21310 loss: 0.0011 lr: 0.02\n","iteration: 21320 loss: 0.0011 lr: 0.02\n","iteration: 21330 loss: 0.0014 lr: 0.02\n","iteration: 21340 loss: 0.0011 lr: 0.02\n","iteration: 21350 loss: 0.0011 lr: 0.02\n","iteration: 21360 loss: 0.0012 lr: 0.02\n","iteration: 21370 loss: 0.0010 lr: 0.02\n","iteration: 21380 loss: 0.0012 lr: 0.02\n","iteration: 21390 loss: 0.0013 lr: 0.02\n","iteration: 21400 loss: 0.0015 lr: 0.02\n","iteration: 21410 loss: 0.0012 lr: 0.02\n","iteration: 21420 loss: 0.0012 lr: 0.02\n","iteration: 21430 loss: 0.0010 lr: 0.02\n","iteration: 21440 loss: 0.0011 lr: 0.02\n","iteration: 21450 loss: 0.0013 lr: 0.02\n","iteration: 21460 loss: 0.0013 lr: 0.02\n","iteration: 21470 loss: 0.0011 lr: 0.02\n","iteration: 21480 loss: 0.0011 lr: 0.02\n","iteration: 21490 loss: 0.0014 lr: 0.02\n","iteration: 21500 loss: 0.0013 lr: 0.02\n","iteration: 21510 loss: 0.0011 lr: 0.02\n","iteration: 21520 loss: 0.0011 lr: 0.02\n","iteration: 21530 loss: 0.0012 lr: 0.02\n","iteration: 21540 loss: 0.0017 lr: 0.02\n","iteration: 21550 loss: 0.0012 lr: 0.02\n","iteration: 21560 loss: 0.0013 lr: 0.02\n","iteration: 21570 loss: 0.0010 lr: 0.02\n","iteration: 21580 loss: 0.0013 lr: 0.02\n","iteration: 21590 loss: 0.0014 lr: 0.02\n","iteration: 21600 loss: 0.0010 lr: 0.02\n","iteration: 21610 loss: 0.0021 lr: 0.02\n","iteration: 21620 loss: 0.0016 lr: 0.02\n","iteration: 21630 loss: 0.0013 lr: 0.02\n","iteration: 21640 loss: 0.0014 lr: 0.02\n","iteration: 21650 loss: 0.0011 lr: 0.02\n","iteration: 21660 loss: 0.0013 lr: 0.02\n","iteration: 21670 loss: 0.0012 lr: 0.02\n","iteration: 21680 loss: 0.0016 lr: 0.02\n","iteration: 21690 loss: 0.0012 lr: 0.02\n","iteration: 21700 loss: 0.0012 lr: 0.02\n","iteration: 21710 loss: 0.0009 lr: 0.02\n","iteration: 21720 loss: 0.0010 lr: 0.02\n","iteration: 21730 loss: 0.0012 lr: 0.02\n","iteration: 21740 loss: 0.0012 lr: 0.02\n","iteration: 21750 loss: 0.0011 lr: 0.02\n","iteration: 21760 loss: 0.0012 lr: 0.02\n","iteration: 21770 loss: 0.0014 lr: 0.02\n","iteration: 21780 loss: 0.0011 lr: 0.02\n","iteration: 21790 loss: 0.0014 lr: 0.02\n","iteration: 21800 loss: 0.0011 lr: 0.02\n","iteration: 21810 loss: 0.0017 lr: 0.02\n","iteration: 21820 loss: 0.0011 lr: 0.02\n","iteration: 21830 loss: 0.0012 lr: 0.02\n","iteration: 21840 loss: 0.0014 lr: 0.02\n","iteration: 21850 loss: 0.0010 lr: 0.02\n","iteration: 21860 loss: 0.0020 lr: 0.02\n","iteration: 21870 loss: 0.0014 lr: 0.02\n","iteration: 21880 loss: 0.0012 lr: 0.02\n","iteration: 21890 loss: 0.0013 lr: 0.02\n","iteration: 21900 loss: 0.0011 lr: 0.02\n","iteration: 21910 loss: 0.0010 lr: 0.02\n","iteration: 21920 loss: 0.0013 lr: 0.02\n","iteration: 21930 loss: 0.0014 lr: 0.02\n","iteration: 21940 loss: 0.0011 lr: 0.02\n","iteration: 21950 loss: 0.0013 lr: 0.02\n","iteration: 21960 loss: 0.0012 lr: 0.02\n","iteration: 21970 loss: 0.0015 lr: 0.02\n","iteration: 21980 loss: 0.0012 lr: 0.02\n","iteration: 21990 loss: 0.0011 lr: 0.02\n","iteration: 22000 loss: 0.0016 lr: 0.02\n","iteration: 22010 loss: 0.0010 lr: 0.02\n","iteration: 22020 loss: 0.0012 lr: 0.02\n","iteration: 22030 loss: 0.0010 lr: 0.02\n","iteration: 22040 loss: 0.0014 lr: 0.02\n","iteration: 22050 loss: 0.0014 lr: 0.02\n","iteration: 22060 loss: 0.0011 lr: 0.02\n","iteration: 22070 loss: 0.0012 lr: 0.02\n","iteration: 22080 loss: 0.0011 lr: 0.02\n","iteration: 22090 loss: 0.0013 lr: 0.02\n","iteration: 22100 loss: 0.0013 lr: 0.02\n","iteration: 22110 loss: 0.0010 lr: 0.02\n","iteration: 22120 loss: 0.0015 lr: 0.02\n","iteration: 22130 loss: 0.0012 lr: 0.02\n","iteration: 22140 loss: 0.0013 lr: 0.02\n","iteration: 22150 loss: 0.0009 lr: 0.02\n","iteration: 22160 loss: 0.0012 lr: 0.02\n","iteration: 22170 loss: 0.0016 lr: 0.02\n","iteration: 22180 loss: 0.0014 lr: 0.02\n","iteration: 22190 loss: 0.0018 lr: 0.02\n","iteration: 22200 loss: 0.0015 lr: 0.02\n","iteration: 22210 loss: 0.0013 lr: 0.02\n","iteration: 22220 loss: 0.0012 lr: 0.02\n","iteration: 22230 loss: 0.0012 lr: 0.02\n","iteration: 22240 loss: 0.0011 lr: 0.02\n","iteration: 22250 loss: 0.0016 lr: 0.02\n","iteration: 22260 loss: 0.0014 lr: 0.02\n","iteration: 22270 loss: 0.0011 lr: 0.02\n","iteration: 22280 loss: 0.0012 lr: 0.02\n","iteration: 22290 loss: 0.0014 lr: 0.02\n","iteration: 22300 loss: 0.0011 lr: 0.02\n","iteration: 22310 loss: 0.0013 lr: 0.02\n","iteration: 22320 loss: 0.0018 lr: 0.02\n","iteration: 22330 loss: 0.0015 lr: 0.02\n","iteration: 22340 loss: 0.0013 lr: 0.02\n","iteration: 22350 loss: 0.0012 lr: 0.02\n","iteration: 22360 loss: 0.0010 lr: 0.02\n","iteration: 22370 loss: 0.0014 lr: 0.02\n","iteration: 22380 loss: 0.0013 lr: 0.02\n","iteration: 22390 loss: 0.0011 lr: 0.02\n","iteration: 22400 loss: 0.0012 lr: 0.02\n","iteration: 22410 loss: 0.0015 lr: 0.02\n","iteration: 22420 loss: 0.0012 lr: 0.02\n","iteration: 22430 loss: 0.0016 lr: 0.02\n","iteration: 22440 loss: 0.0012 lr: 0.02\n","iteration: 22450 loss: 0.0015 lr: 0.02\n","iteration: 22460 loss: 0.0011 lr: 0.02\n","iteration: 22470 loss: 0.0011 lr: 0.02\n","iteration: 22480 loss: 0.0014 lr: 0.02\n","iteration: 22490 loss: 0.0012 lr: 0.02\n","iteration: 22500 loss: 0.0010 lr: 0.02\n","iteration: 22510 loss: 0.0011 lr: 0.02\n","iteration: 22520 loss: 0.0012 lr: 0.02\n","iteration: 22530 loss: 0.0013 lr: 0.02\n","iteration: 22540 loss: 0.0014 lr: 0.02\n","iteration: 22550 loss: 0.0014 lr: 0.02\n","iteration: 22560 loss: 0.0012 lr: 0.02\n","iteration: 22570 loss: 0.0012 lr: 0.02\n","iteration: 22580 loss: 0.0010 lr: 0.02\n","iteration: 22590 loss: 0.0012 lr: 0.02\n","iteration: 22600 loss: 0.0013 lr: 0.02\n","iteration: 22610 loss: 0.0012 lr: 0.02\n","iteration: 22620 loss: 0.0014 lr: 0.02\n","iteration: 22630 loss: 0.0011 lr: 0.02\n","iteration: 22640 loss: 0.0011 lr: 0.02\n","iteration: 22650 loss: 0.0014 lr: 0.02\n","iteration: 22660 loss: 0.0011 lr: 0.02\n","iteration: 22670 loss: 0.0010 lr: 0.02\n","iteration: 22680 loss: 0.0015 lr: 0.02\n","iteration: 22690 loss: 0.0011 lr: 0.02\n","iteration: 22700 loss: 0.0013 lr: 0.02\n","iteration: 22710 loss: 0.0013 lr: 0.02\n","iteration: 22720 loss: 0.0016 lr: 0.02\n","iteration: 22730 loss: 0.0013 lr: 0.02\n","iteration: 22740 loss: 0.0013 lr: 0.02\n","iteration: 22750 loss: 0.0017 lr: 0.02\n","iteration: 22760 loss: 0.0012 lr: 0.02\n","iteration: 22770 loss: 0.0013 lr: 0.02\n","iteration: 22780 loss: 0.0013 lr: 0.02\n","iteration: 22790 loss: 0.0012 lr: 0.02\n","iteration: 22800 loss: 0.0015 lr: 0.02\n","iteration: 22810 loss: 0.0014 lr: 0.02\n","iteration: 22820 loss: 0.0013 lr: 0.02\n","iteration: 22830 loss: 0.0014 lr: 0.02\n","iteration: 22840 loss: 0.0012 lr: 0.02\n","iteration: 22850 loss: 0.0013 lr: 0.02\n","iteration: 22860 loss: 0.0015 lr: 0.02\n","iteration: 22870 loss: 0.0014 lr: 0.02\n","iteration: 22880 loss: 0.0011 lr: 0.02\n","iteration: 22890 loss: 0.0010 lr: 0.02\n","iteration: 22900 loss: 0.0012 lr: 0.02\n","iteration: 22910 loss: 0.0013 lr: 0.02\n","iteration: 22920 loss: 0.0011 lr: 0.02\n","iteration: 22930 loss: 0.0012 lr: 0.02\n","iteration: 22940 loss: 0.0014 lr: 0.02\n","iteration: 22950 loss: 0.0012 lr: 0.02\n","iteration: 22960 loss: 0.0015 lr: 0.02\n","iteration: 22970 loss: 0.0014 lr: 0.02\n","iteration: 22980 loss: 0.0011 lr: 0.02\n","iteration: 22990 loss: 0.0013 lr: 0.02\n","iteration: 23000 loss: 0.0012 lr: 0.02\n","iteration: 23010 loss: 0.0011 lr: 0.02\n","iteration: 23020 loss: 0.0012 lr: 0.02\n","iteration: 23030 loss: 0.0014 lr: 0.02\n","iteration: 23040 loss: 0.0012 lr: 0.02\n","iteration: 23050 loss: 0.0011 lr: 0.02\n","iteration: 23060 loss: 0.0012 lr: 0.02\n","iteration: 23070 loss: 0.0011 lr: 0.02\n","iteration: 23080 loss: 0.0013 lr: 0.02\n","iteration: 23090 loss: 0.0011 lr: 0.02\n","iteration: 23100 loss: 0.0016 lr: 0.02\n","iteration: 23110 loss: 0.0012 lr: 0.02\n","iteration: 23120 loss: 0.0011 lr: 0.02\n","iteration: 23130 loss: 0.0010 lr: 0.02\n","iteration: 23140 loss: 0.0008 lr: 0.02\n","iteration: 23150 loss: 0.0014 lr: 0.02\n","iteration: 23160 loss: 0.0012 lr: 0.02\n","iteration: 23170 loss: 0.0011 lr: 0.02\n","iteration: 23180 loss: 0.0011 lr: 0.02\n","iteration: 23190 loss: 0.0014 lr: 0.02\n","iteration: 23200 loss: 0.0015 lr: 0.02\n","iteration: 23210 loss: 0.0014 lr: 0.02\n","iteration: 23220 loss: 0.0013 lr: 0.02\n","iteration: 23230 loss: 0.0011 lr: 0.02\n","iteration: 23240 loss: 0.0012 lr: 0.02\n","iteration: 23250 loss: 0.0013 lr: 0.02\n","iteration: 23260 loss: 0.0016 lr: 0.02\n","iteration: 23270 loss: 0.0013 lr: 0.02\n","iteration: 23280 loss: 0.0013 lr: 0.02\n","iteration: 23290 loss: 0.0014 lr: 0.02\n","iteration: 23300 loss: 0.0012 lr: 0.02\n","iteration: 23310 loss: 0.0011 lr: 0.02\n","iteration: 23320 loss: 0.0012 lr: 0.02\n","iteration: 23330 loss: 0.0012 lr: 0.02\n","iteration: 23340 loss: 0.0013 lr: 0.02\n","iteration: 23350 loss: 0.0010 lr: 0.02\n","iteration: 23360 loss: 0.0014 lr: 0.02\n","iteration: 23370 loss: 0.0009 lr: 0.02\n","iteration: 23380 loss: 0.0013 lr: 0.02\n","iteration: 23390 loss: 0.0015 lr: 0.02\n","iteration: 23400 loss: 0.0015 lr: 0.02\n","iteration: 23410 loss: 0.0015 lr: 0.02\n","iteration: 23420 loss: 0.0013 lr: 0.02\n","iteration: 23430 loss: 0.0015 lr: 0.02\n","iteration: 23440 loss: 0.0012 lr: 0.02\n","iteration: 23450 loss: 0.0018 lr: 0.02\n","iteration: 23460 loss: 0.0012 lr: 0.02\n","iteration: 23470 loss: 0.0011 lr: 0.02\n","iteration: 23480 loss: 0.0015 lr: 0.02\n","iteration: 23490 loss: 0.0013 lr: 0.02\n","iteration: 23500 loss: 0.0014 lr: 0.02\n","iteration: 23510 loss: 0.0016 lr: 0.02\n","iteration: 23520 loss: 0.0016 lr: 0.02\n","iteration: 23530 loss: 0.0012 lr: 0.02\n","iteration: 23540 loss: 0.0016 lr: 0.02\n","iteration: 23550 loss: 0.0016 lr: 0.02\n","iteration: 23560 loss: 0.0012 lr: 0.02\n","iteration: 23570 loss: 0.0016 lr: 0.02\n","iteration: 23580 loss: 0.0015 lr: 0.02\n","iteration: 23590 loss: 0.0011 lr: 0.02\n","iteration: 23600 loss: 0.0014 lr: 0.02\n","iteration: 23610 loss: 0.0009 lr: 0.02\n","iteration: 23620 loss: 0.0012 lr: 0.02\n","iteration: 23630 loss: 0.0014 lr: 0.02\n","iteration: 23640 loss: 0.0012 lr: 0.02\n","iteration: 23650 loss: 0.0010 lr: 0.02\n","iteration: 23660 loss: 0.0012 lr: 0.02\n","iteration: 23670 loss: 0.0014 lr: 0.02\n","iteration: 23680 loss: 0.0011 lr: 0.02\n","iteration: 23690 loss: 0.0013 lr: 0.02\n","iteration: 23700 loss: 0.0018 lr: 0.02\n","iteration: 23710 loss: 0.0014 lr: 0.02\n","iteration: 23720 loss: 0.0013 lr: 0.02\n","iteration: 23730 loss: 0.0013 lr: 0.02\n","iteration: 23740 loss: 0.0012 lr: 0.02\n","iteration: 23750 loss: 0.0011 lr: 0.02\n","iteration: 23760 loss: 0.0014 lr: 0.02\n","iteration: 23770 loss: 0.0015 lr: 0.02\n","iteration: 23780 loss: 0.0012 lr: 0.02\n","iteration: 23790 loss: 0.0013 lr: 0.02\n","iteration: 23800 loss: 0.0013 lr: 0.02\n","iteration: 23810 loss: 0.0016 lr: 0.02\n","iteration: 23820 loss: 0.0012 lr: 0.02\n","iteration: 23830 loss: 0.0012 lr: 0.02\n","iteration: 23840 loss: 0.0012 lr: 0.02\n","iteration: 23850 loss: 0.0013 lr: 0.02\n","iteration: 23860 loss: 0.0017 lr: 0.02\n","iteration: 23870 loss: 0.0014 lr: 0.02\n","iteration: 23880 loss: 0.0015 lr: 0.02\n","iteration: 23890 loss: 0.0011 lr: 0.02\n","iteration: 23900 loss: 0.0012 lr: 0.02\n","iteration: 23910 loss: 0.0010 lr: 0.02\n","iteration: 23920 loss: 0.0012 lr: 0.02\n","iteration: 23930 loss: 0.0013 lr: 0.02\n","iteration: 23940 loss: 0.0011 lr: 0.02\n","iteration: 23950 loss: 0.0013 lr: 0.02\n","iteration: 23960 loss: 0.0012 lr: 0.02\n","iteration: 23970 loss: 0.0013 lr: 0.02\n","iteration: 23980 loss: 0.0016 lr: 0.02\n","iteration: 23990 loss: 0.0010 lr: 0.02\n","iteration: 24000 loss: 0.0011 lr: 0.02\n","iteration: 24010 loss: 0.0011 lr: 0.02\n","iteration: 24020 loss: 0.0016 lr: 0.02\n","iteration: 24030 loss: 0.0011 lr: 0.02\n","iteration: 24040 loss: 0.0010 lr: 0.02\n","iteration: 24050 loss: 0.0015 lr: 0.02\n","iteration: 24060 loss: 0.0013 lr: 0.02\n","iteration: 24070 loss: 0.0012 lr: 0.02\n","iteration: 24080 loss: 0.0013 lr: 0.02\n","iteration: 24090 loss: 0.0013 lr: 0.02\n","iteration: 24100 loss: 0.0016 lr: 0.02\n","iteration: 24110 loss: 0.0014 lr: 0.02\n","iteration: 24120 loss: 0.0018 lr: 0.02\n","iteration: 24130 loss: 0.0012 lr: 0.02\n","iteration: 24140 loss: 0.0012 lr: 0.02\n","iteration: 24150 loss: 0.0012 lr: 0.02\n","iteration: 24160 loss: 0.0013 lr: 0.02\n","iteration: 24170 loss: 0.0011 lr: 0.02\n","iteration: 24180 loss: 0.0011 lr: 0.02\n","iteration: 24190 loss: 0.0015 lr: 0.02\n","iteration: 24200 loss: 0.0013 lr: 0.02\n","iteration: 24210 loss: 0.0013 lr: 0.02\n","iteration: 24220 loss: 0.0012 lr: 0.02\n","iteration: 24230 loss: 0.0010 lr: 0.02\n","iteration: 24240 loss: 0.0015 lr: 0.02\n","iteration: 24250 loss: 0.0011 lr: 0.02\n","iteration: 24260 loss: 0.0013 lr: 0.02\n","iteration: 24270 loss: 0.0014 lr: 0.02\n","iteration: 24280 loss: 0.0012 lr: 0.02\n","iteration: 24290 loss: 0.0012 lr: 0.02\n","iteration: 24300 loss: 0.0015 lr: 0.02\n","iteration: 24310 loss: 0.0014 lr: 0.02\n","iteration: 24320 loss: 0.0012 lr: 0.02\n","iteration: 24330 loss: 0.0011 lr: 0.02\n","iteration: 24340 loss: 0.0014 lr: 0.02\n","iteration: 24350 loss: 0.0013 lr: 0.02\n","iteration: 24360 loss: 0.0012 lr: 0.02\n","iteration: 24370 loss: 0.0012 lr: 0.02\n","iteration: 24380 loss: 0.0011 lr: 0.02\n","iteration: 24390 loss: 0.0013 lr: 0.02\n","iteration: 24400 loss: 0.0015 lr: 0.02\n","iteration: 24410 loss: 0.0010 lr: 0.02\n","iteration: 24420 loss: 0.0018 lr: 0.02\n","iteration: 24430 loss: 0.0015 lr: 0.02\n","iteration: 24440 loss: 0.0012 lr: 0.02\n","iteration: 24450 loss: 0.0013 lr: 0.02\n","iteration: 24460 loss: 0.0011 lr: 0.02\n","iteration: 24470 loss: 0.0014 lr: 0.02\n","iteration: 24480 loss: 0.0012 lr: 0.02\n","iteration: 24490 loss: 0.0011 lr: 0.02\n","iteration: 24500 loss: 0.0016 lr: 0.02\n","iteration: 24510 loss: 0.0015 lr: 0.02\n","iteration: 24520 loss: 0.0012 lr: 0.02\n","iteration: 24530 loss: 0.0015 lr: 0.02\n","iteration: 24540 loss: 0.0012 lr: 0.02\n","iteration: 24550 loss: 0.0012 lr: 0.02\n","iteration: 24560 loss: 0.0015 lr: 0.02\n","iteration: 24570 loss: 0.0011 lr: 0.02\n","iteration: 24580 loss: 0.0011 lr: 0.02\n","iteration: 24590 loss: 0.0011 lr: 0.02\n","iteration: 24600 loss: 0.0010 lr: 0.02\n","iteration: 24610 loss: 0.0011 lr: 0.02\n","iteration: 24620 loss: 0.0009 lr: 0.02\n","iteration: 24630 loss: 0.0013 lr: 0.02\n","iteration: 24640 loss: 0.0010 lr: 0.02\n","iteration: 24650 loss: 0.0012 lr: 0.02\n","iteration: 24660 loss: 0.0014 lr: 0.02\n","iteration: 24670 loss: 0.0013 lr: 0.02\n","iteration: 24680 loss: 0.0014 lr: 0.02\n","iteration: 24690 loss: 0.0014 lr: 0.02\n","iteration: 24700 loss: 0.0015 lr: 0.02\n","iteration: 24710 loss: 0.0012 lr: 0.02\n","iteration: 24720 loss: 0.0012 lr: 0.02\n","iteration: 24730 loss: 0.0015 lr: 0.02\n","iteration: 24740 loss: 0.0010 lr: 0.02\n","iteration: 24750 loss: 0.0013 lr: 0.02\n","iteration: 24760 loss: 0.0010 lr: 0.02\n","iteration: 24770 loss: 0.0009 lr: 0.02\n","iteration: 24780 loss: 0.0011 lr: 0.02\n","iteration: 24790 loss: 0.0011 lr: 0.02\n","iteration: 24800 loss: 0.0010 lr: 0.02\n","iteration: 24810 loss: 0.0012 lr: 0.02\n","iteration: 24820 loss: 0.0008 lr: 0.02\n","iteration: 24830 loss: 0.0012 lr: 0.02\n","iteration: 24840 loss: 0.0014 lr: 0.02\n","iteration: 24850 loss: 0.0013 lr: 0.02\n","iteration: 24860 loss: 0.0014 lr: 0.02\n","iteration: 24870 loss: 0.0012 lr: 0.02\n","iteration: 24880 loss: 0.0011 lr: 0.02\n","iteration: 24890 loss: 0.0009 lr: 0.02\n","iteration: 24900 loss: 0.0012 lr: 0.02\n","iteration: 24910 loss: 0.0014 lr: 0.02\n","iteration: 24920 loss: 0.0016 lr: 0.02\n","iteration: 24930 loss: 0.0012 lr: 0.02\n","iteration: 24940 loss: 0.0011 lr: 0.02\n","iteration: 24950 loss: 0.0016 lr: 0.02\n","iteration: 24960 loss: 0.0013 lr: 0.02\n","iteration: 24970 loss: 0.0014 lr: 0.02\n","iteration: 24980 loss: 0.0012 lr: 0.02\n","iteration: 24990 loss: 0.0011 lr: 0.02\n","iteration: 25000 loss: 0.0012 lr: 0.02\n","iteration: 25010 loss: 0.0013 lr: 0.02\n","iteration: 25020 loss: 0.0013 lr: 0.02\n","iteration: 25030 loss: 0.0014 lr: 0.02\n","iteration: 25040 loss: 0.0011 lr: 0.02\n","iteration: 25050 loss: 0.0015 lr: 0.02\n","iteration: 25060 loss: 0.0009 lr: 0.02\n","iteration: 25070 loss: 0.0013 lr: 0.02\n","iteration: 25080 loss: 0.0013 lr: 0.02\n","iteration: 25090 loss: 0.0012 lr: 0.02\n","iteration: 25100 loss: 0.0013 lr: 0.02\n","iteration: 25110 loss: 0.0012 lr: 0.02\n","iteration: 25120 loss: 0.0015 lr: 0.02\n","iteration: 25130 loss: 0.0012 lr: 0.02\n","iteration: 25140 loss: 0.0013 lr: 0.02\n","iteration: 25150 loss: 0.0016 lr: 0.02\n","iteration: 25160 loss: 0.0013 lr: 0.02\n","iteration: 25170 loss: 0.0013 lr: 0.02\n","iteration: 25180 loss: 0.0012 lr: 0.02\n","iteration: 25190 loss: 0.0010 lr: 0.02\n","iteration: 25200 loss: 0.0016 lr: 0.02\n","iteration: 25210 loss: 0.0010 lr: 0.02\n","iteration: 25220 loss: 0.0015 lr: 0.02\n","iteration: 25230 loss: 0.0011 lr: 0.02\n","iteration: 25240 loss: 0.0012 lr: 0.02\n","iteration: 25250 loss: 0.0010 lr: 0.02\n","iteration: 25260 loss: 0.0011 lr: 0.02\n","iteration: 25270 loss: 0.0013 lr: 0.02\n","iteration: 25280 loss: 0.0012 lr: 0.02\n","iteration: 25290 loss: 0.0013 lr: 0.02\n","iteration: 25300 loss: 0.0010 lr: 0.02\n","iteration: 25310 loss: 0.0012 lr: 0.02\n","iteration: 25320 loss: 0.0010 lr: 0.02\n","iteration: 25330 loss: 0.0013 lr: 0.02\n","iteration: 25340 loss: 0.0012 lr: 0.02\n","iteration: 25350 loss: 0.0012 lr: 0.02\n","iteration: 25360 loss: 0.0012 lr: 0.02\n","iteration: 25370 loss: 0.0013 lr: 0.02\n","iteration: 25380 loss: 0.0016 lr: 0.02\n","iteration: 25390 loss: 0.0016 lr: 0.02\n","iteration: 25400 loss: 0.0012 lr: 0.02\n","iteration: 25410 loss: 0.0013 lr: 0.02\n","iteration: 25420 loss: 0.0014 lr: 0.02\n","iteration: 25430 loss: 0.0011 lr: 0.02\n","iteration: 25440 loss: 0.0013 lr: 0.02\n","iteration: 25450 loss: 0.0014 lr: 0.02\n","iteration: 25460 loss: 0.0012 lr: 0.02\n","iteration: 25470 loss: 0.0015 lr: 0.02\n","iteration: 25480 loss: 0.0016 lr: 0.02\n","iteration: 25490 loss: 0.0012 lr: 0.02\n","iteration: 25500 loss: 0.0013 lr: 0.02\n","iteration: 25510 loss: 0.0014 lr: 0.02\n","iteration: 25520 loss: 0.0011 lr: 0.02\n","iteration: 25530 loss: 0.0014 lr: 0.02\n","iteration: 25540 loss: 0.0011 lr: 0.02\n","iteration: 25550 loss: 0.0014 lr: 0.02\n","iteration: 25560 loss: 0.0012 lr: 0.02\n","iteration: 25570 loss: 0.0013 lr: 0.02\n","iteration: 25580 loss: 0.0013 lr: 0.02\n","iteration: 25590 loss: 0.0011 lr: 0.02\n","iteration: 25600 loss: 0.0012 lr: 0.02\n","iteration: 25610 loss: 0.0017 lr: 0.02\n","iteration: 25620 loss: 0.0017 lr: 0.02\n","iteration: 25630 loss: 0.0017 lr: 0.02\n","iteration: 25640 loss: 0.0011 lr: 0.02\n","iteration: 25650 loss: 0.0011 lr: 0.02\n","iteration: 25660 loss: 0.0012 lr: 0.02\n","iteration: 25670 loss: 0.0011 lr: 0.02\n","iteration: 25680 loss: 0.0012 lr: 0.02\n","iteration: 25690 loss: 0.0020 lr: 0.02\n","iteration: 25700 loss: 0.0015 lr: 0.02\n","iteration: 25710 loss: 0.0012 lr: 0.02\n","iteration: 25720 loss: 0.0011 lr: 0.02\n","iteration: 25730 loss: 0.0011 lr: 0.02\n","iteration: 25740 loss: 0.0011 lr: 0.02\n","iteration: 25750 loss: 0.0014 lr: 0.02\n","iteration: 25760 loss: 0.0010 lr: 0.02\n","iteration: 25770 loss: 0.0014 lr: 0.02\n","iteration: 25780 loss: 0.0015 lr: 0.02\n","iteration: 25790 loss: 0.0011 lr: 0.02\n","iteration: 25800 loss: 0.0011 lr: 0.02\n","iteration: 25810 loss: 0.0012 lr: 0.02\n","iteration: 25820 loss: 0.0011 lr: 0.02\n","iteration: 25830 loss: 0.0012 lr: 0.02\n","iteration: 25840 loss: 0.0012 lr: 0.02\n","iteration: 25850 loss: 0.0012 lr: 0.02\n","iteration: 25860 loss: 0.0014 lr: 0.02\n","iteration: 25870 loss: 0.0012 lr: 0.02\n","iteration: 25880 loss: 0.0013 lr: 0.02\n","iteration: 25890 loss: 0.0013 lr: 0.02\n","iteration: 25900 loss: 0.0009 lr: 0.02\n","iteration: 25910 loss: 0.0015 lr: 0.02\n","iteration: 25920 loss: 0.0014 lr: 0.02\n","iteration: 25930 loss: 0.0012 lr: 0.02\n","iteration: 25940 loss: 0.0011 lr: 0.02\n","iteration: 25950 loss: 0.0015 lr: 0.02\n","iteration: 25960 loss: 0.0012 lr: 0.02\n","iteration: 25970 loss: 0.0013 lr: 0.02\n","iteration: 25980 loss: 0.0011 lr: 0.02\n","iteration: 25990 loss: 0.0013 lr: 0.02\n","iteration: 26000 loss: 0.0014 lr: 0.02\n","iteration: 26010 loss: 0.0010 lr: 0.02\n","iteration: 26020 loss: 0.0012 lr: 0.02\n","iteration: 26030 loss: 0.0011 lr: 0.02\n","iteration: 26040 loss: 0.0011 lr: 0.02\n","iteration: 26050 loss: 0.0009 lr: 0.02\n","iteration: 26060 loss: 0.0013 lr: 0.02\n","iteration: 26070 loss: 0.0012 lr: 0.02\n","iteration: 26080 loss: 0.0013 lr: 0.02\n","iteration: 26090 loss: 0.0012 lr: 0.02\n","iteration: 26100 loss: 0.0014 lr: 0.02\n","iteration: 26110 loss: 0.0013 lr: 0.02\n","iteration: 26120 loss: 0.0015 lr: 0.02\n","iteration: 26130 loss: 0.0016 lr: 0.02\n","iteration: 26140 loss: 0.0014 lr: 0.02\n","iteration: 26150 loss: 0.0011 lr: 0.02\n","iteration: 26160 loss: 0.0011 lr: 0.02\n","iteration: 26170 loss: 0.0013 lr: 0.02\n","iteration: 26180 loss: 0.0013 lr: 0.02\n","iteration: 26190 loss: 0.0009 lr: 0.02\n","iteration: 26200 loss: 0.0014 lr: 0.02\n","iteration: 26210 loss: 0.0013 lr: 0.02\n","iteration: 26220 loss: 0.0009 lr: 0.02\n","iteration: 26230 loss: 0.0012 lr: 0.02\n","iteration: 26240 loss: 0.0011 lr: 0.02\n","iteration: 26250 loss: 0.0012 lr: 0.02\n","iteration: 26260 loss: 0.0012 lr: 0.02\n","iteration: 26270 loss: 0.0011 lr: 0.02\n","iteration: 26280 loss: 0.0013 lr: 0.02\n","iteration: 26290 loss: 0.0011 lr: 0.02\n","iteration: 26300 loss: 0.0014 lr: 0.02\n","iteration: 26310 loss: 0.0011 lr: 0.02\n","iteration: 26320 loss: 0.0012 lr: 0.02\n","iteration: 26330 loss: 0.0010 lr: 0.02\n","iteration: 26340 loss: 0.0017 lr: 0.02\n","iteration: 26350 loss: 0.0012 lr: 0.02\n","iteration: 26360 loss: 0.0015 lr: 0.02\n","iteration: 26370 loss: 0.0015 lr: 0.02\n","iteration: 26380 loss: 0.0012 lr: 0.02\n","iteration: 26390 loss: 0.0013 lr: 0.02\n","iteration: 26400 loss: 0.0012 lr: 0.02\n","iteration: 26410 loss: 0.0010 lr: 0.02\n","iteration: 26420 loss: 0.0015 lr: 0.02\n","iteration: 26430 loss: 0.0016 lr: 0.02\n","iteration: 26440 loss: 0.0012 lr: 0.02\n","iteration: 26450 loss: 0.0013 lr: 0.02\n","iteration: 26460 loss: 0.0013 lr: 0.02\n","iteration: 26470 loss: 0.0013 lr: 0.02\n","iteration: 26480 loss: 0.0013 lr: 0.02\n","iteration: 26490 loss: 0.0015 lr: 0.02\n","iteration: 26500 loss: 0.0012 lr: 0.02\n","iteration: 26510 loss: 0.0015 lr: 0.02\n","iteration: 26520 loss: 0.0011 lr: 0.02\n","iteration: 26530 loss: 0.0014 lr: 0.02\n","iteration: 26540 loss: 0.0013 lr: 0.02\n","iteration: 26550 loss: 0.0016 lr: 0.02\n","iteration: 26560 loss: 0.0015 lr: 0.02\n","iteration: 26570 loss: 0.0010 lr: 0.02\n","iteration: 26580 loss: 0.0016 lr: 0.02\n","iteration: 26590 loss: 0.0014 lr: 0.02\n","iteration: 26600 loss: 0.0014 lr: 0.02\n","iteration: 26610 loss: 0.0014 lr: 0.02\n","iteration: 26620 loss: 0.0012 lr: 0.02\n","iteration: 26630 loss: 0.0012 lr: 0.02\n","iteration: 26640 loss: 0.0016 lr: 0.02\n","iteration: 26650 loss: 0.0017 lr: 0.02\n","iteration: 26660 loss: 0.0011 lr: 0.02\n","iteration: 26670 loss: 0.0019 lr: 0.02\n","iteration: 26680 loss: 0.0011 lr: 0.02\n","iteration: 26690 loss: 0.0016 lr: 0.02\n","iteration: 26700 loss: 0.0012 lr: 0.02\n","iteration: 26710 loss: 0.0011 lr: 0.02\n","iteration: 26720 loss: 0.0014 lr: 0.02\n","iteration: 26730 loss: 0.0012 lr: 0.02\n","iteration: 26740 loss: 0.0011 lr: 0.02\n","iteration: 26750 loss: 0.0014 lr: 0.02\n","iteration: 26760 loss: 0.0015 lr: 0.02\n","iteration: 26770 loss: 0.0016 lr: 0.02\n","iteration: 26780 loss: 0.0012 lr: 0.02\n","iteration: 26790 loss: 0.0012 lr: 0.02\n","iteration: 26800 loss: 0.0014 lr: 0.02\n","iteration: 26810 loss: 0.0013 lr: 0.02\n","iteration: 26820 loss: 0.0013 lr: 0.02\n","iteration: 26830 loss: 0.0012 lr: 0.02\n","iteration: 26840 loss: 0.0012 lr: 0.02\n","iteration: 26850 loss: 0.0016 lr: 0.02\n","iteration: 26860 loss: 0.0014 lr: 0.02\n","iteration: 26870 loss: 0.0011 lr: 0.02\n","iteration: 26880 loss: 0.0010 lr: 0.02\n","iteration: 26890 loss: 0.0014 lr: 0.02\n","iteration: 26900 loss: 0.0012 lr: 0.02\n","iteration: 26910 loss: 0.0015 lr: 0.02\n","iteration: 26920 loss: 0.0014 lr: 0.02\n","iteration: 26930 loss: 0.0013 lr: 0.02\n","iteration: 26940 loss: 0.0011 lr: 0.02\n","iteration: 26950 loss: 0.0009 lr: 0.02\n","iteration: 26960 loss: 0.0012 lr: 0.02\n","iteration: 26970 loss: 0.0015 lr: 0.02\n","iteration: 26980 loss: 0.0013 lr: 0.02\n","iteration: 26990 loss: 0.0011 lr: 0.02\n","iteration: 27000 loss: 0.0016 lr: 0.02\n","iteration: 27010 loss: 0.0013 lr: 0.02\n","iteration: 27020 loss: 0.0012 lr: 0.02\n","iteration: 27030 loss: 0.0016 lr: 0.02\n","iteration: 27040 loss: 0.0013 lr: 0.02\n","iteration: 27050 loss: 0.0011 lr: 0.02\n","iteration: 27060 loss: 0.0013 lr: 0.02\n","iteration: 27070 loss: 0.0013 lr: 0.02\n","iteration: 27080 loss: 0.0012 lr: 0.02\n","iteration: 27090 loss: 0.0010 lr: 0.02\n","iteration: 27100 loss: 0.0011 lr: 0.02\n","iteration: 27110 loss: 0.0013 lr: 0.02\n","iteration: 27120 loss: 0.0011 lr: 0.02\n","iteration: 27130 loss: 0.0011 lr: 0.02\n","iteration: 27140 loss: 0.0010 lr: 0.02\n","iteration: 27150 loss: 0.0011 lr: 0.02\n","iteration: 27160 loss: 0.0012 lr: 0.02\n","iteration: 27170 loss: 0.0010 lr: 0.02\n","iteration: 27180 loss: 0.0012 lr: 0.02\n","iteration: 27190 loss: 0.0011 lr: 0.02\n","iteration: 27200 loss: 0.0014 lr: 0.02\n","iteration: 27210 loss: 0.0011 lr: 0.02\n","iteration: 27220 loss: 0.0016 lr: 0.02\n","iteration: 27230 loss: 0.0011 lr: 0.02\n","iteration: 27240 loss: 0.0014 lr: 0.02\n","iteration: 27250 loss: 0.0011 lr: 0.02\n","iteration: 27260 loss: 0.0014 lr: 0.02\n","iteration: 27270 loss: 0.0016 lr: 0.02\n","iteration: 27280 loss: 0.0014 lr: 0.02\n","iteration: 27290 loss: 0.0015 lr: 0.02\n","iteration: 27300 loss: 0.0014 lr: 0.02\n","iteration: 27310 loss: 0.0013 lr: 0.02\n","iteration: 27320 loss: 0.0012 lr: 0.02\n","iteration: 27330 loss: 0.0012 lr: 0.02\n","iteration: 27340 loss: 0.0014 lr: 0.02\n","iteration: 27350 loss: 0.0013 lr: 0.02\n","iteration: 27360 loss: 0.0013 lr: 0.02\n","iteration: 27370 loss: 0.0011 lr: 0.02\n","iteration: 27380 loss: 0.0010 lr: 0.02\n","iteration: 27390 loss: 0.0012 lr: 0.02\n","iteration: 27400 loss: 0.0013 lr: 0.02\n","iteration: 27410 loss: 0.0015 lr: 0.02\n","iteration: 27420 loss: 0.0012 lr: 0.02\n","iteration: 27430 loss: 0.0018 lr: 0.02\n","iteration: 27440 loss: 0.0014 lr: 0.02\n","iteration: 27450 loss: 0.0013 lr: 0.02\n","iteration: 27460 loss: 0.0013 lr: 0.02\n","iteration: 27470 loss: 0.0011 lr: 0.02\n","iteration: 27480 loss: 0.0014 lr: 0.02\n","iteration: 27490 loss: 0.0012 lr: 0.02\n","iteration: 27500 loss: 0.0014 lr: 0.02\n","iteration: 27510 loss: 0.0012 lr: 0.02\n","iteration: 27520 loss: 0.0017 lr: 0.02\n","iteration: 27530 loss: 0.0013 lr: 0.02\n","iteration: 27540 loss: 0.0010 lr: 0.02\n","iteration: 27550 loss: 0.0012 lr: 0.02\n","iteration: 27560 loss: 0.0016 lr: 0.02\n","iteration: 27570 loss: 0.0011 lr: 0.02\n","iteration: 27580 loss: 0.0013 lr: 0.02\n","iteration: 27590 loss: 0.0016 lr: 0.02\n","iteration: 27600 loss: 0.0011 lr: 0.02\n","iteration: 27610 loss: 0.0014 lr: 0.02\n","iteration: 27620 loss: 0.0011 lr: 0.02\n","iteration: 27630 loss: 0.0011 lr: 0.02\n","iteration: 27640 loss: 0.0011 lr: 0.02\n","iteration: 27650 loss: 0.0014 lr: 0.02\n","iteration: 27660 loss: 0.0011 lr: 0.02\n","iteration: 27670 loss: 0.0011 lr: 0.02\n","iteration: 27680 loss: 0.0017 lr: 0.02\n","iteration: 27690 loss: 0.0015 lr: 0.02\n","iteration: 27700 loss: 0.0012 lr: 0.02\n","iteration: 27710 loss: 0.0017 lr: 0.02\n","iteration: 27720 loss: 0.0010 lr: 0.02\n","iteration: 27730 loss: 0.0012 lr: 0.02\n","iteration: 27740 loss: 0.0015 lr: 0.02\n","iteration: 27750 loss: 0.0015 lr: 0.02\n","iteration: 27760 loss: 0.0013 lr: 0.02\n","iteration: 27770 loss: 0.0012 lr: 0.02\n","iteration: 27780 loss: 0.0013 lr: 0.02\n","iteration: 27790 loss: 0.0012 lr: 0.02\n","iteration: 27800 loss: 0.0010 lr: 0.02\n","iteration: 27810 loss: 0.0013 lr: 0.02\n","iteration: 27820 loss: 0.0013 lr: 0.02\n","iteration: 27830 loss: 0.0013 lr: 0.02\n","iteration: 27840 loss: 0.0011 lr: 0.02\n","iteration: 27850 loss: 0.0013 lr: 0.02\n","iteration: 27860 loss: 0.0013 lr: 0.02\n","iteration: 27870 loss: 0.0010 lr: 0.02\n","iteration: 27880 loss: 0.0011 lr: 0.02\n","iteration: 27890 loss: 0.0010 lr: 0.02\n","iteration: 27900 loss: 0.0014 lr: 0.02\n","iteration: 27910 loss: 0.0014 lr: 0.02\n","iteration: 27920 loss: 0.0012 lr: 0.02\n","iteration: 27930 loss: 0.0013 lr: 0.02\n","iteration: 27940 loss: 0.0011 lr: 0.02\n","iteration: 27950 loss: 0.0015 lr: 0.02\n","iteration: 27960 loss: 0.0014 lr: 0.02\n","iteration: 27970 loss: 0.0013 lr: 0.02\n","iteration: 27980 loss: 0.0014 lr: 0.02\n","iteration: 27990 loss: 0.0010 lr: 0.02\n","iteration: 28000 loss: 0.0014 lr: 0.02\n","iteration: 28010 loss: 0.0011 lr: 0.02\n","iteration: 28020 loss: 0.0013 lr: 0.02\n","iteration: 28030 loss: 0.0013 lr: 0.02\n","iteration: 28040 loss: 0.0012 lr: 0.02\n","iteration: 28050 loss: 0.0012 lr: 0.02\n","iteration: 28060 loss: 0.0013 lr: 0.02\n","iteration: 28070 loss: 0.0012 lr: 0.02\n","iteration: 28080 loss: 0.0012 lr: 0.02\n","iteration: 28090 loss: 0.0013 lr: 0.02\n","iteration: 28100 loss: 0.0011 lr: 0.02\n","iteration: 28110 loss: 0.0009 lr: 0.02\n","iteration: 28120 loss: 0.0009 lr: 0.02\n","iteration: 28130 loss: 0.0012 lr: 0.02\n","iteration: 28140 loss: 0.0011 lr: 0.02\n","iteration: 28150 loss: 0.0012 lr: 0.02\n","iteration: 28160 loss: 0.0012 lr: 0.02\n","iteration: 28170 loss: 0.0011 lr: 0.02\n","iteration: 28180 loss: 0.0011 lr: 0.02\n","iteration: 28190 loss: 0.0012 lr: 0.02\n","iteration: 28200 loss: 0.0015 lr: 0.02\n","iteration: 28210 loss: 0.0012 lr: 0.02\n","iteration: 28220 loss: 0.0011 lr: 0.02\n","iteration: 28230 loss: 0.0010 lr: 0.02\n","iteration: 28240 loss: 0.0015 lr: 0.02\n","iteration: 28250 loss: 0.0012 lr: 0.02\n","iteration: 28260 loss: 0.0014 lr: 0.02\n","iteration: 28270 loss: 0.0012 lr: 0.02\n","iteration: 28280 loss: 0.0012 lr: 0.02\n","iteration: 28290 loss: 0.0011 lr: 0.02\n","iteration: 28300 loss: 0.0011 lr: 0.02\n","iteration: 28310 loss: 0.0011 lr: 0.02\n","iteration: 28320 loss: 0.0011 lr: 0.02\n","iteration: 28330 loss: 0.0012 lr: 0.02\n","iteration: 28340 loss: 0.0012 lr: 0.02\n","iteration: 28350 loss: 0.0018 lr: 0.02\n","iteration: 28360 loss: 0.0012 lr: 0.02\n","iteration: 28370 loss: 0.0011 lr: 0.02\n","iteration: 28380 loss: 0.0012 lr: 0.02\n","iteration: 28390 loss: 0.0010 lr: 0.02\n","iteration: 28400 loss: 0.0016 lr: 0.02\n","iteration: 28410 loss: 0.0014 lr: 0.02\n","iteration: 28420 loss: 0.0012 lr: 0.02\n","iteration: 28430 loss: 0.0015 lr: 0.02\n","iteration: 28440 loss: 0.0013 lr: 0.02\n","iteration: 28450 loss: 0.0012 lr: 0.02\n","iteration: 28460 loss: 0.0010 lr: 0.02\n","iteration: 28470 loss: 0.0013 lr: 0.02\n","iteration: 28480 loss: 0.0012 lr: 0.02\n","iteration: 28490 loss: 0.0013 lr: 0.02\n","iteration: 28500 loss: 0.0015 lr: 0.02\n","iteration: 28510 loss: 0.0012 lr: 0.02\n","iteration: 28520 loss: 0.0013 lr: 0.02\n","iteration: 28530 loss: 0.0012 lr: 0.02\n","iteration: 28540 loss: 0.0010 lr: 0.02\n","iteration: 28550 loss: 0.0012 lr: 0.02\n","iteration: 28560 loss: 0.0015 lr: 0.02\n","iteration: 28570 loss: 0.0013 lr: 0.02\n","iteration: 28580 loss: 0.0014 lr: 0.02\n","iteration: 28590 loss: 0.0013 lr: 0.02\n","iteration: 28600 loss: 0.0011 lr: 0.02\n","iteration: 28610 loss: 0.0016 lr: 0.02\n","iteration: 28620 loss: 0.0009 lr: 0.02\n","iteration: 28630 loss: 0.0013 lr: 0.02\n","iteration: 28640 loss: 0.0012 lr: 0.02\n","iteration: 28650 loss: 0.0015 lr: 0.02\n","iteration: 28660 loss: 0.0012 lr: 0.02\n","iteration: 28670 loss: 0.0013 lr: 0.02\n","iteration: 28680 loss: 0.0010 lr: 0.02\n","iteration: 28690 loss: 0.0015 lr: 0.02\n","iteration: 28700 loss: 0.0013 lr: 0.02\n","iteration: 28710 loss: 0.0013 lr: 0.02\n","iteration: 28720 loss: 0.0013 lr: 0.02\n","iteration: 28730 loss: 0.0010 lr: 0.02\n","iteration: 28740 loss: 0.0012 lr: 0.02\n","iteration: 28750 loss: 0.0014 lr: 0.02\n","iteration: 28760 loss: 0.0010 lr: 0.02\n","iteration: 28770 loss: 0.0011 lr: 0.02\n","iteration: 28780 loss: 0.0013 lr: 0.02\n","iteration: 28790 loss: 0.0013 lr: 0.02\n","iteration: 28800 loss: 0.0013 lr: 0.02\n","iteration: 28810 loss: 0.0014 lr: 0.02\n","iteration: 28820 loss: 0.0014 lr: 0.02\n","iteration: 28830 loss: 0.0014 lr: 0.02\n","iteration: 28840 loss: 0.0013 lr: 0.02\n","iteration: 28850 loss: 0.0010 lr: 0.02\n","iteration: 28860 loss: 0.0013 lr: 0.02\n","iteration: 28870 loss: 0.0011 lr: 0.02\n","iteration: 28880 loss: 0.0012 lr: 0.02\n","iteration: 28890 loss: 0.0010 lr: 0.02\n","iteration: 28900 loss: 0.0011 lr: 0.02\n","iteration: 28910 loss: 0.0010 lr: 0.02\n","iteration: 28920 loss: 0.0010 lr: 0.02\n","iteration: 28930 loss: 0.0013 lr: 0.02\n","iteration: 28940 loss: 0.0012 lr: 0.02\n","iteration: 28950 loss: 0.0014 lr: 0.02\n","iteration: 28960 loss: 0.0013 lr: 0.02\n","iteration: 28970 loss: 0.0010 lr: 0.02\n","iteration: 28980 loss: 0.0013 lr: 0.02\n","iteration: 28990 loss: 0.0015 lr: 0.02\n","iteration: 29000 loss: 0.0014 lr: 0.02\n","iteration: 29010 loss: 0.0012 lr: 0.02\n","iteration: 29020 loss: 0.0012 lr: 0.02\n","iteration: 29030 loss: 0.0013 lr: 0.02\n","iteration: 29040 loss: 0.0014 lr: 0.02\n","iteration: 29050 loss: 0.0014 lr: 0.02\n","iteration: 29060 loss: 0.0013 lr: 0.02\n","iteration: 29070 loss: 0.0012 lr: 0.02\n","iteration: 29080 loss: 0.0010 lr: 0.02\n","iteration: 29090 loss: 0.0011 lr: 0.02\n","iteration: 29100 loss: 0.0010 lr: 0.02\n","iteration: 29110 loss: 0.0012 lr: 0.02\n","iteration: 29120 loss: 0.0012 lr: 0.02\n","iteration: 29130 loss: 0.0010 lr: 0.02\n","iteration: 29140 loss: 0.0011 lr: 0.02\n","iteration: 29150 loss: 0.0013 lr: 0.02\n","iteration: 29160 loss: 0.0014 lr: 0.02\n","iteration: 29170 loss: 0.0009 lr: 0.02\n","iteration: 29180 loss: 0.0013 lr: 0.02\n","iteration: 29190 loss: 0.0012 lr: 0.02\n","iteration: 29200 loss: 0.0012 lr: 0.02\n","iteration: 29210 loss: 0.0013 lr: 0.02\n","iteration: 29220 loss: 0.0011 lr: 0.02\n","iteration: 29230 loss: 0.0012 lr: 0.02\n","iteration: 29240 loss: 0.0011 lr: 0.02\n","iteration: 29250 loss: 0.0011 lr: 0.02\n","iteration: 29260 loss: 0.0010 lr: 0.02\n","iteration: 29270 loss: 0.0013 lr: 0.02\n","iteration: 29280 loss: 0.0018 lr: 0.02\n","iteration: 29290 loss: 0.0014 lr: 0.02\n","iteration: 29300 loss: 0.0014 lr: 0.02\n","iteration: 29310 loss: 0.0010 lr: 0.02\n","iteration: 29320 loss: 0.0011 lr: 0.02\n","iteration: 29330 loss: 0.0017 lr: 0.02\n","iteration: 29340 loss: 0.0013 lr: 0.02\n","iteration: 29350 loss: 0.0012 lr: 0.02\n","iteration: 29360 loss: 0.0018 lr: 0.02\n","iteration: 29370 loss: 0.0013 lr: 0.02\n","iteration: 29380 loss: 0.0013 lr: 0.02\n","iteration: 29390 loss: 0.0014 lr: 0.02\n","iteration: 29400 loss: 0.0014 lr: 0.02\n","iteration: 29410 loss: 0.0010 lr: 0.02\n","iteration: 29420 loss: 0.0014 lr: 0.02\n","iteration: 29430 loss: 0.0014 lr: 0.02\n","iteration: 29440 loss: 0.0013 lr: 0.02\n","iteration: 29450 loss: 0.0012 lr: 0.02\n","iteration: 29460 loss: 0.0011 lr: 0.02\n","iteration: 29470 loss: 0.0014 lr: 0.02\n","iteration: 29480 loss: 0.0013 lr: 0.02\n","iteration: 29490 loss: 0.0012 lr: 0.02\n","iteration: 29500 loss: 0.0010 lr: 0.02\n","iteration: 29510 loss: 0.0010 lr: 0.02\n","iteration: 29520 loss: 0.0011 lr: 0.02\n","iteration: 29530 loss: 0.0014 lr: 0.02\n","iteration: 29540 loss: 0.0013 lr: 0.02\n","iteration: 29550 loss: 0.0016 lr: 0.02\n","iteration: 29560 loss: 0.0013 lr: 0.02\n","iteration: 29570 loss: 0.0012 lr: 0.02\n","iteration: 29580 loss: 0.0012 lr: 0.02\n","iteration: 29590 loss: 0.0011 lr: 0.02\n","iteration: 29600 loss: 0.0010 lr: 0.02\n","iteration: 29610 loss: 0.0013 lr: 0.02\n","iteration: 29620 loss: 0.0011 lr: 0.02\n","iteration: 29630 loss: 0.0010 lr: 0.02\n","iteration: 29640 loss: 0.0012 lr: 0.02\n","iteration: 29650 loss: 0.0013 lr: 0.02\n","iteration: 29660 loss: 0.0015 lr: 0.02\n","iteration: 29670 loss: 0.0014 lr: 0.02\n","iteration: 29680 loss: 0.0011 lr: 0.02\n","iteration: 29690 loss: 0.0013 lr: 0.02\n","iteration: 29700 loss: 0.0012 lr: 0.02\n","iteration: 29710 loss: 0.0014 lr: 0.02\n","iteration: 29720 loss: 0.0012 lr: 0.02\n","iteration: 29730 loss: 0.0010 lr: 0.02\n","iteration: 29740 loss: 0.0013 lr: 0.02\n","iteration: 29750 loss: 0.0012 lr: 0.02\n","iteration: 29760 loss: 0.0011 lr: 0.02\n","iteration: 29770 loss: 0.0013 lr: 0.02\n","iteration: 29780 loss: 0.0011 lr: 0.02\n","iteration: 29790 loss: 0.0013 lr: 0.02\n","iteration: 29800 loss: 0.0013 lr: 0.02\n","iteration: 29810 loss: 0.0013 lr: 0.02\n","iteration: 29820 loss: 0.0011 lr: 0.02\n","iteration: 29830 loss: 0.0013 lr: 0.02\n","iteration: 29840 loss: 0.0014 lr: 0.02\n","iteration: 29850 loss: 0.0012 lr: 0.02\n","iteration: 29860 loss: 0.0014 lr: 0.02\n","iteration: 29870 loss: 0.0012 lr: 0.02\n","iteration: 29880 loss: 0.0010 lr: 0.02\n","iteration: 29890 loss: 0.0011 lr: 0.02\n","iteration: 29900 loss: 0.0011 lr: 0.02\n","iteration: 29910 loss: 0.0013 lr: 0.02\n","iteration: 29920 loss: 0.0012 lr: 0.02\n","iteration: 29930 loss: 0.0015 lr: 0.02\n","iteration: 29940 loss: 0.0012 lr: 0.02\n","iteration: 29950 loss: 0.0010 lr: 0.02\n","iteration: 29960 loss: 0.0011 lr: 0.02\n","iteration: 29970 loss: 0.0013 lr: 0.02\n","iteration: 29980 loss: 0.0013 lr: 0.02\n","iteration: 29990 loss: 0.0015 lr: 0.02\n","iteration: 30000 loss: 0.0012 lr: 0.02\n","iteration: 30010 loss: 0.0010 lr: 0.02\n","iteration: 30020 loss: 0.0012 lr: 0.02\n","iteration: 30030 loss: 0.0012 lr: 0.02\n","iteration: 30040 loss: 0.0011 lr: 0.02\n","iteration: 30050 loss: 0.0010 lr: 0.02\n","iteration: 30060 loss: 0.0010 lr: 0.02\n","iteration: 30070 loss: 0.0011 lr: 0.02\n","iteration: 30080 loss: 0.0012 lr: 0.02\n","iteration: 30090 loss: 0.0011 lr: 0.02\n","iteration: 30100 loss: 0.0016 lr: 0.02\n","iteration: 30110 loss: 0.0014 lr: 0.02\n","iteration: 30120 loss: 0.0012 lr: 0.02\n","iteration: 30130 loss: 0.0012 lr: 0.02\n","iteration: 30140 loss: 0.0013 lr: 0.02\n","iteration: 30150 loss: 0.0015 lr: 0.02\n","iteration: 30160 loss: 0.0011 lr: 0.02\n","iteration: 30170 loss: 0.0011 lr: 0.02\n","iteration: 30180 loss: 0.0011 lr: 0.02\n","iteration: 30190 loss: 0.0012 lr: 0.02\n","iteration: 30200 loss: 0.0012 lr: 0.02\n","iteration: 30210 loss: 0.0012 lr: 0.02\n","iteration: 30220 loss: 0.0014 lr: 0.02\n","iteration: 30230 loss: 0.0011 lr: 0.02\n","iteration: 30240 loss: 0.0012 lr: 0.02\n","iteration: 30250 loss: 0.0014 lr: 0.02\n","iteration: 30260 loss: 0.0016 lr: 0.02\n","iteration: 30270 loss: 0.0012 lr: 0.02\n","iteration: 30280 loss: 0.0011 lr: 0.02\n","iteration: 30290 loss: 0.0011 lr: 0.02\n","iteration: 30300 loss: 0.0010 lr: 0.02\n","iteration: 30310 loss: 0.0012 lr: 0.02\n","iteration: 30320 loss: 0.0012 lr: 0.02\n","iteration: 30330 loss: 0.0014 lr: 0.02\n","iteration: 30340 loss: 0.0011 lr: 0.02\n","iteration: 30350 loss: 0.0011 lr: 0.02\n","iteration: 30360 loss: 0.0013 lr: 0.02\n","iteration: 30370 loss: 0.0008 lr: 0.02\n","iteration: 30380 loss: 0.0011 lr: 0.02\n","iteration: 30390 loss: 0.0012 lr: 0.02\n","iteration: 30400 loss: 0.0013 lr: 0.02\n","iteration: 30410 loss: 0.0013 lr: 0.02\n","iteration: 30420 loss: 0.0012 lr: 0.02\n","iteration: 30430 loss: 0.0013 lr: 0.02\n","iteration: 30440 loss: 0.0013 lr: 0.02\n","iteration: 30450 loss: 0.0015 lr: 0.02\n","iteration: 30460 loss: 0.0014 lr: 0.02\n","iteration: 30470 loss: 0.0011 lr: 0.02\n","iteration: 30480 loss: 0.0016 lr: 0.02\n","iteration: 30490 loss: 0.0011 lr: 0.02\n","iteration: 30500 loss: 0.0009 lr: 0.02\n","iteration: 30510 loss: 0.0008 lr: 0.02\n","iteration: 30520 loss: 0.0014 lr: 0.02\n","iteration: 30530 loss: 0.0012 lr: 0.02\n","iteration: 30540 loss: 0.0010 lr: 0.02\n","iteration: 30550 loss: 0.0010 lr: 0.02\n","iteration: 30560 loss: 0.0010 lr: 0.02\n","iteration: 30570 loss: 0.0014 lr: 0.02\n","iteration: 30580 loss: 0.0010 lr: 0.02\n","iteration: 30590 loss: 0.0012 lr: 0.02\n","iteration: 30600 loss: 0.0010 lr: 0.02\n","iteration: 30610 loss: 0.0013 lr: 0.02\n","iteration: 30620 loss: 0.0011 lr: 0.02\n","iteration: 30630 loss: 0.0015 lr: 0.02\n","iteration: 30640 loss: 0.0011 lr: 0.02\n","iteration: 30650 loss: 0.0012 lr: 0.02\n","iteration: 30660 loss: 0.0012 lr: 0.02\n","iteration: 30670 loss: 0.0012 lr: 0.02\n","iteration: 30680 loss: 0.0010 lr: 0.02\n","iteration: 30690 loss: 0.0014 lr: 0.02\n","iteration: 30700 loss: 0.0011 lr: 0.02\n","iteration: 30710 loss: 0.0012 lr: 0.02\n","iteration: 30720 loss: 0.0014 lr: 0.02\n","iteration: 30730 loss: 0.0012 lr: 0.02\n","iteration: 30740 loss: 0.0011 lr: 0.02\n","iteration: 30750 loss: 0.0010 lr: 0.02\n","iteration: 30760 loss: 0.0013 lr: 0.02\n","iteration: 30770 loss: 0.0010 lr: 0.02\n","iteration: 30780 loss: 0.0014 lr: 0.02\n","iteration: 30790 loss: 0.0015 lr: 0.02\n","iteration: 30800 loss: 0.0012 lr: 0.02\n","iteration: 30810 loss: 0.0013 lr: 0.02\n","iteration: 30820 loss: 0.0011 lr: 0.02\n","iteration: 30830 loss: 0.0011 lr: 0.02\n","iteration: 30840 loss: 0.0012 lr: 0.02\n","iteration: 30850 loss: 0.0012 lr: 0.02\n","iteration: 30860 loss: 0.0010 lr: 0.02\n","iteration: 30870 loss: 0.0014 lr: 0.02\n","iteration: 30880 loss: 0.0014 lr: 0.02\n","iteration: 30890 loss: 0.0012 lr: 0.02\n","iteration: 30900 loss: 0.0011 lr: 0.02\n","iteration: 30910 loss: 0.0013 lr: 0.02\n","iteration: 30920 loss: 0.0015 lr: 0.02\n","iteration: 30930 loss: 0.0013 lr: 0.02\n","iteration: 30940 loss: 0.0012 lr: 0.02\n","iteration: 30950 loss: 0.0013 lr: 0.02\n","iteration: 30960 loss: 0.0015 lr: 0.02\n","iteration: 30970 loss: 0.0010 lr: 0.02\n","iteration: 30980 loss: 0.0012 lr: 0.02\n","iteration: 30990 loss: 0.0013 lr: 0.02\n","iteration: 31000 loss: 0.0013 lr: 0.02\n","iteration: 31010 loss: 0.0011 lr: 0.02\n","iteration: 31020 loss: 0.0010 lr: 0.02\n","iteration: 31030 loss: 0.0010 lr: 0.02\n","iteration: 31040 loss: 0.0011 lr: 0.02\n","iteration: 31050 loss: 0.0009 lr: 0.02\n","iteration: 31060 loss: 0.0014 lr: 0.02\n","iteration: 31070 loss: 0.0017 lr: 0.02\n","iteration: 31080 loss: 0.0011 lr: 0.02\n","iteration: 31090 loss: 0.0013 lr: 0.02\n","iteration: 31100 loss: 0.0011 lr: 0.02\n","iteration: 31110 loss: 0.0011 lr: 0.02\n","iteration: 31120 loss: 0.0010 lr: 0.02\n","iteration: 31130 loss: 0.0010 lr: 0.02\n","iteration: 31140 loss: 0.0011 lr: 0.02\n","iteration: 31150 loss: 0.0012 lr: 0.02\n","iteration: 31160 loss: 0.0013 lr: 0.02\n","iteration: 31170 loss: 0.0015 lr: 0.02\n","iteration: 31180 loss: 0.0012 lr: 0.02\n","iteration: 31190 loss: 0.0012 lr: 0.02\n","iteration: 31200 loss: 0.0014 lr: 0.02\n","iteration: 31210 loss: 0.0011 lr: 0.02\n","iteration: 31220 loss: 0.0013 lr: 0.02\n","iteration: 31230 loss: 0.0010 lr: 0.02\n","iteration: 31240 loss: 0.0010 lr: 0.02\n","iteration: 31250 loss: 0.0015 lr: 0.02\n","iteration: 31260 loss: 0.0014 lr: 0.02\n","iteration: 31270 loss: 0.0013 lr: 0.02\n","iteration: 31280 loss: 0.0013 lr: 0.02\n","iteration: 31290 loss: 0.0012 lr: 0.02\n","iteration: 31300 loss: 0.0012 lr: 0.02\n","iteration: 31310 loss: 0.0013 lr: 0.02\n","iteration: 31320 loss: 0.0010 lr: 0.02\n","iteration: 31330 loss: 0.0011 lr: 0.02\n","iteration: 31340 loss: 0.0010 lr: 0.02\n","iteration: 31350 loss: 0.0011 lr: 0.02\n","iteration: 31360 loss: 0.0011 lr: 0.02\n","iteration: 31370 loss: 0.0012 lr: 0.02\n","iteration: 31380 loss: 0.0012 lr: 0.02\n","iteration: 31390 loss: 0.0011 lr: 0.02\n","iteration: 31400 loss: 0.0012 lr: 0.02\n","iteration: 31410 loss: 0.0015 lr: 0.02\n","iteration: 31420 loss: 0.0013 lr: 0.02\n","iteration: 31430 loss: 0.0013 lr: 0.02\n","iteration: 31440 loss: 0.0016 lr: 0.02\n","iteration: 31450 loss: 0.0015 lr: 0.02\n","iteration: 31460 loss: 0.0010 lr: 0.02\n","iteration: 31470 loss: 0.0012 lr: 0.02\n","iteration: 31480 loss: 0.0014 lr: 0.02\n","iteration: 31490 loss: 0.0011 lr: 0.02\n","iteration: 31500 loss: 0.0013 lr: 0.02\n","iteration: 31510 loss: 0.0015 lr: 0.02\n","iteration: 31520 loss: 0.0009 lr: 0.02\n","iteration: 31530 loss: 0.0011 lr: 0.02\n","iteration: 31540 loss: 0.0010 lr: 0.02\n","iteration: 31550 loss: 0.0010 lr: 0.02\n","iteration: 31560 loss: 0.0010 lr: 0.02\n","iteration: 31570 loss: 0.0010 lr: 0.02\n","iteration: 31580 loss: 0.0011 lr: 0.02\n","iteration: 31590 loss: 0.0011 lr: 0.02\n","iteration: 31600 loss: 0.0009 lr: 0.02\n","iteration: 31610 loss: 0.0014 lr: 0.02\n","iteration: 31620 loss: 0.0014 lr: 0.02\n","iteration: 31630 loss: 0.0011 lr: 0.02\n","iteration: 31640 loss: 0.0012 lr: 0.02\n","iteration: 31650 loss: 0.0013 lr: 0.02\n","iteration: 31660 loss: 0.0012 lr: 0.02\n","iteration: 31670 loss: 0.0012 lr: 0.02\n","iteration: 31680 loss: 0.0010 lr: 0.02\n","iteration: 31690 loss: 0.0012 lr: 0.02\n","iteration: 31700 loss: 0.0014 lr: 0.02\n","iteration: 31710 loss: 0.0010 lr: 0.02\n","iteration: 31720 loss: 0.0012 lr: 0.02\n","iteration: 31730 loss: 0.0013 lr: 0.02\n","iteration: 31740 loss: 0.0012 lr: 0.02\n","iteration: 31750 loss: 0.0012 lr: 0.02\n","iteration: 31760 loss: 0.0010 lr: 0.02\n","iteration: 31770 loss: 0.0023 lr: 0.02\n","iteration: 31780 loss: 0.0017 lr: 0.02\n","iteration: 31790 loss: 0.0012 lr: 0.02\n","iteration: 31800 loss: 0.0011 lr: 0.02\n","iteration: 31810 loss: 0.0015 lr: 0.02\n","iteration: 31820 loss: 0.0014 lr: 0.02\n","iteration: 31830 loss: 0.0014 lr: 0.02\n","iteration: 31840 loss: 0.0011 lr: 0.02\n","iteration: 31850 loss: 0.0011 lr: 0.02\n","iteration: 31860 loss: 0.0011 lr: 0.02\n","iteration: 31870 loss: 0.0011 lr: 0.02\n","iteration: 31880 loss: 0.0013 lr: 0.02\n","iteration: 31890 loss: 0.0012 lr: 0.02\n","iteration: 31900 loss: 0.0012 lr: 0.02\n","iteration: 31910 loss: 0.0010 lr: 0.02\n","iteration: 31920 loss: 0.0014 lr: 0.02\n","iteration: 31930 loss: 0.0012 lr: 0.02\n","iteration: 31940 loss: 0.0012 lr: 0.02\n","iteration: 31950 loss: 0.0014 lr: 0.02\n","iteration: 31960 loss: 0.0014 lr: 0.02\n","iteration: 31970 loss: 0.0011 lr: 0.02\n","iteration: 31980 loss: 0.0014 lr: 0.02\n","iteration: 31990 loss: 0.0010 lr: 0.02\n","iteration: 32000 loss: 0.0011 lr: 0.02\n","iteration: 32010 loss: 0.0010 lr: 0.02\n","iteration: 32020 loss: 0.0013 lr: 0.02\n","iteration: 32030 loss: 0.0012 lr: 0.02\n","iteration: 32040 loss: 0.0011 lr: 0.02\n","iteration: 32050 loss: 0.0015 lr: 0.02\n","iteration: 32060 loss: 0.0010 lr: 0.02\n","iteration: 32070 loss: 0.0013 lr: 0.02\n","iteration: 32080 loss: 0.0012 lr: 0.02\n","iteration: 32090 loss: 0.0012 lr: 0.02\n","iteration: 32100 loss: 0.0012 lr: 0.02\n","iteration: 32110 loss: 0.0012 lr: 0.02\n","iteration: 32120 loss: 0.0009 lr: 0.02\n","iteration: 32130 loss: 0.0012 lr: 0.02\n","iteration: 32140 loss: 0.0012 lr: 0.02\n","iteration: 32150 loss: 0.0010 lr: 0.02\n","iteration: 32160 loss: 0.0010 lr: 0.02\n","iteration: 32170 loss: 0.0011 lr: 0.02\n","iteration: 32180 loss: 0.0010 lr: 0.02\n","iteration: 32190 loss: 0.0012 lr: 0.02\n","iteration: 32200 loss: 0.0012 lr: 0.02\n","iteration: 32210 loss: 0.0011 lr: 0.02\n","iteration: 32220 loss: 0.0013 lr: 0.02\n","iteration: 32230 loss: 0.0011 lr: 0.02\n","iteration: 32240 loss: 0.0013 lr: 0.02\n","iteration: 32250 loss: 0.0014 lr: 0.02\n","iteration: 32260 loss: 0.0011 lr: 0.02\n","iteration: 32270 loss: 0.0009 lr: 0.02\n","iteration: 32280 loss: 0.0011 lr: 0.02\n","iteration: 32290 loss: 0.0015 lr: 0.02\n","iteration: 32300 loss: 0.0012 lr: 0.02\n","iteration: 32310 loss: 0.0012 lr: 0.02\n","iteration: 32320 loss: 0.0013 lr: 0.02\n","iteration: 32330 loss: 0.0012 lr: 0.02\n","iteration: 32340 loss: 0.0014 lr: 0.02\n","iteration: 32350 loss: 0.0011 lr: 0.02\n","iteration: 32360 loss: 0.0010 lr: 0.02\n","iteration: 32370 loss: 0.0012 lr: 0.02\n","iteration: 32380 loss: 0.0011 lr: 0.02\n","iteration: 32390 loss: 0.0015 lr: 0.02\n","iteration: 32400 loss: 0.0014 lr: 0.02\n","iteration: 32410 loss: 0.0010 lr: 0.02\n","iteration: 32420 loss: 0.0012 lr: 0.02\n","iteration: 32430 loss: 0.0011 lr: 0.02\n","iteration: 32440 loss: 0.0014 lr: 0.02\n","iteration: 32450 loss: 0.0011 lr: 0.02\n","iteration: 32460 loss: 0.0011 lr: 0.02\n","iteration: 32470 loss: 0.0013 lr: 0.02\n","iteration: 32480 loss: 0.0013 lr: 0.02\n","iteration: 32490 loss: 0.0014 lr: 0.02\n","iteration: 32500 loss: 0.0009 lr: 0.02\n","iteration: 32510 loss: 0.0009 lr: 0.02\n","iteration: 32520 loss: 0.0014 lr: 0.02\n","iteration: 32530 loss: 0.0012 lr: 0.02\n","iteration: 32540 loss: 0.0014 lr: 0.02\n","iteration: 32550 loss: 0.0014 lr: 0.02\n","iteration: 32560 loss: 0.0014 lr: 0.02\n","iteration: 32570 loss: 0.0012 lr: 0.02\n","iteration: 32580 loss: 0.0011 lr: 0.02\n","iteration: 32590 loss: 0.0011 lr: 0.02\n","iteration: 32600 loss: 0.0015 lr: 0.02\n","iteration: 32610 loss: 0.0013 lr: 0.02\n","iteration: 32620 loss: 0.0010 lr: 0.02\n","iteration: 32630 loss: 0.0015 lr: 0.02\n","iteration: 32640 loss: 0.0012 lr: 0.02\n","iteration: 32650 loss: 0.0012 lr: 0.02\n","iteration: 32660 loss: 0.0014 lr: 0.02\n","iteration: 32670 loss: 0.0012 lr: 0.02\n","iteration: 32680 loss: 0.0013 lr: 0.02\n","iteration: 32690 loss: 0.0014 lr: 0.02\n","iteration: 32700 loss: 0.0019 lr: 0.02\n","iteration: 32710 loss: 0.0012 lr: 0.02\n","iteration: 32720 loss: 0.0012 lr: 0.02\n","iteration: 32730 loss: 0.0012 lr: 0.02\n","iteration: 32740 loss: 0.0013 lr: 0.02\n","iteration: 32750 loss: 0.0010 lr: 0.02\n","iteration: 32760 loss: 0.0016 lr: 0.02\n","iteration: 32770 loss: 0.0013 lr: 0.02\n","iteration: 32780 loss: 0.0011 lr: 0.02\n","iteration: 32790 loss: 0.0012 lr: 0.02\n","iteration: 32800 loss: 0.0011 lr: 0.02\n","iteration: 32810 loss: 0.0010 lr: 0.02\n","iteration: 32820 loss: 0.0012 lr: 0.02\n","iteration: 32830 loss: 0.0015 lr: 0.02\n","iteration: 32840 loss: 0.0011 lr: 0.02\n","iteration: 32850 loss: 0.0011 lr: 0.02\n","iteration: 32860 loss: 0.0011 lr: 0.02\n","iteration: 32870 loss: 0.0012 lr: 0.02\n","iteration: 32880 loss: 0.0014 lr: 0.02\n","iteration: 32890 loss: 0.0013 lr: 0.02\n","iteration: 32900 loss: 0.0013 lr: 0.02\n","iteration: 32910 loss: 0.0011 lr: 0.02\n","iteration: 32920 loss: 0.0009 lr: 0.02\n","iteration: 32930 loss: 0.0009 lr: 0.02\n","iteration: 32940 loss: 0.0013 lr: 0.02\n","iteration: 32950 loss: 0.0011 lr: 0.02\n","iteration: 32960 loss: 0.0012 lr: 0.02\n","iteration: 32970 loss: 0.0012 lr: 0.02\n","iteration: 32980 loss: 0.0012 lr: 0.02\n","iteration: 32990 loss: 0.0012 lr: 0.02\n","iteration: 33000 loss: 0.0014 lr: 0.02\n","iteration: 33010 loss: 0.0012 lr: 0.02\n","iteration: 33020 loss: 0.0012 lr: 0.02\n","iteration: 33030 loss: 0.0010 lr: 0.02\n","iteration: 33040 loss: 0.0013 lr: 0.02\n","iteration: 33050 loss: 0.0013 lr: 0.02\n","iteration: 33060 loss: 0.0011 lr: 0.02\n","iteration: 33070 loss: 0.0013 lr: 0.02\n","iteration: 33080 loss: 0.0011 lr: 0.02\n","iteration: 33090 loss: 0.0010 lr: 0.02\n","iteration: 33100 loss: 0.0012 lr: 0.02\n","iteration: 33110 loss: 0.0012 lr: 0.02\n","iteration: 33120 loss: 0.0013 lr: 0.02\n","iteration: 33130 loss: 0.0011 lr: 0.02\n","iteration: 33140 loss: 0.0011 lr: 0.02\n","iteration: 33150 loss: 0.0011 lr: 0.02\n","iteration: 33160 loss: 0.0014 lr: 0.02\n","iteration: 33170 loss: 0.0012 lr: 0.02\n","iteration: 33180 loss: 0.0013 lr: 0.02\n","iteration: 33190 loss: 0.0013 lr: 0.02\n","iteration: 33200 loss: 0.0013 lr: 0.02\n","iteration: 33210 loss: 0.0011 lr: 0.02\n","iteration: 33220 loss: 0.0014 lr: 0.02\n","iteration: 33230 loss: 0.0010 lr: 0.02\n","iteration: 33240 loss: 0.0016 lr: 0.02\n","iteration: 33250 loss: 0.0017 lr: 0.02\n","iteration: 33260 loss: 0.0013 lr: 0.02\n","iteration: 33270 loss: 0.0015 lr: 0.02\n","iteration: 33280 loss: 0.0011 lr: 0.02\n","iteration: 33290 loss: 0.0013 lr: 0.02\n","iteration: 33300 loss: 0.0013 lr: 0.02\n","iteration: 33310 loss: 0.0009 lr: 0.02\n","iteration: 33320 loss: 0.0011 lr: 0.02\n","iteration: 33330 loss: 0.0014 lr: 0.02\n","iteration: 33340 loss: 0.0014 lr: 0.02\n","iteration: 33350 loss: 0.0012 lr: 0.02\n","iteration: 33360 loss: 0.0016 lr: 0.02\n","iteration: 33370 loss: 0.0010 lr: 0.02\n","iteration: 33380 loss: 0.0013 lr: 0.02\n","iteration: 33390 loss: 0.0010 lr: 0.02\n","iteration: 33400 loss: 0.0010 lr: 0.02\n","iteration: 33410 loss: 0.0014 lr: 0.02\n","iteration: 33420 loss: 0.0014 lr: 0.02\n","iteration: 33430 loss: 0.0012 lr: 0.02\n","iteration: 33440 loss: 0.0012 lr: 0.02\n","iteration: 33450 loss: 0.0016 lr: 0.02\n","iteration: 33460 loss: 0.0012 lr: 0.02\n","iteration: 33470 loss: 0.0011 lr: 0.02\n","iteration: 33480 loss: 0.0012 lr: 0.02\n","iteration: 33490 loss: 0.0011 lr: 0.02\n","iteration: 33500 loss: 0.0010 lr: 0.02\n","iteration: 33510 loss: 0.0011 lr: 0.02\n","iteration: 33520 loss: 0.0012 lr: 0.02\n","iteration: 33530 loss: 0.0012 lr: 0.02\n","iteration: 33540 loss: 0.0009 lr: 0.02\n","iteration: 33550 loss: 0.0011 lr: 0.02\n","iteration: 33560 loss: 0.0011 lr: 0.02\n","iteration: 33570 loss: 0.0015 lr: 0.02\n","iteration: 33580 loss: 0.0014 lr: 0.02\n","iteration: 33590 loss: 0.0014 lr: 0.02\n","iteration: 33600 loss: 0.0011 lr: 0.02\n","iteration: 33610 loss: 0.0014 lr: 0.02\n","iteration: 33620 loss: 0.0010 lr: 0.02\n","iteration: 33630 loss: 0.0015 lr: 0.02\n","iteration: 33640 loss: 0.0012 lr: 0.02\n","iteration: 33650 loss: 0.0010 lr: 0.02\n","iteration: 33660 loss: 0.0010 lr: 0.02\n","iteration: 33670 loss: 0.0010 lr: 0.02\n","iteration: 33680 loss: 0.0012 lr: 0.02\n","iteration: 33690 loss: 0.0012 lr: 0.02\n","iteration: 33700 loss: 0.0008 lr: 0.02\n","iteration: 33710 loss: 0.0013 lr: 0.02\n","iteration: 33720 loss: 0.0010 lr: 0.02\n","iteration: 33730 loss: 0.0012 lr: 0.02\n","iteration: 33740 loss: 0.0016 lr: 0.02\n","iteration: 33750 loss: 0.0013 lr: 0.02\n","iteration: 33760 loss: 0.0013 lr: 0.02\n","iteration: 33770 loss: 0.0016 lr: 0.02\n","iteration: 33780 loss: 0.0013 lr: 0.02\n","iteration: 33790 loss: 0.0014 lr: 0.02\n","iteration: 33800 loss: 0.0011 lr: 0.02\n","iteration: 33810 loss: 0.0013 lr: 0.02\n","iteration: 33820 loss: 0.0013 lr: 0.02\n","iteration: 33830 loss: 0.0012 lr: 0.02\n","iteration: 33840 loss: 0.0012 lr: 0.02\n","iteration: 33850 loss: 0.0011 lr: 0.02\n","iteration: 33860 loss: 0.0014 lr: 0.02\n","iteration: 33870 loss: 0.0015 lr: 0.02\n","iteration: 33880 loss: 0.0009 lr: 0.02\n","iteration: 33890 loss: 0.0012 lr: 0.02\n","iteration: 33900 loss: 0.0012 lr: 0.02\n","iteration: 33910 loss: 0.0008 lr: 0.02\n","iteration: 33920 loss: 0.0010 lr: 0.02\n","iteration: 33930 loss: 0.0012 lr: 0.02\n","iteration: 33940 loss: 0.0016 lr: 0.02\n","iteration: 33950 loss: 0.0011 lr: 0.02\n","iteration: 33960 loss: 0.0009 lr: 0.02\n","iteration: 33970 loss: 0.0011 lr: 0.02\n","iteration: 33980 loss: 0.0011 lr: 0.02\n","iteration: 33990 loss: 0.0015 lr: 0.02\n","iteration: 34000 loss: 0.0014 lr: 0.02\n","iteration: 34010 loss: 0.0011 lr: 0.02\n","iteration: 34020 loss: 0.0014 lr: 0.02\n","iteration: 34030 loss: 0.0014 lr: 0.02\n","iteration: 34040 loss: 0.0016 lr: 0.02\n","iteration: 34050 loss: 0.0011 lr: 0.02\n","iteration: 34060 loss: 0.0013 lr: 0.02\n","iteration: 34070 loss: 0.0014 lr: 0.02\n","iteration: 34080 loss: 0.0013 lr: 0.02\n","iteration: 34090 loss: 0.0017 lr: 0.02\n","iteration: 34100 loss: 0.0014 lr: 0.02\n","iteration: 34110 loss: 0.0011 lr: 0.02\n","iteration: 34120 loss: 0.0012 lr: 0.02\n","iteration: 34130 loss: 0.0014 lr: 0.02\n","iteration: 34140 loss: 0.0013 lr: 0.02\n","iteration: 34150 loss: 0.0013 lr: 0.02\n","iteration: 34160 loss: 0.0010 lr: 0.02\n","iteration: 34170 loss: 0.0015 lr: 0.02\n","iteration: 34180 loss: 0.0013 lr: 0.02\n","iteration: 34190 loss: 0.0014 lr: 0.02\n","iteration: 34200 loss: 0.0013 lr: 0.02\n","iteration: 34210 loss: 0.0012 lr: 0.02\n","iteration: 34220 loss: 0.0012 lr: 0.02\n","iteration: 34230 loss: 0.0010 lr: 0.02\n","iteration: 34240 loss: 0.0011 lr: 0.02\n","iteration: 34250 loss: 0.0014 lr: 0.02\n","iteration: 34260 loss: 0.0011 lr: 0.02\n","iteration: 34270 loss: 0.0014 lr: 0.02\n","iteration: 34280 loss: 0.0013 lr: 0.02\n","iteration: 34290 loss: 0.0010 lr: 0.02\n","iteration: 34300 loss: 0.0012 lr: 0.02\n","iteration: 34310 loss: 0.0010 lr: 0.02\n","iteration: 34320 loss: 0.0011 lr: 0.02\n","iteration: 34330 loss: 0.0015 lr: 0.02\n","iteration: 34340 loss: 0.0012 lr: 0.02\n","iteration: 34350 loss: 0.0011 lr: 0.02\n","iteration: 34360 loss: 0.0015 lr: 0.02\n","iteration: 34370 loss: 0.0014 lr: 0.02\n","iteration: 34380 loss: 0.0010 lr: 0.02\n","iteration: 34390 loss: 0.0009 lr: 0.02\n","iteration: 34400 loss: 0.0009 lr: 0.02\n","iteration: 34410 loss: 0.0012 lr: 0.02\n","iteration: 34420 loss: 0.0010 lr: 0.02\n","iteration: 34430 loss: 0.0014 lr: 0.02\n","iteration: 34440 loss: 0.0013 lr: 0.02\n","iteration: 34450 loss: 0.0015 lr: 0.02\n","iteration: 34460 loss: 0.0011 lr: 0.02\n","iteration: 34470 loss: 0.0012 lr: 0.02\n","iteration: 34480 loss: 0.0010 lr: 0.02\n","iteration: 34490 loss: 0.0010 lr: 0.02\n","iteration: 34500 loss: 0.0012 lr: 0.02\n","iteration: 34510 loss: 0.0011 lr: 0.02\n","iteration: 34520 loss: 0.0013 lr: 0.02\n","iteration: 34530 loss: 0.0015 lr: 0.02\n","iteration: 34540 loss: 0.0012 lr: 0.02\n","iteration: 34550 loss: 0.0012 lr: 0.02\n","iteration: 34560 loss: 0.0012 lr: 0.02\n","iteration: 34570 loss: 0.0011 lr: 0.02\n","iteration: 34580 loss: 0.0015 lr: 0.02\n","iteration: 34590 loss: 0.0013 lr: 0.02\n","iteration: 34600 loss: 0.0014 lr: 0.02\n","iteration: 34610 loss: 0.0012 lr: 0.02\n","iteration: 34620 loss: 0.0012 lr: 0.02\n","iteration: 34630 loss: 0.0011 lr: 0.02\n","iteration: 34640 loss: 0.0012 lr: 0.02\n","iteration: 34650 loss: 0.0013 lr: 0.02\n","iteration: 34660 loss: 0.0015 lr: 0.02\n","iteration: 34670 loss: 0.0014 lr: 0.02\n","iteration: 34680 loss: 0.0012 lr: 0.02\n","iteration: 34690 loss: 0.0011 lr: 0.02\n","iteration: 34700 loss: 0.0011 lr: 0.02\n","iteration: 34710 loss: 0.0012 lr: 0.02\n","iteration: 34720 loss: 0.0013 lr: 0.02\n","iteration: 34730 loss: 0.0012 lr: 0.02\n","iteration: 34740 loss: 0.0012 lr: 0.02\n","iteration: 34750 loss: 0.0013 lr: 0.02\n","iteration: 34760 loss: 0.0010 lr: 0.02\n","iteration: 34770 loss: 0.0012 lr: 0.02\n","iteration: 34780 loss: 0.0011 lr: 0.02\n","iteration: 34790 loss: 0.0011 lr: 0.02\n","iteration: 34800 loss: 0.0010 lr: 0.02\n","iteration: 34810 loss: 0.0012 lr: 0.02\n","iteration: 34820 loss: 0.0010 lr: 0.02\n","iteration: 34830 loss: 0.0014 lr: 0.02\n","iteration: 34840 loss: 0.0013 lr: 0.02\n","iteration: 34850 loss: 0.0015 lr: 0.02\n","iteration: 34860 loss: 0.0010 lr: 0.02\n","iteration: 34870 loss: 0.0014 lr: 0.02\n","iteration: 34880 loss: 0.0011 lr: 0.02\n","iteration: 34890 loss: 0.0013 lr: 0.02\n","iteration: 34900 loss: 0.0011 lr: 0.02\n","iteration: 34910 loss: 0.0011 lr: 0.02\n","iteration: 34920 loss: 0.0011 lr: 0.02\n","iteration: 34930 loss: 0.0010 lr: 0.02\n","iteration: 34940 loss: 0.0019 lr: 0.02\n","iteration: 34950 loss: 0.0010 lr: 0.02\n","iteration: 34960 loss: 0.0013 lr: 0.02\n","iteration: 34970 loss: 0.0016 lr: 0.02\n","iteration: 34980 loss: 0.0012 lr: 0.02\n","iteration: 34990 loss: 0.0015 lr: 0.02\n","iteration: 35000 loss: 0.0009 lr: 0.02\n","iteration: 35010 loss: 0.0010 lr: 0.02\n","iteration: 35020 loss: 0.0010 lr: 0.02\n","iteration: 35030 loss: 0.0013 lr: 0.02\n","iteration: 35040 loss: 0.0012 lr: 0.02\n","iteration: 35050 loss: 0.0010 lr: 0.02\n","iteration: 35060 loss: 0.0011 lr: 0.02\n","iteration: 35070 loss: 0.0013 lr: 0.02\n","iteration: 35080 loss: 0.0016 lr: 0.02\n","iteration: 35090 loss: 0.0010 lr: 0.02\n","iteration: 35100 loss: 0.0012 lr: 0.02\n","iteration: 35110 loss: 0.0012 lr: 0.02\n","iteration: 35120 loss: 0.0011 lr: 0.02\n","iteration: 35130 loss: 0.0011 lr: 0.02\n","iteration: 35140 loss: 0.0014 lr: 0.02\n","iteration: 35150 loss: 0.0011 lr: 0.02\n","iteration: 35160 loss: 0.0014 lr: 0.02\n","iteration: 35170 loss: 0.0009 lr: 0.02\n","iteration: 35180 loss: 0.0013 lr: 0.02\n","iteration: 35190 loss: 0.0011 lr: 0.02\n","iteration: 35200 loss: 0.0011 lr: 0.02\n","iteration: 35210 loss: 0.0012 lr: 0.02\n","iteration: 35220 loss: 0.0012 lr: 0.02\n","iteration: 35230 loss: 0.0012 lr: 0.02\n","iteration: 35240 loss: 0.0013 lr: 0.02\n","iteration: 35250 loss: 0.0010 lr: 0.02\n","iteration: 35260 loss: 0.0012 lr: 0.02\n","iteration: 35270 loss: 0.0013 lr: 0.02\n","iteration: 35280 loss: 0.0012 lr: 0.02\n","iteration: 35290 loss: 0.0013 lr: 0.02\n","iteration: 35300 loss: 0.0011 lr: 0.02\n","iteration: 35310 loss: 0.0013 lr: 0.02\n","iteration: 35320 loss: 0.0012 lr: 0.02\n","iteration: 35330 loss: 0.0011 lr: 0.02\n","iteration: 35340 loss: 0.0011 lr: 0.02\n","iteration: 35350 loss: 0.0010 lr: 0.02\n","iteration: 35360 loss: 0.0013 lr: 0.02\n","iteration: 35370 loss: 0.0011 lr: 0.02\n","iteration: 35380 loss: 0.0011 lr: 0.02\n","iteration: 35390 loss: 0.0013 lr: 0.02\n","iteration: 35400 loss: 0.0012 lr: 0.02\n","iteration: 35410 loss: 0.0015 lr: 0.02\n","iteration: 35420 loss: 0.0013 lr: 0.02\n","iteration: 35430 loss: 0.0013 lr: 0.02\n","iteration: 35440 loss: 0.0012 lr: 0.02\n","iteration: 35450 loss: 0.0010 lr: 0.02\n","iteration: 35460 loss: 0.0010 lr: 0.02\n","iteration: 35470 loss: 0.0017 lr: 0.02\n","iteration: 35480 loss: 0.0013 lr: 0.02\n","iteration: 35490 loss: 0.0016 lr: 0.02\n","iteration: 35500 loss: 0.0012 lr: 0.02\n","iteration: 35510 loss: 0.0009 lr: 0.02\n","iteration: 35520 loss: 0.0012 lr: 0.02\n","iteration: 35530 loss: 0.0012 lr: 0.02\n","iteration: 35540 loss: 0.0017 lr: 0.02\n","iteration: 35550 loss: 0.0012 lr: 0.02\n","iteration: 35560 loss: 0.0012 lr: 0.02\n","iteration: 35570 loss: 0.0011 lr: 0.02\n","iteration: 35580 loss: 0.0012 lr: 0.02\n","iteration: 35590 loss: 0.0010 lr: 0.02\n","iteration: 35600 loss: 0.0011 lr: 0.02\n","iteration: 35610 loss: 0.0013 lr: 0.02\n","iteration: 35620 loss: 0.0012 lr: 0.02\n","iteration: 35630 loss: 0.0017 lr: 0.02\n","iteration: 35640 loss: 0.0013 lr: 0.02\n","iteration: 35650 loss: 0.0012 lr: 0.02\n","iteration: 35660 loss: 0.0012 lr: 0.02\n","iteration: 35670 loss: 0.0012 lr: 0.02\n","iteration: 35680 loss: 0.0013 lr: 0.02\n","iteration: 35690 loss: 0.0009 lr: 0.02\n","iteration: 35700 loss: 0.0011 lr: 0.02\n","iteration: 35710 loss: 0.0009 lr: 0.02\n","iteration: 35720 loss: 0.0015 lr: 0.02\n","iteration: 35730 loss: 0.0016 lr: 0.02\n","iteration: 35740 loss: 0.0010 lr: 0.02\n","iteration: 35750 loss: 0.0011 lr: 0.02\n","iteration: 35760 loss: 0.0011 lr: 0.02\n","iteration: 35770 loss: 0.0013 lr: 0.02\n","iteration: 35780 loss: 0.0010 lr: 0.02\n","iteration: 35790 loss: 0.0011 lr: 0.02\n","iteration: 35800 loss: 0.0011 lr: 0.02\n","iteration: 35810 loss: 0.0011 lr: 0.02\n","iteration: 35820 loss: 0.0013 lr: 0.02\n","iteration: 35830 loss: 0.0011 lr: 0.02\n","iteration: 35840 loss: 0.0011 lr: 0.02\n","iteration: 35850 loss: 0.0013 lr: 0.02\n","iteration: 35860 loss: 0.0012 lr: 0.02\n","iteration: 35870 loss: 0.0012 lr: 0.02\n","iteration: 35880 loss: 0.0013 lr: 0.02\n","iteration: 35890 loss: 0.0007 lr: 0.02\n","iteration: 35900 loss: 0.0011 lr: 0.02\n","iteration: 35910 loss: 0.0010 lr: 0.02\n","iteration: 35920 loss: 0.0010 lr: 0.02\n","iteration: 35930 loss: 0.0010 lr: 0.02\n","iteration: 35940 loss: 0.0013 lr: 0.02\n","iteration: 35950 loss: 0.0009 lr: 0.02\n","iteration: 35960 loss: 0.0013 lr: 0.02\n","iteration: 35970 loss: 0.0013 lr: 0.02\n","iteration: 35980 loss: 0.0013 lr: 0.02\n","iteration: 35990 loss: 0.0012 lr: 0.02\n","iteration: 36000 loss: 0.0010 lr: 0.02\n","iteration: 36010 loss: 0.0014 lr: 0.02\n","iteration: 36020 loss: 0.0013 lr: 0.02\n","iteration: 36030 loss: 0.0013 lr: 0.02\n","iteration: 36040 loss: 0.0011 lr: 0.02\n","iteration: 36050 loss: 0.0013 lr: 0.02\n","iteration: 36060 loss: 0.0010 lr: 0.02\n","iteration: 36070 loss: 0.0011 lr: 0.02\n","iteration: 36080 loss: 0.0014 lr: 0.02\n","iteration: 36090 loss: 0.0013 lr: 0.02\n","iteration: 36100 loss: 0.0011 lr: 0.02\n","iteration: 36110 loss: 0.0013 lr: 0.02\n","iteration: 36120 loss: 0.0011 lr: 0.02\n","iteration: 36130 loss: 0.0012 lr: 0.02\n","iteration: 36140 loss: 0.0012 lr: 0.02\n","iteration: 36150 loss: 0.0012 lr: 0.02\n","iteration: 36160 loss: 0.0012 lr: 0.02\n","iteration: 36170 loss: 0.0010 lr: 0.02\n","iteration: 36180 loss: 0.0012 lr: 0.02\n","iteration: 36190 loss: 0.0014 lr: 0.02\n","iteration: 36200 loss: 0.0013 lr: 0.02\n","iteration: 36210 loss: 0.0020 lr: 0.02\n","iteration: 36220 loss: 0.0016 lr: 0.02\n","iteration: 36230 loss: 0.0012 lr: 0.02\n","iteration: 36240 loss: 0.0012 lr: 0.02\n","iteration: 36250 loss: 0.0016 lr: 0.02\n","iteration: 36260 loss: 0.0014 lr: 0.02\n","iteration: 36270 loss: 0.0016 lr: 0.02\n","iteration: 36280 loss: 0.0013 lr: 0.02\n","iteration: 36290 loss: 0.0016 lr: 0.02\n","iteration: 36300 loss: 0.0010 lr: 0.02\n","iteration: 36310 loss: 0.0013 lr: 0.02\n","iteration: 36320 loss: 0.0014 lr: 0.02\n","iteration: 36330 loss: 0.0012 lr: 0.02\n","iteration: 36340 loss: 0.0009 lr: 0.02\n","iteration: 36350 loss: 0.0011 lr: 0.02\n","iteration: 36360 loss: 0.0012 lr: 0.02\n","iteration: 36370 loss: 0.0012 lr: 0.02\n","iteration: 36380 loss: 0.0015 lr: 0.02\n","iteration: 36390 loss: 0.0013 lr: 0.02\n","iteration: 36400 loss: 0.0010 lr: 0.02\n","iteration: 36410 loss: 0.0010 lr: 0.02\n","iteration: 36420 loss: 0.0012 lr: 0.02\n","iteration: 36430 loss: 0.0014 lr: 0.02\n","iteration: 36440 loss: 0.0012 lr: 0.02\n","iteration: 36450 loss: 0.0012 lr: 0.02\n","iteration: 36460 loss: 0.0015 lr: 0.02\n","iteration: 36470 loss: 0.0016 lr: 0.02\n","iteration: 36480 loss: 0.0015 lr: 0.02\n","iteration: 36490 loss: 0.0011 lr: 0.02\n","iteration: 36500 loss: 0.0013 lr: 0.02\n","iteration: 36510 loss: 0.0013 lr: 0.02\n","iteration: 36520 loss: 0.0013 lr: 0.02\n","iteration: 36530 loss: 0.0014 lr: 0.02\n","iteration: 36540 loss: 0.0013 lr: 0.02\n","iteration: 36550 loss: 0.0013 lr: 0.02\n","iteration: 36560 loss: 0.0013 lr: 0.02\n","iteration: 36570 loss: 0.0011 lr: 0.02\n","iteration: 36580 loss: 0.0013 lr: 0.02\n","iteration: 36590 loss: 0.0015 lr: 0.02\n","iteration: 36600 loss: 0.0013 lr: 0.02\n","iteration: 36610 loss: 0.0016 lr: 0.02\n","iteration: 36620 loss: 0.0011 lr: 0.02\n","iteration: 36630 loss: 0.0012 lr: 0.02\n","iteration: 36640 loss: 0.0010 lr: 0.02\n","iteration: 36650 loss: 0.0016 lr: 0.02\n","iteration: 36660 loss: 0.0014 lr: 0.02\n","iteration: 36670 loss: 0.0012 lr: 0.02\n","iteration: 36680 loss: 0.0014 lr: 0.02\n","iteration: 36690 loss: 0.0015 lr: 0.02\n","iteration: 36700 loss: 0.0012 lr: 0.02\n","iteration: 36710 loss: 0.0009 lr: 0.02\n","iteration: 36720 loss: 0.0011 lr: 0.02\n","iteration: 36730 loss: 0.0014 lr: 0.02\n","iteration: 36740 loss: 0.0012 lr: 0.02\n","iteration: 36750 loss: 0.0013 lr: 0.02\n","iteration: 36760 loss: 0.0012 lr: 0.02\n","iteration: 36770 loss: 0.0011 lr: 0.02\n","iteration: 36780 loss: 0.0014 lr: 0.02\n","iteration: 36790 loss: 0.0012 lr: 0.02\n","iteration: 36800 loss: 0.0010 lr: 0.02\n","iteration: 36810 loss: 0.0013 lr: 0.02\n","iteration: 36820 loss: 0.0010 lr: 0.02\n","iteration: 36830 loss: 0.0014 lr: 0.02\n","iteration: 36840 loss: 0.0011 lr: 0.02\n","iteration: 36850 loss: 0.0011 lr: 0.02\n","iteration: 36860 loss: 0.0014 lr: 0.02\n","iteration: 36870 loss: 0.0014 lr: 0.02\n","iteration: 36880 loss: 0.0013 lr: 0.02\n","iteration: 36890 loss: 0.0014 lr: 0.02\n","iteration: 36900 loss: 0.0012 lr: 0.02\n","iteration: 36910 loss: 0.0013 lr: 0.02\n","iteration: 36920 loss: 0.0012 lr: 0.02\n","iteration: 36930 loss: 0.0012 lr: 0.02\n","iteration: 36940 loss: 0.0009 lr: 0.02\n","iteration: 36950 loss: 0.0011 lr: 0.02\n","iteration: 36960 loss: 0.0013 lr: 0.02\n","iteration: 36970 loss: 0.0013 lr: 0.02\n","iteration: 36980 loss: 0.0013 lr: 0.02\n","iteration: 36990 loss: 0.0013 lr: 0.02\n","iteration: 37000 loss: 0.0012 lr: 0.02\n","iteration: 37010 loss: 0.0012 lr: 0.02\n","iteration: 37020 loss: 0.0010 lr: 0.02\n","iteration: 37030 loss: 0.0014 lr: 0.02\n","iteration: 37040 loss: 0.0011 lr: 0.02\n","iteration: 37050 loss: 0.0011 lr: 0.02\n","iteration: 37060 loss: 0.0013 lr: 0.02\n","iteration: 37070 loss: 0.0024 lr: 0.02\n","iteration: 37080 loss: 0.0013 lr: 0.02\n","iteration: 37090 loss: 0.0011 lr: 0.02\n","iteration: 37100 loss: 0.0014 lr: 0.02\n","iteration: 37110 loss: 0.0011 lr: 0.02\n","iteration: 37120 loss: 0.0018 lr: 0.02\n","iteration: 37130 loss: 0.0014 lr: 0.02\n","iteration: 37140 loss: 0.0013 lr: 0.02\n","iteration: 37150 loss: 0.0014 lr: 0.02\n","iteration: 37160 loss: 0.0013 lr: 0.02\n","iteration: 37170 loss: 0.0011 lr: 0.02\n","iteration: 37180 loss: 0.0012 lr: 0.02\n","iteration: 37190 loss: 0.0013 lr: 0.02\n","iteration: 37200 loss: 0.0016 lr: 0.02\n","iteration: 37210 loss: 0.0015 lr: 0.02\n","iteration: 37220 loss: 0.0015 lr: 0.02\n","iteration: 37230 loss: 0.0011 lr: 0.02\n","iteration: 37240 loss: 0.0015 lr: 0.02\n","iteration: 37250 loss: 0.0013 lr: 0.02\n","iteration: 37260 loss: 0.0011 lr: 0.02\n","iteration: 37270 loss: 0.0010 lr: 0.02\n","iteration: 37280 loss: 0.0014 lr: 0.02\n","iteration: 37290 loss: 0.0012 lr: 0.02\n","iteration: 37300 loss: 0.0010 lr: 0.02\n","iteration: 37310 loss: 0.0010 lr: 0.02\n","iteration: 37320 loss: 0.0014 lr: 0.02\n","iteration: 37330 loss: 0.0012 lr: 0.02\n","iteration: 37340 loss: 0.0010 lr: 0.02\n","iteration: 37350 loss: 0.0015 lr: 0.02\n","iteration: 37360 loss: 0.0011 lr: 0.02\n","iteration: 37370 loss: 0.0013 lr: 0.02\n","iteration: 37380 loss: 0.0013 lr: 0.02\n","iteration: 37390 loss: 0.0010 lr: 0.02\n","iteration: 37400 loss: 0.0016 lr: 0.02\n","iteration: 37410 loss: 0.0011 lr: 0.02\n","iteration: 37420 loss: 0.0011 lr: 0.02\n","iteration: 37430 loss: 0.0012 lr: 0.02\n","iteration: 37440 loss: 0.0012 lr: 0.02\n","iteration: 37450 loss: 0.0012 lr: 0.02\n","iteration: 37460 loss: 0.0012 lr: 0.02\n","iteration: 37470 loss: 0.0014 lr: 0.02\n","iteration: 37480 loss: 0.0010 lr: 0.02\n","iteration: 37490 loss: 0.0014 lr: 0.02\n","iteration: 37500 loss: 0.0008 lr: 0.02\n","iteration: 37510 loss: 0.0013 lr: 0.02\n","iteration: 37520 loss: 0.0014 lr: 0.02\n","iteration: 37530 loss: 0.0010 lr: 0.02\n","iteration: 37540 loss: 0.0014 lr: 0.02\n","iteration: 37550 loss: 0.0010 lr: 0.02\n","iteration: 37560 loss: 0.0012 lr: 0.02\n","iteration: 37570 loss: 0.0014 lr: 0.02\n","iteration: 37580 loss: 0.0011 lr: 0.02\n","iteration: 37590 loss: 0.0010 lr: 0.02\n","iteration: 37600 loss: 0.0013 lr: 0.02\n","iteration: 37610 loss: 0.0012 lr: 0.02\n","iteration: 37620 loss: 0.0014 lr: 0.02\n","iteration: 37630 loss: 0.0014 lr: 0.02\n","iteration: 37640 loss: 0.0010 lr: 0.02\n","iteration: 37650 loss: 0.0012 lr: 0.02\n","iteration: 37660 loss: 0.0014 lr: 0.02\n","iteration: 37670 loss: 0.0010 lr: 0.02\n","iteration: 37680 loss: 0.0012 lr: 0.02\n","iteration: 37690 loss: 0.0012 lr: 0.02\n","iteration: 37700 loss: 0.0015 lr: 0.02\n","iteration: 37710 loss: 0.0014 lr: 0.02\n","iteration: 37720 loss: 0.0016 lr: 0.02\n","iteration: 37730 loss: 0.0010 lr: 0.02\n","iteration: 37740 loss: 0.0013 lr: 0.02\n","iteration: 37750 loss: 0.0012 lr: 0.02\n","iteration: 37760 loss: 0.0012 lr: 0.02\n","iteration: 37770 loss: 0.0012 lr: 0.02\n","iteration: 37780 loss: 0.0012 lr: 0.02\n","iteration: 37790 loss: 0.0011 lr: 0.02\n","iteration: 37800 loss: 0.0012 lr: 0.02\n","iteration: 37810 loss: 0.0012 lr: 0.02\n","iteration: 37820 loss: 0.0014 lr: 0.02\n","iteration: 37830 loss: 0.0011 lr: 0.02\n","iteration: 37840 loss: 0.0011 lr: 0.02\n","iteration: 37850 loss: 0.0015 lr: 0.02\n","iteration: 37860 loss: 0.0013 lr: 0.02\n","iteration: 37870 loss: 0.0010 lr: 0.02\n","iteration: 37880 loss: 0.0014 lr: 0.02\n","iteration: 37890 loss: 0.0010 lr: 0.02\n","iteration: 37900 loss: 0.0013 lr: 0.02\n","iteration: 37910 loss: 0.0015 lr: 0.02\n","iteration: 37920 loss: 0.0011 lr: 0.02\n","iteration: 37930 loss: 0.0011 lr: 0.02\n","iteration: 37940 loss: 0.0009 lr: 0.02\n","iteration: 37950 loss: 0.0015 lr: 0.02\n","iteration: 37960 loss: 0.0012 lr: 0.02\n","iteration: 37970 loss: 0.0011 lr: 0.02\n","iteration: 37980 loss: 0.0012 lr: 0.02\n","iteration: 37990 loss: 0.0014 lr: 0.02\n","iteration: 38000 loss: 0.0013 lr: 0.02\n","iteration: 38010 loss: 0.0016 lr: 0.02\n","iteration: 38020 loss: 0.0012 lr: 0.02\n","iteration: 38030 loss: 0.0012 lr: 0.02\n","iteration: 38040 loss: 0.0011 lr: 0.02\n","iteration: 38050 loss: 0.0011 lr: 0.02\n","iteration: 38060 loss: 0.0014 lr: 0.02\n","iteration: 38070 loss: 0.0010 lr: 0.02\n","iteration: 38080 loss: 0.0013 lr: 0.02\n","iteration: 38090 loss: 0.0013 lr: 0.02\n","iteration: 38100 loss: 0.0012 lr: 0.02\n","iteration: 38110 loss: 0.0012 lr: 0.02\n","iteration: 38120 loss: 0.0014 lr: 0.02\n","iteration: 38130 loss: 0.0011 lr: 0.02\n","iteration: 38140 loss: 0.0009 lr: 0.02\n","iteration: 38150 loss: 0.0012 lr: 0.02\n","iteration: 38160 loss: 0.0012 lr: 0.02\n","iteration: 38170 loss: 0.0014 lr: 0.02\n","iteration: 38180 loss: 0.0011 lr: 0.02\n","iteration: 38190 loss: 0.0014 lr: 0.02\n","iteration: 38200 loss: 0.0010 lr: 0.02\n","iteration: 38210 loss: 0.0012 lr: 0.02\n","iteration: 38220 loss: 0.0012 lr: 0.02\n","iteration: 38230 loss: 0.0012 lr: 0.02\n","iteration: 38240 loss: 0.0011 lr: 0.02\n","iteration: 38250 loss: 0.0012 lr: 0.02\n","iteration: 38260 loss: 0.0012 lr: 0.02\n","iteration: 38270 loss: 0.0013 lr: 0.02\n","iteration: 38280 loss: 0.0012 lr: 0.02\n","iteration: 38290 loss: 0.0012 lr: 0.02\n","iteration: 38300 loss: 0.0016 lr: 0.02\n","iteration: 38310 loss: 0.0021 lr: 0.02\n","iteration: 38320 loss: 0.0012 lr: 0.02\n","iteration: 38330 loss: 0.0010 lr: 0.02\n","iteration: 38340 loss: 0.0011 lr: 0.02\n","iteration: 38350 loss: 0.0011 lr: 0.02\n","iteration: 38360 loss: 0.0013 lr: 0.02\n","iteration: 38370 loss: 0.0011 lr: 0.02\n","iteration: 38380 loss: 0.0012 lr: 0.02\n","iteration: 38390 loss: 0.0011 lr: 0.02\n","iteration: 38400 loss: 0.0014 lr: 0.02\n","iteration: 38410 loss: 0.0010 lr: 0.02\n","iteration: 38420 loss: 0.0010 lr: 0.02\n","iteration: 38430 loss: 0.0012 lr: 0.02\n","iteration: 38440 loss: 0.0011 lr: 0.02\n","iteration: 38450 loss: 0.0014 lr: 0.02\n","iteration: 38460 loss: 0.0011 lr: 0.02\n","iteration: 38470 loss: 0.0012 lr: 0.02\n","iteration: 38480 loss: 0.0013 lr: 0.02\n","iteration: 38490 loss: 0.0013 lr: 0.02\n","iteration: 38500 loss: 0.0013 lr: 0.02\n","iteration: 38510 loss: 0.0014 lr: 0.02\n","iteration: 38520 loss: 0.0012 lr: 0.02\n","iteration: 38530 loss: 0.0013 lr: 0.02\n","iteration: 38540 loss: 0.0017 lr: 0.02\n","iteration: 38550 loss: 0.0012 lr: 0.02\n","iteration: 38560 loss: 0.0013 lr: 0.02\n","iteration: 38570 loss: 0.0014 lr: 0.02\n","iteration: 38580 loss: 0.0014 lr: 0.02\n","iteration: 38590 loss: 0.0013 lr: 0.02\n","iteration: 38600 loss: 0.0014 lr: 0.02\n","iteration: 38610 loss: 0.0011 lr: 0.02\n","iteration: 38620 loss: 0.0013 lr: 0.02\n","iteration: 38630 loss: 0.0011 lr: 0.02\n","iteration: 38640 loss: 0.0011 lr: 0.02\n","iteration: 38650 loss: 0.0012 lr: 0.02\n","iteration: 38660 loss: 0.0012 lr: 0.02\n","iteration: 38670 loss: 0.0012 lr: 0.02\n","iteration: 38680 loss: 0.0011 lr: 0.02\n","iteration: 38690 loss: 0.0012 lr: 0.02\n","iteration: 38700 loss: 0.0012 lr: 0.02\n","iteration: 38710 loss: 0.0012 lr: 0.02\n","iteration: 38720 loss: 0.0010 lr: 0.02\n","iteration: 38730 loss: 0.0015 lr: 0.02\n","iteration: 38740 loss: 0.0012 lr: 0.02\n","iteration: 38750 loss: 0.0011 lr: 0.02\n","iteration: 38760 loss: 0.0010 lr: 0.02\n","iteration: 38770 loss: 0.0011 lr: 0.02\n","iteration: 38780 loss: 0.0015 lr: 0.02\n","iteration: 38790 loss: 0.0014 lr: 0.02\n","iteration: 38800 loss: 0.0013 lr: 0.02\n","iteration: 38810 loss: 0.0011 lr: 0.02\n","iteration: 38820 loss: 0.0014 lr: 0.02\n","iteration: 38830 loss: 0.0012 lr: 0.02\n","iteration: 38840 loss: 0.0013 lr: 0.02\n","iteration: 38850 loss: 0.0013 lr: 0.02\n","iteration: 38860 loss: 0.0011 lr: 0.02\n","iteration: 38870 loss: 0.0010 lr: 0.02\n","iteration: 38880 loss: 0.0015 lr: 0.02\n","iteration: 38890 loss: 0.0011 lr: 0.02\n","iteration: 38900 loss: 0.0012 lr: 0.02\n","iteration: 38910 loss: 0.0009 lr: 0.02\n","iteration: 38920 loss: 0.0012 lr: 0.02\n","iteration: 38930 loss: 0.0009 lr: 0.02\n","iteration: 38940 loss: 0.0010 lr: 0.02\n","iteration: 38950 loss: 0.0015 lr: 0.02\n","iteration: 38960 loss: 0.0013 lr: 0.02\n","iteration: 38970 loss: 0.0012 lr: 0.02\n","iteration: 38980 loss: 0.0011 lr: 0.02\n","iteration: 38990 loss: 0.0011 lr: 0.02\n","iteration: 39000 loss: 0.0014 lr: 0.02\n","iteration: 39010 loss: 0.0015 lr: 0.02\n","iteration: 39020 loss: 0.0010 lr: 0.02\n","iteration: 39030 loss: 0.0011 lr: 0.02\n","iteration: 39040 loss: 0.0014 lr: 0.02\n","iteration: 39050 loss: 0.0011 lr: 0.02\n","iteration: 39060 loss: 0.0010 lr: 0.02\n","iteration: 39070 loss: 0.0012 lr: 0.02\n","iteration: 39080 loss: 0.0015 lr: 0.02\n","iteration: 39090 loss: 0.0011 lr: 0.02\n","iteration: 39100 loss: 0.0009 lr: 0.02\n","iteration: 39110 loss: 0.0013 lr: 0.02\n","iteration: 39120 loss: 0.0017 lr: 0.02\n","iteration: 39130 loss: 0.0009 lr: 0.02\n","iteration: 39140 loss: 0.0011 lr: 0.02\n","iteration: 39150 loss: 0.0010 lr: 0.02\n","iteration: 39160 loss: 0.0013 lr: 0.02\n","iteration: 39170 loss: 0.0011 lr: 0.02\n","iteration: 39180 loss: 0.0014 lr: 0.02\n","iteration: 39190 loss: 0.0013 lr: 0.02\n","iteration: 39200 loss: 0.0010 lr: 0.02\n","iteration: 39210 loss: 0.0010 lr: 0.02\n","iteration: 39220 loss: 0.0013 lr: 0.02\n","iteration: 39230 loss: 0.0013 lr: 0.02\n","iteration: 39240 loss: 0.0013 lr: 0.02\n","iteration: 39250 loss: 0.0011 lr: 0.02\n","iteration: 39260 loss: 0.0010 lr: 0.02\n","iteration: 39270 loss: 0.0015 lr: 0.02\n","iteration: 39280 loss: 0.0013 lr: 0.02\n","iteration: 39290 loss: 0.0011 lr: 0.02\n","iteration: 39300 loss: 0.0012 lr: 0.02\n","iteration: 39310 loss: 0.0012 lr: 0.02\n","iteration: 39320 loss: 0.0013 lr: 0.02\n","iteration: 39330 loss: 0.0013 lr: 0.02\n","iteration: 39340 loss: 0.0011 lr: 0.02\n","iteration: 39350 loss: 0.0011 lr: 0.02\n","iteration: 39360 loss: 0.0012 lr: 0.02\n","iteration: 39370 loss: 0.0010 lr: 0.02\n","iteration: 39380 loss: 0.0011 lr: 0.02\n","iteration: 39390 loss: 0.0011 lr: 0.02\n","iteration: 39400 loss: 0.0014 lr: 0.02\n","iteration: 39410 loss: 0.0011 lr: 0.02\n","iteration: 39420 loss: 0.0012 lr: 0.02\n","iteration: 39430 loss: 0.0010 lr: 0.02\n","iteration: 39440 loss: 0.0013 lr: 0.02\n","iteration: 39450 loss: 0.0010 lr: 0.02\n","iteration: 39460 loss: 0.0012 lr: 0.02\n","iteration: 39470 loss: 0.0011 lr: 0.02\n","iteration: 39480 loss: 0.0013 lr: 0.02\n","iteration: 39490 loss: 0.0013 lr: 0.02\n","iteration: 39500 loss: 0.0014 lr: 0.02\n","iteration: 39510 loss: 0.0012 lr: 0.02\n","iteration: 39520 loss: 0.0015 lr: 0.02\n","iteration: 39530 loss: 0.0016 lr: 0.02\n","iteration: 39540 loss: 0.0011 lr: 0.02\n","iteration: 39550 loss: 0.0015 lr: 0.02\n","iteration: 39560 loss: 0.0011 lr: 0.02\n","iteration: 39570 loss: 0.0015 lr: 0.02\n","iteration: 39580 loss: 0.0014 lr: 0.02\n","iteration: 39590 loss: 0.0013 lr: 0.02\n","iteration: 39600 loss: 0.0010 lr: 0.02\n","iteration: 39610 loss: 0.0010 lr: 0.02\n","iteration: 39620 loss: 0.0011 lr: 0.02\n","iteration: 39630 loss: 0.0012 lr: 0.02\n","iteration: 39640 loss: 0.0013 lr: 0.02\n","iteration: 39650 loss: 0.0013 lr: 0.02\n","iteration: 39660 loss: 0.0014 lr: 0.02\n","iteration: 39670 loss: 0.0014 lr: 0.02\n","iteration: 39680 loss: 0.0011 lr: 0.02\n","iteration: 39690 loss: 0.0011 lr: 0.02\n","iteration: 39700 loss: 0.0015 lr: 0.02\n","iteration: 39710 loss: 0.0012 lr: 0.02\n","iteration: 39720 loss: 0.0015 lr: 0.02\n","iteration: 39730 loss: 0.0012 lr: 0.02\n","iteration: 39740 loss: 0.0013 lr: 0.02\n","iteration: 39750 loss: 0.0010 lr: 0.02\n","iteration: 39760 loss: 0.0011 lr: 0.02\n","iteration: 39770 loss: 0.0010 lr: 0.02\n","iteration: 39780 loss: 0.0009 lr: 0.02\n","iteration: 39790 loss: 0.0011 lr: 0.02\n","iteration: 39800 loss: 0.0019 lr: 0.02\n","iteration: 39810 loss: 0.0013 lr: 0.02\n","iteration: 39820 loss: 0.0009 lr: 0.02\n","iteration: 39830 loss: 0.0012 lr: 0.02\n","iteration: 39840 loss: 0.0012 lr: 0.02\n","iteration: 39850 loss: 0.0011 lr: 0.02\n","iteration: 39860 loss: 0.0012 lr: 0.02\n","iteration: 39870 loss: 0.0013 lr: 0.02\n","iteration: 39880 loss: 0.0012 lr: 0.02\n","iteration: 39890 loss: 0.0010 lr: 0.02\n","iteration: 39900 loss: 0.0011 lr: 0.02\n","iteration: 39910 loss: 0.0012 lr: 0.02\n","iteration: 39920 loss: 0.0011 lr: 0.02\n","iteration: 39930 loss: 0.0010 lr: 0.02\n","iteration: 39940 loss: 0.0012 lr: 0.02\n","iteration: 39950 loss: 0.0012 lr: 0.02\n","iteration: 39960 loss: 0.0013 lr: 0.02\n","iteration: 39970 loss: 0.0014 lr: 0.02\n","iteration: 39980 loss: 0.0011 lr: 0.02\n","iteration: 39990 loss: 0.0009 lr: 0.02\n","iteration: 40000 loss: 0.0009 lr: 0.02\n","iteration: 40010 loss: 0.0012 lr: 0.02\n","iteration: 40020 loss: 0.0013 lr: 0.02\n","iteration: 40030 loss: 0.0011 lr: 0.02\n","iteration: 40040 loss: 0.0013 lr: 0.02\n","iteration: 40050 loss: 0.0013 lr: 0.02\n","iteration: 40060 loss: 0.0010 lr: 0.02\n","iteration: 40070 loss: 0.0020 lr: 0.02\n","iteration: 40080 loss: 0.0013 lr: 0.02\n","iteration: 40090 loss: 0.0011 lr: 0.02\n","iteration: 40100 loss: 0.0010 lr: 0.02\n","iteration: 40110 loss: 0.0011 lr: 0.02\n","iteration: 40120 loss: 0.0014 lr: 0.02\n","iteration: 40130 loss: 0.0011 lr: 0.02\n","iteration: 40140 loss: 0.0012 lr: 0.02\n","iteration: 40150 loss: 0.0012 lr: 0.02\n","iteration: 40160 loss: 0.0009 lr: 0.02\n","iteration: 40170 loss: 0.0012 lr: 0.02\n","iteration: 40180 loss: 0.0011 lr: 0.02\n","iteration: 40190 loss: 0.0010 lr: 0.02\n","iteration: 40200 loss: 0.0010 lr: 0.02\n","iteration: 40210 loss: 0.0013 lr: 0.02\n","iteration: 40220 loss: 0.0011 lr: 0.02\n","iteration: 40230 loss: 0.0011 lr: 0.02\n","iteration: 40240 loss: 0.0011 lr: 0.02\n","iteration: 40250 loss: 0.0010 lr: 0.02\n","iteration: 40260 loss: 0.0013 lr: 0.02\n","iteration: 40270 loss: 0.0010 lr: 0.02\n","iteration: 40280 loss: 0.0011 lr: 0.02\n","iteration: 40290 loss: 0.0012 lr: 0.02\n","iteration: 40300 loss: 0.0015 lr: 0.02\n","iteration: 40310 loss: 0.0013 lr: 0.02\n","iteration: 40320 loss: 0.0018 lr: 0.02\n","iteration: 40330 loss: 0.0014 lr: 0.02\n","iteration: 40340 loss: 0.0011 lr: 0.02\n","iteration: 40350 loss: 0.0008 lr: 0.02\n","iteration: 40360 loss: 0.0010 lr: 0.02\n","iteration: 40370 loss: 0.0011 lr: 0.02\n","iteration: 40380 loss: 0.0013 lr: 0.02\n","iteration: 40390 loss: 0.0011 lr: 0.02\n","iteration: 40400 loss: 0.0013 lr: 0.02\n","iteration: 40410 loss: 0.0013 lr: 0.02\n","iteration: 40420 loss: 0.0013 lr: 0.02\n","iteration: 40430 loss: 0.0011 lr: 0.02\n","iteration: 40440 loss: 0.0010 lr: 0.02\n","iteration: 40450 loss: 0.0012 lr: 0.02\n","iteration: 40460 loss: 0.0013 lr: 0.02\n","iteration: 40470 loss: 0.0010 lr: 0.02\n","iteration: 40480 loss: 0.0011 lr: 0.02\n","iteration: 40490 loss: 0.0011 lr: 0.02\n","iteration: 40500 loss: 0.0012 lr: 0.02\n","iteration: 40510 loss: 0.0014 lr: 0.02\n","iteration: 40520 loss: 0.0011 lr: 0.02\n","iteration: 40530 loss: 0.0013 lr: 0.02\n","iteration: 40540 loss: 0.0012 lr: 0.02\n","iteration: 40550 loss: 0.0011 lr: 0.02\n","iteration: 40560 loss: 0.0011 lr: 0.02\n","iteration: 40570 loss: 0.0012 lr: 0.02\n","iteration: 40580 loss: 0.0014 lr: 0.02\n","iteration: 40590 loss: 0.0010 lr: 0.02\n","iteration: 40600 loss: 0.0011 lr: 0.02\n","iteration: 40610 loss: 0.0009 lr: 0.02\n","iteration: 40620 loss: 0.0010 lr: 0.02\n","iteration: 40630 loss: 0.0013 lr: 0.02\n","iteration: 40640 loss: 0.0014 lr: 0.02\n","iteration: 40650 loss: 0.0012 lr: 0.02\n","iteration: 40660 loss: 0.0013 lr: 0.02\n","iteration: 40670 loss: 0.0010 lr: 0.02\n","iteration: 40680 loss: 0.0010 lr: 0.02\n","iteration: 40690 loss: 0.0011 lr: 0.02\n","iteration: 40700 loss: 0.0013 lr: 0.02\n","iteration: 40710 loss: 0.0011 lr: 0.02\n","iteration: 40720 loss: 0.0013 lr: 0.02\n","iteration: 40730 loss: 0.0015 lr: 0.02\n","iteration: 40740 loss: 0.0015 lr: 0.02\n","iteration: 40750 loss: 0.0011 lr: 0.02\n","iteration: 40760 loss: 0.0012 lr: 0.02\n","iteration: 40770 loss: 0.0012 lr: 0.02\n","iteration: 40780 loss: 0.0014 lr: 0.02\n","iteration: 40790 loss: 0.0013 lr: 0.02\n","iteration: 40800 loss: 0.0012 lr: 0.02\n","iteration: 40810 loss: 0.0011 lr: 0.02\n","iteration: 40820 loss: 0.0012 lr: 0.02\n","iteration: 40830 loss: 0.0011 lr: 0.02\n","iteration: 40840 loss: 0.0010 lr: 0.02\n","iteration: 40850 loss: 0.0013 lr: 0.02\n","iteration: 40860 loss: 0.0011 lr: 0.02\n","iteration: 40870 loss: 0.0012 lr: 0.02\n","iteration: 40880 loss: 0.0013 lr: 0.02\n","iteration: 40890 loss: 0.0013 lr: 0.02\n","iteration: 40900 loss: 0.0011 lr: 0.02\n","iteration: 40910 loss: 0.0011 lr: 0.02\n","iteration: 40920 loss: 0.0010 lr: 0.02\n","iteration: 40930 loss: 0.0010 lr: 0.02\n","iteration: 40940 loss: 0.0012 lr: 0.02\n","iteration: 40950 loss: 0.0009 lr: 0.02\n","iteration: 40960 loss: 0.0012 lr: 0.02\n","iteration: 40970 loss: 0.0011 lr: 0.02\n","iteration: 40980 loss: 0.0011 lr: 0.02\n","iteration: 40990 loss: 0.0012 lr: 0.02\n","iteration: 41000 loss: 0.0012 lr: 0.02\n","iteration: 41010 loss: 0.0012 lr: 0.02\n","iteration: 41020 loss: 0.0012 lr: 0.02\n","iteration: 41030 loss: 0.0010 lr: 0.02\n","iteration: 41040 loss: 0.0012 lr: 0.02\n","iteration: 41050 loss: 0.0017 lr: 0.02\n","iteration: 41060 loss: 0.0013 lr: 0.02\n","iteration: 41070 loss: 0.0015 lr: 0.02\n","iteration: 41080 loss: 0.0012 lr: 0.02\n","iteration: 41090 loss: 0.0010 lr: 0.02\n","iteration: 41100 loss: 0.0014 lr: 0.02\n","iteration: 41110 loss: 0.0012 lr: 0.02\n","iteration: 41120 loss: 0.0013 lr: 0.02\n","iteration: 41130 loss: 0.0010 lr: 0.02\n","iteration: 41140 loss: 0.0011 lr: 0.02\n","iteration: 41150 loss: 0.0011 lr: 0.02\n","iteration: 41160 loss: 0.0014 lr: 0.02\n","iteration: 41170 loss: 0.0012 lr: 0.02\n","iteration: 41180 loss: 0.0012 lr: 0.02\n","iteration: 41190 loss: 0.0013 lr: 0.02\n","iteration: 41200 loss: 0.0012 lr: 0.02\n","iteration: 41210 loss: 0.0012 lr: 0.02\n","iteration: 41220 loss: 0.0013 lr: 0.02\n","iteration: 41230 loss: 0.0015 lr: 0.02\n","iteration: 41240 loss: 0.0008 lr: 0.02\n","iteration: 41250 loss: 0.0011 lr: 0.02\n","iteration: 41260 loss: 0.0013 lr: 0.02\n","iteration: 41270 loss: 0.0011 lr: 0.02\n","iteration: 41280 loss: 0.0009 lr: 0.02\n","iteration: 41290 loss: 0.0013 lr: 0.02\n","iteration: 41300 loss: 0.0010 lr: 0.02\n","iteration: 41310 loss: 0.0011 lr: 0.02\n","iteration: 41320 loss: 0.0013 lr: 0.02\n","iteration: 41330 loss: 0.0012 lr: 0.02\n","iteration: 41340 loss: 0.0011 lr: 0.02\n","iteration: 41350 loss: 0.0014 lr: 0.02\n","iteration: 41360 loss: 0.0011 lr: 0.02\n","iteration: 41370 loss: 0.0011 lr: 0.02\n","iteration: 41380 loss: 0.0013 lr: 0.02\n","iteration: 41390 loss: 0.0012 lr: 0.02\n","iteration: 41400 loss: 0.0011 lr: 0.02\n","iteration: 41410 loss: 0.0013 lr: 0.02\n","iteration: 41420 loss: 0.0011 lr: 0.02\n","iteration: 41430 loss: 0.0011 lr: 0.02\n","iteration: 41440 loss: 0.0012 lr: 0.02\n","iteration: 41450 loss: 0.0014 lr: 0.02\n","iteration: 41460 loss: 0.0010 lr: 0.02\n","iteration: 41470 loss: 0.0013 lr: 0.02\n","iteration: 41480 loss: 0.0010 lr: 0.02\n","iteration: 41490 loss: 0.0011 lr: 0.02\n","iteration: 41500 loss: 0.0010 lr: 0.02\n","iteration: 41510 loss: 0.0013 lr: 0.02\n","iteration: 41520 loss: 0.0009 lr: 0.02\n","iteration: 41530 loss: 0.0016 lr: 0.02\n","iteration: 41540 loss: 0.0013 lr: 0.02\n","iteration: 41550 loss: 0.0011 lr: 0.02\n","iteration: 41560 loss: 0.0013 lr: 0.02\n","iteration: 41570 loss: 0.0010 lr: 0.02\n","iteration: 41580 loss: 0.0015 lr: 0.02\n","iteration: 41590 loss: 0.0013 lr: 0.02\n","iteration: 41600 loss: 0.0010 lr: 0.02\n","iteration: 41610 loss: 0.0012 lr: 0.02\n","iteration: 41620 loss: 0.0011 lr: 0.02\n","iteration: 41630 loss: 0.0012 lr: 0.02\n","iteration: 41640 loss: 0.0012 lr: 0.02\n","iteration: 41650 loss: 0.0011 lr: 0.02\n","iteration: 41660 loss: 0.0010 lr: 0.02\n","iteration: 41670 loss: 0.0013 lr: 0.02\n","iteration: 41680 loss: 0.0012 lr: 0.02\n","iteration: 41690 loss: 0.0011 lr: 0.02\n","iteration: 41700 loss: 0.0011 lr: 0.02\n","iteration: 41710 loss: 0.0011 lr: 0.02\n","iteration: 41720 loss: 0.0013 lr: 0.02\n","iteration: 41730 loss: 0.0015 lr: 0.02\n","iteration: 41740 loss: 0.0013 lr: 0.02\n","iteration: 41750 loss: 0.0014 lr: 0.02\n","iteration: 41760 loss: 0.0011 lr: 0.02\n","iteration: 41770 loss: 0.0015 lr: 0.02\n","iteration: 41780 loss: 0.0010 lr: 0.02\n","iteration: 41790 loss: 0.0015 lr: 0.02\n","iteration: 41800 loss: 0.0016 lr: 0.02\n","iteration: 41810 loss: 0.0012 lr: 0.02\n","iteration: 41820 loss: 0.0016 lr: 0.02\n","iteration: 41830 loss: 0.0009 lr: 0.02\n","iteration: 41840 loss: 0.0013 lr: 0.02\n","iteration: 41850 loss: 0.0014 lr: 0.02\n","iteration: 41860 loss: 0.0008 lr: 0.02\n","iteration: 41870 loss: 0.0009 lr: 0.02\n","iteration: 41880 loss: 0.0013 lr: 0.02\n","iteration: 41890 loss: 0.0011 lr: 0.02\n","iteration: 41900 loss: 0.0011 lr: 0.02\n","iteration: 41910 loss: 0.0013 lr: 0.02\n","iteration: 41920 loss: 0.0011 lr: 0.02\n","iteration: 41930 loss: 0.0015 lr: 0.02\n","iteration: 41940 loss: 0.0014 lr: 0.02\n","iteration: 41950 loss: 0.0015 lr: 0.02\n","iteration: 41960 loss: 0.0009 lr: 0.02\n","iteration: 41970 loss: 0.0015 lr: 0.02\n","iteration: 41980 loss: 0.0012 lr: 0.02\n","iteration: 41990 loss: 0.0011 lr: 0.02\n","iteration: 42000 loss: 0.0012 lr: 0.02\n","iteration: 42010 loss: 0.0014 lr: 0.02\n","iteration: 42020 loss: 0.0016 lr: 0.02\n","iteration: 42030 loss: 0.0013 lr: 0.02\n","iteration: 42040 loss: 0.0014 lr: 0.02\n","iteration: 42050 loss: 0.0012 lr: 0.02\n","iteration: 42060 loss: 0.0012 lr: 0.02\n","iteration: 42070 loss: 0.0014 lr: 0.02\n","iteration: 42080 loss: 0.0014 lr: 0.02\n","iteration: 42090 loss: 0.0013 lr: 0.02\n","iteration: 42100 loss: 0.0014 lr: 0.02\n","iteration: 42110 loss: 0.0011 lr: 0.02\n","iteration: 42120 loss: 0.0013 lr: 0.02\n","iteration: 42130 loss: 0.0012 lr: 0.02\n","iteration: 42140 loss: 0.0010 lr: 0.02\n","iteration: 42150 loss: 0.0012 lr: 0.02\n","iteration: 42160 loss: 0.0013 lr: 0.02\n","iteration: 42170 loss: 0.0010 lr: 0.02\n","iteration: 42180 loss: 0.0012 lr: 0.02\n","iteration: 42190 loss: 0.0009 lr: 0.02\n","iteration: 42200 loss: 0.0012 lr: 0.02\n","iteration: 42210 loss: 0.0015 lr: 0.02\n","iteration: 42220 loss: 0.0012 lr: 0.02\n","iteration: 42230 loss: 0.0011 lr: 0.02\n","iteration: 42240 loss: 0.0011 lr: 0.02\n","iteration: 42250 loss: 0.0015 lr: 0.02\n","iteration: 42260 loss: 0.0014 lr: 0.02\n","iteration: 42270 loss: 0.0010 lr: 0.02\n","iteration: 42280 loss: 0.0014 lr: 0.02\n","iteration: 42290 loss: 0.0014 lr: 0.02\n","iteration: 42300 loss: 0.0011 lr: 0.02\n","iteration: 42310 loss: 0.0014 lr: 0.02\n","iteration: 42320 loss: 0.0012 lr: 0.02\n","iteration: 42330 loss: 0.0014 lr: 0.02\n","iteration: 42340 loss: 0.0010 lr: 0.02\n","iteration: 42350 loss: 0.0014 lr: 0.02\n","iteration: 42360 loss: 0.0011 lr: 0.02\n","iteration: 42370 loss: 0.0011 lr: 0.02\n","iteration: 42380 loss: 0.0009 lr: 0.02\n","iteration: 42390 loss: 0.0010 lr: 0.02\n","iteration: 42400 loss: 0.0013 lr: 0.02\n","iteration: 42410 loss: 0.0015 lr: 0.02\n","iteration: 42420 loss: 0.0011 lr: 0.02\n","iteration: 42430 loss: 0.0011 lr: 0.02\n","iteration: 42440 loss: 0.0013 lr: 0.02\n","iteration: 42450 loss: 0.0011 lr: 0.02\n","iteration: 42460 loss: 0.0009 lr: 0.02\n","iteration: 42470 loss: 0.0012 lr: 0.02\n","iteration: 42480 loss: 0.0013 lr: 0.02\n","iteration: 42490 loss: 0.0014 lr: 0.02\n","iteration: 42500 loss: 0.0010 lr: 0.02\n","iteration: 42510 loss: 0.0011 lr: 0.02\n","iteration: 42520 loss: 0.0010 lr: 0.02\n","iteration: 42530 loss: 0.0013 lr: 0.02\n","iteration: 42540 loss: 0.0012 lr: 0.02\n","iteration: 42550 loss: 0.0014 lr: 0.02\n","iteration: 42560 loss: 0.0011 lr: 0.02\n","iteration: 42570 loss: 0.0011 lr: 0.02\n","iteration: 42580 loss: 0.0010 lr: 0.02\n","iteration: 42590 loss: 0.0010 lr: 0.02\n","iteration: 42600 loss: 0.0015 lr: 0.02\n","iteration: 42610 loss: 0.0011 lr: 0.02\n","iteration: 42620 loss: 0.0013 lr: 0.02\n","iteration: 42630 loss: 0.0012 lr: 0.02\n","iteration: 42640 loss: 0.0014 lr: 0.02\n","iteration: 42650 loss: 0.0010 lr: 0.02\n","iteration: 42660 loss: 0.0012 lr: 0.02\n","iteration: 42670 loss: 0.0013 lr: 0.02\n","iteration: 42680 loss: 0.0011 lr: 0.02\n","iteration: 42690 loss: 0.0011 lr: 0.02\n","iteration: 42700 loss: 0.0015 lr: 0.02\n","iteration: 42710 loss: 0.0011 lr: 0.02\n","iteration: 42720 loss: 0.0014 lr: 0.02\n","iteration: 42730 loss: 0.0012 lr: 0.02\n","iteration: 42740 loss: 0.0015 lr: 0.02\n","iteration: 42750 loss: 0.0012 lr: 0.02\n","iteration: 42760 loss: 0.0014 lr: 0.02\n","iteration: 42770 loss: 0.0013 lr: 0.02\n","iteration: 42780 loss: 0.0011 lr: 0.02\n","iteration: 42790 loss: 0.0011 lr: 0.02\n","iteration: 42800 loss: 0.0014 lr: 0.02\n","iteration: 42810 loss: 0.0010 lr: 0.02\n","iteration: 42820 loss: 0.0014 lr: 0.02\n","iteration: 42830 loss: 0.0010 lr: 0.02\n","iteration: 42840 loss: 0.0010 lr: 0.02\n","iteration: 42850 loss: 0.0018 lr: 0.02\n","iteration: 42860 loss: 0.0010 lr: 0.02\n","iteration: 42870 loss: 0.0016 lr: 0.02\n","iteration: 42880 loss: 0.0011 lr: 0.02\n","iteration: 42890 loss: 0.0011 lr: 0.02\n","iteration: 42900 loss: 0.0012 lr: 0.02\n","iteration: 42910 loss: 0.0013 lr: 0.02\n","iteration: 42920 loss: 0.0009 lr: 0.02\n","iteration: 42930 loss: 0.0010 lr: 0.02\n","iteration: 42940 loss: 0.0011 lr: 0.02\n","iteration: 42950 loss: 0.0013 lr: 0.02\n","iteration: 42960 loss: 0.0013 lr: 0.02\n","iteration: 42970 loss: 0.0010 lr: 0.02\n","iteration: 42980 loss: 0.0012 lr: 0.02\n","iteration: 42990 loss: 0.0012 lr: 0.02\n","iteration: 43000 loss: 0.0013 lr: 0.02\n","iteration: 43010 loss: 0.0011 lr: 0.02\n","iteration: 43020 loss: 0.0010 lr: 0.02\n","iteration: 43030 loss: 0.0013 lr: 0.02\n","iteration: 43040 loss: 0.0010 lr: 0.02\n","iteration: 43050 loss: 0.0011 lr: 0.02\n","iteration: 43060 loss: 0.0011 lr: 0.02\n","iteration: 43070 loss: 0.0009 lr: 0.02\n","iteration: 43080 loss: 0.0012 lr: 0.02\n","iteration: 43090 loss: 0.0012 lr: 0.02\n","iteration: 43100 loss: 0.0011 lr: 0.02\n","iteration: 43110 loss: 0.0012 lr: 0.02\n","iteration: 43120 loss: 0.0010 lr: 0.02\n","iteration: 43130 loss: 0.0011 lr: 0.02\n","iteration: 43140 loss: 0.0014 lr: 0.02\n","iteration: 43150 loss: 0.0012 lr: 0.02\n","iteration: 43160 loss: 0.0010 lr: 0.02\n","iteration: 43170 loss: 0.0016 lr: 0.02\n","iteration: 43180 loss: 0.0010 lr: 0.02\n","iteration: 43190 loss: 0.0009 lr: 0.02\n","iteration: 43200 loss: 0.0010 lr: 0.02\n","iteration: 43210 loss: 0.0010 lr: 0.02\n","iteration: 43220 loss: 0.0010 lr: 0.02\n","iteration: 43230 loss: 0.0012 lr: 0.02\n","iteration: 43240 loss: 0.0012 lr: 0.02\n","iteration: 43250 loss: 0.0012 lr: 0.02\n","iteration: 43260 loss: 0.0014 lr: 0.02\n","iteration: 43270 loss: 0.0015 lr: 0.02\n","iteration: 43280 loss: 0.0011 lr: 0.02\n","iteration: 43290 loss: 0.0011 lr: 0.02\n","iteration: 43300 loss: 0.0010 lr: 0.02\n","iteration: 43310 loss: 0.0012 lr: 0.02\n","iteration: 43320 loss: 0.0011 lr: 0.02\n","iteration: 43330 loss: 0.0012 lr: 0.02\n","iteration: 43340 loss: 0.0011 lr: 0.02\n","iteration: 43350 loss: 0.0013 lr: 0.02\n","iteration: 43360 loss: 0.0015 lr: 0.02\n","iteration: 43370 loss: 0.0013 lr: 0.02\n","iteration: 43380 loss: 0.0011 lr: 0.02\n","iteration: 43390 loss: 0.0015 lr: 0.02\n","iteration: 43400 loss: 0.0013 lr: 0.02\n","iteration: 43410 loss: 0.0011 lr: 0.02\n","iteration: 43420 loss: 0.0013 lr: 0.02\n","iteration: 43430 loss: 0.0011 lr: 0.02\n","iteration: 43440 loss: 0.0012 lr: 0.02\n","iteration: 43450 loss: 0.0012 lr: 0.02\n","iteration: 43460 loss: 0.0013 lr: 0.02\n","iteration: 43470 loss: 0.0012 lr: 0.02\n","iteration: 43480 loss: 0.0014 lr: 0.02\n","iteration: 43490 loss: 0.0010 lr: 0.02\n","iteration: 43500 loss: 0.0011 lr: 0.02\n","iteration: 43510 loss: 0.0010 lr: 0.02\n","iteration: 43520 loss: 0.0013 lr: 0.02\n","iteration: 43530 loss: 0.0011 lr: 0.02\n","iteration: 43540 loss: 0.0009 lr: 0.02\n","iteration: 43550 loss: 0.0014 lr: 0.02\n","iteration: 43560 loss: 0.0016 lr: 0.02\n","iteration: 43570 loss: 0.0011 lr: 0.02\n","iteration: 43580 loss: 0.0011 lr: 0.02\n","iteration: 43590 loss: 0.0011 lr: 0.02\n","iteration: 43600 loss: 0.0010 lr: 0.02\n","iteration: 43610 loss: 0.0010 lr: 0.02\n","iteration: 43620 loss: 0.0011 lr: 0.02\n","iteration: 43630 loss: 0.0011 lr: 0.02\n","iteration: 43640 loss: 0.0014 lr: 0.02\n","iteration: 43650 loss: 0.0013 lr: 0.02\n","iteration: 43660 loss: 0.0011 lr: 0.02\n","iteration: 43670 loss: 0.0011 lr: 0.02\n","iteration: 43680 loss: 0.0012 lr: 0.02\n","iteration: 43690 loss: 0.0012 lr: 0.02\n","iteration: 43700 loss: 0.0013 lr: 0.02\n","iteration: 43710 loss: 0.0009 lr: 0.02\n","iteration: 43720 loss: 0.0011 lr: 0.02\n","iteration: 43730 loss: 0.0009 lr: 0.02\n","iteration: 43740 loss: 0.0012 lr: 0.02\n","iteration: 43750 loss: 0.0011 lr: 0.02\n","iteration: 43760 loss: 0.0011 lr: 0.02\n","iteration: 43770 loss: 0.0011 lr: 0.02\n","iteration: 43780 loss: 0.0013 lr: 0.02\n","iteration: 43790 loss: 0.0009 lr: 0.02\n","iteration: 43800 loss: 0.0016 lr: 0.02\n","iteration: 43810 loss: 0.0012 lr: 0.02\n","iteration: 43820 loss: 0.0010 lr: 0.02\n","iteration: 43830 loss: 0.0012 lr: 0.02\n","iteration: 43840 loss: 0.0012 lr: 0.02\n","iteration: 43850 loss: 0.0013 lr: 0.02\n","iteration: 43860 loss: 0.0011 lr: 0.02\n","iteration: 43870 loss: 0.0011 lr: 0.02\n","iteration: 43880 loss: 0.0014 lr: 0.02\n","iteration: 43890 loss: 0.0012 lr: 0.02\n","iteration: 43900 loss: 0.0011 lr: 0.02\n","iteration: 43910 loss: 0.0013 lr: 0.02\n","iteration: 43920 loss: 0.0012 lr: 0.02\n","iteration: 43930 loss: 0.0013 lr: 0.02\n","iteration: 43940 loss: 0.0013 lr: 0.02\n","iteration: 43950 loss: 0.0012 lr: 0.02\n","iteration: 43960 loss: 0.0016 lr: 0.02\n","iteration: 43970 loss: 0.0010 lr: 0.02\n","iteration: 43980 loss: 0.0009 lr: 0.02\n","iteration: 43990 loss: 0.0012 lr: 0.02\n","iteration: 44000 loss: 0.0011 lr: 0.02\n","iteration: 44010 loss: 0.0013 lr: 0.02\n","iteration: 44020 loss: 0.0011 lr: 0.02\n","iteration: 44030 loss: 0.0012 lr: 0.02\n","iteration: 44040 loss: 0.0014 lr: 0.02\n","iteration: 44050 loss: 0.0013 lr: 0.02\n","iteration: 44060 loss: 0.0013 lr: 0.02\n","iteration: 44070 loss: 0.0013 lr: 0.02\n","iteration: 44080 loss: 0.0014 lr: 0.02\n","iteration: 44090 loss: 0.0014 lr: 0.02\n","iteration: 44100 loss: 0.0013 lr: 0.02\n","iteration: 44110 loss: 0.0012 lr: 0.02\n","iteration: 44120 loss: 0.0015 lr: 0.02\n","iteration: 44130 loss: 0.0014 lr: 0.02\n","iteration: 44140 loss: 0.0011 lr: 0.02\n","iteration: 44150 loss: 0.0011 lr: 0.02\n","iteration: 44160 loss: 0.0012 lr: 0.02\n","iteration: 44170 loss: 0.0011 lr: 0.02\n","iteration: 44180 loss: 0.0011 lr: 0.02\n","iteration: 44190 loss: 0.0011 lr: 0.02\n","iteration: 44200 loss: 0.0011 lr: 0.02\n","iteration: 44210 loss: 0.0011 lr: 0.02\n","iteration: 44220 loss: 0.0012 lr: 0.02\n","iteration: 44230 loss: 0.0010 lr: 0.02\n","iteration: 44240 loss: 0.0013 lr: 0.02\n","iteration: 44250 loss: 0.0012 lr: 0.02\n","iteration: 44260 loss: 0.0009 lr: 0.02\n","iteration: 44270 loss: 0.0012 lr: 0.02\n","iteration: 44280 loss: 0.0011 lr: 0.02\n","iteration: 44290 loss: 0.0015 lr: 0.02\n","iteration: 44300 loss: 0.0013 lr: 0.02\n","iteration: 44310 loss: 0.0013 lr: 0.02\n","iteration: 44320 loss: 0.0017 lr: 0.02\n","iteration: 44330 loss: 0.0014 lr: 0.02\n","iteration: 44340 loss: 0.0012 lr: 0.02\n","iteration: 44350 loss: 0.0012 lr: 0.02\n","iteration: 44360 loss: 0.0015 lr: 0.02\n","iteration: 44370 loss: 0.0015 lr: 0.02\n","iteration: 44380 loss: 0.0013 lr: 0.02\n","iteration: 44390 loss: 0.0012 lr: 0.02\n","iteration: 44400 loss: 0.0011 lr: 0.02\n","iteration: 44410 loss: 0.0012 lr: 0.02\n","iteration: 44420 loss: 0.0013 lr: 0.02\n","iteration: 44430 loss: 0.0014 lr: 0.02\n","iteration: 44440 loss: 0.0010 lr: 0.02\n","iteration: 44450 loss: 0.0011 lr: 0.02\n","iteration: 44460 loss: 0.0012 lr: 0.02\n","iteration: 44470 loss: 0.0010 lr: 0.02\n","iteration: 44480 loss: 0.0012 lr: 0.02\n","iteration: 44490 loss: 0.0012 lr: 0.02\n","iteration: 44500 loss: 0.0010 lr: 0.02\n","iteration: 44510 loss: 0.0008 lr: 0.02\n","iteration: 44520 loss: 0.0013 lr: 0.02\n","iteration: 44530 loss: 0.0011 lr: 0.02\n","iteration: 44540 loss: 0.0010 lr: 0.02\n","iteration: 44550 loss: 0.0011 lr: 0.02\n","iteration: 44560 loss: 0.0014 lr: 0.02\n","iteration: 44570 loss: 0.0009 lr: 0.02\n","iteration: 44580 loss: 0.0010 lr: 0.02\n","iteration: 44590 loss: 0.0011 lr: 0.02\n","iteration: 44600 loss: 0.0011 lr: 0.02\n","iteration: 44610 loss: 0.0010 lr: 0.02\n","iteration: 44620 loss: 0.0014 lr: 0.02\n","iteration: 44630 loss: 0.0011 lr: 0.02\n","iteration: 44640 loss: 0.0014 lr: 0.02\n","iteration: 44650 loss: 0.0012 lr: 0.02\n","iteration: 44660 loss: 0.0011 lr: 0.02\n","iteration: 44670 loss: 0.0010 lr: 0.02\n","iteration: 44680 loss: 0.0011 lr: 0.02\n","iteration: 44690 loss: 0.0011 lr: 0.02\n","iteration: 44700 loss: 0.0011 lr: 0.02\n","iteration: 44710 loss: 0.0012 lr: 0.02\n","iteration: 44720 loss: 0.0010 lr: 0.02\n","iteration: 44730 loss: 0.0015 lr: 0.02\n","iteration: 44740 loss: 0.0010 lr: 0.02\n","iteration: 44750 loss: 0.0013 lr: 0.02\n","iteration: 44760 loss: 0.0012 lr: 0.02\n","iteration: 44770 loss: 0.0009 lr: 0.02\n","iteration: 44780 loss: 0.0008 lr: 0.02\n","iteration: 44790 loss: 0.0011 lr: 0.02\n","iteration: 44800 loss: 0.0011 lr: 0.02\n","iteration: 44810 loss: 0.0012 lr: 0.02\n","iteration: 44820 loss: 0.0013 lr: 0.02\n","iteration: 44830 loss: 0.0012 lr: 0.02\n","iteration: 44840 loss: 0.0013 lr: 0.02\n","iteration: 44850 loss: 0.0008 lr: 0.02\n","iteration: 44860 loss: 0.0013 lr: 0.02\n","iteration: 44870 loss: 0.0012 lr: 0.02\n","iteration: 44880 loss: 0.0012 lr: 0.02\n","iteration: 44890 loss: 0.0011 lr: 0.02\n","iteration: 44900 loss: 0.0011 lr: 0.02\n","iteration: 44910 loss: 0.0014 lr: 0.02\n","iteration: 44920 loss: 0.0010 lr: 0.02\n","iteration: 44930 loss: 0.0010 lr: 0.02\n","iteration: 44940 loss: 0.0010 lr: 0.02\n","iteration: 44950 loss: 0.0013 lr: 0.02\n","iteration: 44960 loss: 0.0009 lr: 0.02\n","iteration: 44970 loss: 0.0011 lr: 0.02\n","iteration: 44980 loss: 0.0012 lr: 0.02\n","iteration: 44990 loss: 0.0011 lr: 0.02\n","iteration: 45000 loss: 0.0014 lr: 0.02\n","iteration: 45010 loss: 0.0013 lr: 0.02\n","iteration: 45020 loss: 0.0015 lr: 0.02\n","iteration: 45030 loss: 0.0011 lr: 0.02\n","iteration: 45040 loss: 0.0010 lr: 0.02\n","iteration: 45050 loss: 0.0016 lr: 0.02\n","iteration: 45060 loss: 0.0014 lr: 0.02\n","iteration: 45070 loss: 0.0014 lr: 0.02\n","iteration: 45080 loss: 0.0013 lr: 0.02\n","iteration: 45090 loss: 0.0013 lr: 0.02\n","iteration: 45100 loss: 0.0012 lr: 0.02\n","iteration: 45110 loss: 0.0011 lr: 0.02\n","iteration: 45120 loss: 0.0009 lr: 0.02\n","iteration: 45130 loss: 0.0010 lr: 0.02\n","iteration: 45140 loss: 0.0010 lr: 0.02\n","iteration: 45150 loss: 0.0017 lr: 0.02\n","iteration: 45160 loss: 0.0015 lr: 0.02\n","iteration: 45170 loss: 0.0010 lr: 0.02\n","iteration: 45180 loss: 0.0013 lr: 0.02\n","iteration: 45190 loss: 0.0014 lr: 0.02\n","iteration: 45200 loss: 0.0010 lr: 0.02\n","iteration: 45210 loss: 0.0012 lr: 0.02\n","iteration: 45220 loss: 0.0012 lr: 0.02\n","iteration: 45230 loss: 0.0012 lr: 0.02\n","iteration: 45240 loss: 0.0012 lr: 0.02\n","iteration: 45250 loss: 0.0011 lr: 0.02\n","iteration: 45260 loss: 0.0011 lr: 0.02\n","iteration: 45270 loss: 0.0010 lr: 0.02\n","iteration: 45280 loss: 0.0012 lr: 0.02\n","iteration: 45290 loss: 0.0009 lr: 0.02\n","iteration: 45300 loss: 0.0012 lr: 0.02\n","iteration: 45310 loss: 0.0011 lr: 0.02\n","iteration: 45320 loss: 0.0014 lr: 0.02\n","iteration: 45330 loss: 0.0010 lr: 0.02\n","iteration: 45340 loss: 0.0011 lr: 0.02\n","iteration: 45350 loss: 0.0012 lr: 0.02\n","iteration: 45360 loss: 0.0013 lr: 0.02\n","iteration: 45370 loss: 0.0012 lr: 0.02\n","iteration: 45380 loss: 0.0012 lr: 0.02\n","iteration: 45390 loss: 0.0016 lr: 0.02\n","iteration: 45400 loss: 0.0011 lr: 0.02\n","iteration: 45410 loss: 0.0010 lr: 0.02\n","iteration: 45420 loss: 0.0012 lr: 0.02\n","iteration: 45430 loss: 0.0011 lr: 0.02\n","iteration: 45440 loss: 0.0012 lr: 0.02\n","iteration: 45450 loss: 0.0013 lr: 0.02\n","iteration: 45460 loss: 0.0014 lr: 0.02\n","iteration: 45470 loss: 0.0011 lr: 0.02\n","iteration: 45480 loss: 0.0014 lr: 0.02\n","iteration: 45490 loss: 0.0011 lr: 0.02\n","iteration: 45500 loss: 0.0016 lr: 0.02\n","iteration: 45510 loss: 0.0014 lr: 0.02\n","iteration: 45520 loss: 0.0014 lr: 0.02\n","iteration: 45530 loss: 0.0014 lr: 0.02\n","iteration: 45540 loss: 0.0009 lr: 0.02\n","iteration: 45550 loss: 0.0012 lr: 0.02\n","iteration: 45560 loss: 0.0010 lr: 0.02\n","iteration: 45570 loss: 0.0012 lr: 0.02\n","iteration: 45580 loss: 0.0012 lr: 0.02\n","iteration: 45590 loss: 0.0010 lr: 0.02\n","iteration: 45600 loss: 0.0010 lr: 0.02\n","iteration: 45610 loss: 0.0013 lr: 0.02\n","iteration: 45620 loss: 0.0012 lr: 0.02\n","iteration: 45630 loss: 0.0014 lr: 0.02\n","iteration: 45640 loss: 0.0009 lr: 0.02\n","iteration: 45650 loss: 0.0013 lr: 0.02\n","iteration: 45660 loss: 0.0012 lr: 0.02\n","iteration: 45670 loss: 0.0011 lr: 0.02\n","iteration: 45680 loss: 0.0011 lr: 0.02\n","iteration: 45690 loss: 0.0011 lr: 0.02\n","iteration: 45700 loss: 0.0011 lr: 0.02\n","iteration: 45710 loss: 0.0012 lr: 0.02\n","iteration: 45720 loss: 0.0013 lr: 0.02\n","iteration: 45730 loss: 0.0015 lr: 0.02\n","iteration: 45740 loss: 0.0017 lr: 0.02\n","iteration: 45750 loss: 0.0015 lr: 0.02\n","iteration: 45760 loss: 0.0013 lr: 0.02\n","iteration: 45770 loss: 0.0015 lr: 0.02\n","iteration: 45780 loss: 0.0013 lr: 0.02\n","iteration: 45790 loss: 0.0010 lr: 0.02\n","iteration: 45800 loss: 0.0010 lr: 0.02\n","iteration: 45810 loss: 0.0010 lr: 0.02\n","iteration: 45820 loss: 0.0014 lr: 0.02\n","iteration: 45830 loss: 0.0013 lr: 0.02\n","iteration: 45840 loss: 0.0011 lr: 0.02\n","iteration: 45850 loss: 0.0013 lr: 0.02\n","iteration: 45860 loss: 0.0012 lr: 0.02\n","iteration: 45870 loss: 0.0012 lr: 0.02\n","iteration: 45880 loss: 0.0011 lr: 0.02\n","iteration: 45890 loss: 0.0010 lr: 0.02\n","iteration: 45900 loss: 0.0011 lr: 0.02\n","iteration: 45910 loss: 0.0011 lr: 0.02\n","iteration: 45920 loss: 0.0013 lr: 0.02\n","iteration: 45930 loss: 0.0010 lr: 0.02\n","iteration: 45940 loss: 0.0012 lr: 0.02\n","iteration: 45950 loss: 0.0014 lr: 0.02\n","iteration: 45960 loss: 0.0013 lr: 0.02\n","iteration: 45970 loss: 0.0012 lr: 0.02\n","iteration: 45980 loss: 0.0010 lr: 0.02\n","iteration: 45990 loss: 0.0011 lr: 0.02\n","iteration: 46000 loss: 0.0010 lr: 0.02\n","iteration: 46010 loss: 0.0014 lr: 0.02\n","iteration: 46020 loss: 0.0013 lr: 0.02\n","iteration: 46030 loss: 0.0012 lr: 0.02\n","iteration: 46040 loss: 0.0013 lr: 0.02\n","iteration: 46050 loss: 0.0012 lr: 0.02\n","iteration: 46060 loss: 0.0011 lr: 0.02\n","iteration: 46070 loss: 0.0011 lr: 0.02\n","iteration: 46080 loss: 0.0012 lr: 0.02\n","iteration: 46090 loss: 0.0011 lr: 0.02\n","iteration: 46100 loss: 0.0012 lr: 0.02\n","iteration: 46110 loss: 0.0011 lr: 0.02\n","iteration: 46120 loss: 0.0014 lr: 0.02\n","iteration: 46130 loss: 0.0012 lr: 0.02\n","iteration: 46140 loss: 0.0013 lr: 0.02\n","iteration: 46150 loss: 0.0009 lr: 0.02\n","iteration: 46160 loss: 0.0012 lr: 0.02\n","iteration: 46170 loss: 0.0014 lr: 0.02\n","iteration: 46180 loss: 0.0015 lr: 0.02\n","iteration: 46190 loss: 0.0012 lr: 0.02\n","iteration: 46200 loss: 0.0010 lr: 0.02\n","iteration: 46210 loss: 0.0011 lr: 0.02\n","iteration: 46220 loss: 0.0009 lr: 0.02\n","iteration: 46230 loss: 0.0011 lr: 0.02\n","iteration: 46240 loss: 0.0011 lr: 0.02\n","iteration: 46250 loss: 0.0011 lr: 0.02\n","iteration: 46260 loss: 0.0008 lr: 0.02\n","iteration: 46270 loss: 0.0010 lr: 0.02\n","iteration: 46280 loss: 0.0010 lr: 0.02\n","iteration: 46290 loss: 0.0011 lr: 0.02\n","iteration: 46300 loss: 0.0013 lr: 0.02\n","iteration: 46310 loss: 0.0011 lr: 0.02\n","iteration: 46320 loss: 0.0011 lr: 0.02\n","iteration: 46330 loss: 0.0012 lr: 0.02\n","iteration: 46340 loss: 0.0014 lr: 0.02\n","iteration: 46350 loss: 0.0013 lr: 0.02\n","iteration: 46360 loss: 0.0012 lr: 0.02\n","iteration: 46370 loss: 0.0011 lr: 0.02\n","iteration: 46380 loss: 0.0012 lr: 0.02\n","iteration: 46390 loss: 0.0010 lr: 0.02\n","iteration: 46400 loss: 0.0010 lr: 0.02\n","iteration: 46410 loss: 0.0015 lr: 0.02\n","iteration: 46420 loss: 0.0010 lr: 0.02\n","iteration: 46430 loss: 0.0011 lr: 0.02\n","iteration: 46440 loss: 0.0012 lr: 0.02\n","iteration: 46450 loss: 0.0012 lr: 0.02\n","iteration: 46460 loss: 0.0012 lr: 0.02\n","iteration: 46470 loss: 0.0014 lr: 0.02\n","iteration: 46480 loss: 0.0013 lr: 0.02\n","iteration: 46490 loss: 0.0010 lr: 0.02\n","iteration: 46500 loss: 0.0013 lr: 0.02\n","iteration: 46510 loss: 0.0010 lr: 0.02\n","iteration: 46520 loss: 0.0011 lr: 0.02\n","iteration: 46530 loss: 0.0011 lr: 0.02\n","iteration: 46540 loss: 0.0012 lr: 0.02\n","iteration: 46550 loss: 0.0011 lr: 0.02\n","iteration: 46560 loss: 0.0011 lr: 0.02\n","iteration: 46570 loss: 0.0011 lr: 0.02\n","iteration: 46580 loss: 0.0012 lr: 0.02\n","iteration: 46590 loss: 0.0012 lr: 0.02\n","iteration: 46600 loss: 0.0013 lr: 0.02\n","iteration: 46610 loss: 0.0013 lr: 0.02\n","iteration: 46620 loss: 0.0010 lr: 0.02\n","iteration: 46630 loss: 0.0016 lr: 0.02\n","iteration: 46640 loss: 0.0013 lr: 0.02\n","iteration: 46650 loss: 0.0012 lr: 0.02\n","iteration: 46660 loss: 0.0011 lr: 0.02\n","iteration: 46670 loss: 0.0013 lr: 0.02\n","iteration: 46680 loss: 0.0019 lr: 0.02\n","iteration: 46690 loss: 0.0014 lr: 0.02\n","iteration: 46700 loss: 0.0011 lr: 0.02\n","iteration: 46710 loss: 0.0012 lr: 0.02\n","iteration: 46720 loss: 0.0014 lr: 0.02\n","iteration: 46730 loss: 0.0012 lr: 0.02\n","iteration: 46740 loss: 0.0011 lr: 0.02\n","iteration: 46750 loss: 0.0013 lr: 0.02\n","iteration: 46760 loss: 0.0013 lr: 0.02\n","iteration: 46770 loss: 0.0013 lr: 0.02\n","iteration: 46780 loss: 0.0011 lr: 0.02\n","iteration: 46790 loss: 0.0010 lr: 0.02\n","iteration: 46800 loss: 0.0016 lr: 0.02\n","iteration: 46810 loss: 0.0009 lr: 0.02\n","iteration: 46820 loss: 0.0010 lr: 0.02\n","iteration: 46830 loss: 0.0011 lr: 0.02\n","iteration: 46840 loss: 0.0010 lr: 0.02\n","iteration: 46850 loss: 0.0010 lr: 0.02\n","iteration: 46860 loss: 0.0013 lr: 0.02\n","iteration: 46870 loss: 0.0012 lr: 0.02\n","iteration: 46880 loss: 0.0011 lr: 0.02\n","iteration: 46890 loss: 0.0014 lr: 0.02\n","iteration: 46900 loss: 0.0011 lr: 0.02\n","iteration: 46910 loss: 0.0014 lr: 0.02\n","iteration: 46920 loss: 0.0011 lr: 0.02\n","iteration: 46930 loss: 0.0011 lr: 0.02\n","iteration: 46940 loss: 0.0010 lr: 0.02\n","iteration: 46950 loss: 0.0009 lr: 0.02\n","iteration: 46960 loss: 0.0015 lr: 0.02\n","iteration: 46970 loss: 0.0010 lr: 0.02\n","iteration: 46980 loss: 0.0012 lr: 0.02\n","iteration: 46990 loss: 0.0013 lr: 0.02\n","iteration: 47000 loss: 0.0014 lr: 0.02\n","iteration: 47010 loss: 0.0011 lr: 0.02\n","iteration: 47020 loss: 0.0009 lr: 0.02\n","iteration: 47030 loss: 0.0011 lr: 0.02\n","iteration: 47040 loss: 0.0013 lr: 0.02\n","iteration: 47050 loss: 0.0013 lr: 0.02\n","iteration: 47060 loss: 0.0013 lr: 0.02\n","iteration: 47070 loss: 0.0013 lr: 0.02\n","iteration: 47080 loss: 0.0014 lr: 0.02\n","iteration: 47090 loss: 0.0011 lr: 0.02\n","iteration: 47100 loss: 0.0013 lr: 0.02\n","iteration: 47110 loss: 0.0011 lr: 0.02\n","iteration: 47120 loss: 0.0013 lr: 0.02\n","iteration: 47130 loss: 0.0013 lr: 0.02\n","iteration: 47140 loss: 0.0010 lr: 0.02\n","iteration: 47150 loss: 0.0011 lr: 0.02\n","iteration: 47160 loss: 0.0011 lr: 0.02\n","iteration: 47170 loss: 0.0012 lr: 0.02\n","iteration: 47180 loss: 0.0013 lr: 0.02\n","iteration: 47190 loss: 0.0012 lr: 0.02\n","iteration: 47200 loss: 0.0015 lr: 0.02\n","iteration: 47210 loss: 0.0013 lr: 0.02\n","iteration: 47220 loss: 0.0011 lr: 0.02\n","iteration: 47230 loss: 0.0015 lr: 0.02\n","iteration: 47240 loss: 0.0014 lr: 0.02\n","iteration: 47250 loss: 0.0012 lr: 0.02\n","iteration: 47260 loss: 0.0011 lr: 0.02\n","iteration: 47270 loss: 0.0013 lr: 0.02\n","iteration: 47280 loss: 0.0011 lr: 0.02\n","iteration: 47290 loss: 0.0010 lr: 0.02\n","iteration: 47300 loss: 0.0010 lr: 0.02\n","iteration: 47310 loss: 0.0012 lr: 0.02\n","iteration: 47320 loss: 0.0012 lr: 0.02\n","iteration: 47330 loss: 0.0011 lr: 0.02\n","iteration: 47340 loss: 0.0012 lr: 0.02\n","iteration: 47350 loss: 0.0012 lr: 0.02\n","iteration: 47360 loss: 0.0012 lr: 0.02\n","iteration: 47370 loss: 0.0011 lr: 0.02\n","iteration: 47380 loss: 0.0011 lr: 0.02\n","iteration: 47390 loss: 0.0013 lr: 0.02\n","iteration: 47400 loss: 0.0013 lr: 0.02\n","iteration: 47410 loss: 0.0013 lr: 0.02\n","iteration: 47420 loss: 0.0011 lr: 0.02\n","iteration: 47430 loss: 0.0013 lr: 0.02\n","iteration: 47440 loss: 0.0015 lr: 0.02\n","iteration: 47450 loss: 0.0010 lr: 0.02\n","iteration: 47460 loss: 0.0010 lr: 0.02\n","iteration: 47470 loss: 0.0015 lr: 0.02\n","iteration: 47480 loss: 0.0010 lr: 0.02\n","iteration: 47490 loss: 0.0012 lr: 0.02\n","iteration: 47500 loss: 0.0014 lr: 0.02\n","iteration: 47510 loss: 0.0012 lr: 0.02\n","iteration: 47520 loss: 0.0012 lr: 0.02\n","iteration: 47530 loss: 0.0012 lr: 0.02\n","iteration: 47540 loss: 0.0017 lr: 0.02\n","iteration: 47550 loss: 0.0014 lr: 0.02\n","iteration: 47560 loss: 0.0009 lr: 0.02\n","iteration: 47570 loss: 0.0009 lr: 0.02\n","iteration: 47580 loss: 0.0010 lr: 0.02\n","iteration: 47590 loss: 0.0011 lr: 0.02\n","iteration: 47600 loss: 0.0009 lr: 0.02\n","iteration: 47610 loss: 0.0015 lr: 0.02\n","iteration: 47620 loss: 0.0012 lr: 0.02\n","iteration: 47630 loss: 0.0011 lr: 0.02\n","iteration: 47640 loss: 0.0012 lr: 0.02\n","iteration: 47650 loss: 0.0015 lr: 0.02\n","iteration: 47660 loss: 0.0013 lr: 0.02\n","iteration: 47670 loss: 0.0011 lr: 0.02\n","iteration: 47680 loss: 0.0014 lr: 0.02\n","iteration: 47690 loss: 0.0012 lr: 0.02\n","iteration: 47700 loss: 0.0010 lr: 0.02\n","iteration: 47710 loss: 0.0011 lr: 0.02\n","iteration: 47720 loss: 0.0010 lr: 0.02\n","iteration: 47730 loss: 0.0012 lr: 0.02\n","iteration: 47740 loss: 0.0011 lr: 0.02\n","iteration: 47750 loss: 0.0010 lr: 0.02\n","iteration: 47760 loss: 0.0009 lr: 0.02\n","iteration: 47770 loss: 0.0011 lr: 0.02\n","iteration: 47780 loss: 0.0012 lr: 0.02\n","iteration: 47790 loss: 0.0014 lr: 0.02\n","iteration: 47800 loss: 0.0012 lr: 0.02\n","iteration: 47810 loss: 0.0009 lr: 0.02\n","iteration: 47820 loss: 0.0014 lr: 0.02\n","iteration: 47830 loss: 0.0010 lr: 0.02\n","iteration: 47840 loss: 0.0013 lr: 0.02\n","iteration: 47850 loss: 0.0013 lr: 0.02\n","iteration: 47860 loss: 0.0014 lr: 0.02\n","iteration: 47870 loss: 0.0014 lr: 0.02\n","iteration: 47880 loss: 0.0011 lr: 0.02\n","iteration: 47890 loss: 0.0012 lr: 0.02\n","iteration: 47900 loss: 0.0012 lr: 0.02\n","iteration: 47910 loss: 0.0011 lr: 0.02\n","iteration: 47920 loss: 0.0017 lr: 0.02\n","iteration: 47930 loss: 0.0011 lr: 0.02\n","iteration: 47940 loss: 0.0010 lr: 0.02\n","iteration: 47950 loss: 0.0012 lr: 0.02\n","iteration: 47960 loss: 0.0011 lr: 0.02\n","iteration: 47970 loss: 0.0009 lr: 0.02\n","iteration: 47980 loss: 0.0014 lr: 0.02\n","iteration: 47990 loss: 0.0012 lr: 0.02\n","iteration: 48000 loss: 0.0014 lr: 0.02\n","iteration: 48010 loss: 0.0012 lr: 0.02\n","iteration: 48020 loss: 0.0013 lr: 0.02\n","iteration: 48030 loss: 0.0009 lr: 0.02\n","iteration: 48040 loss: 0.0013 lr: 0.02\n","iteration: 48050 loss: 0.0011 lr: 0.02\n","iteration: 48060 loss: 0.0011 lr: 0.02\n","iteration: 48070 loss: 0.0010 lr: 0.02\n","iteration: 48080 loss: 0.0009 lr: 0.02\n","iteration: 48090 loss: 0.0012 lr: 0.02\n","iteration: 48100 loss: 0.0014 lr: 0.02\n","iteration: 48110 loss: 0.0010 lr: 0.02\n","iteration: 48120 loss: 0.0013 lr: 0.02\n","iteration: 48130 loss: 0.0013 lr: 0.02\n","iteration: 48140 loss: 0.0011 lr: 0.02\n","iteration: 48150 loss: 0.0012 lr: 0.02\n","iteration: 48160 loss: 0.0012 lr: 0.02\n","iteration: 48170 loss: 0.0014 lr: 0.02\n","iteration: 48180 loss: 0.0013 lr: 0.02\n","iteration: 48190 loss: 0.0013 lr: 0.02\n","iteration: 48200 loss: 0.0011 lr: 0.02\n","iteration: 48210 loss: 0.0011 lr: 0.02\n","iteration: 48220 loss: 0.0013 lr: 0.02\n","iteration: 48230 loss: 0.0012 lr: 0.02\n","iteration: 48240 loss: 0.0014 lr: 0.02\n","iteration: 48250 loss: 0.0010 lr: 0.02\n","iteration: 48260 loss: 0.0012 lr: 0.02\n","iteration: 48270 loss: 0.0012 lr: 0.02\n","iteration: 48280 loss: 0.0013 lr: 0.02\n","iteration: 48290 loss: 0.0010 lr: 0.02\n","iteration: 48300 loss: 0.0012 lr: 0.02\n","iteration: 48310 loss: 0.0015 lr: 0.02\n","iteration: 48320 loss: 0.0010 lr: 0.02\n","iteration: 48330 loss: 0.0014 lr: 0.02\n","iteration: 48340 loss: 0.0009 lr: 0.02\n","iteration: 48350 loss: 0.0012 lr: 0.02\n","iteration: 48360 loss: 0.0011 lr: 0.02\n","iteration: 48370 loss: 0.0012 lr: 0.02\n","iteration: 48380 loss: 0.0010 lr: 0.02\n","iteration: 48390 loss: 0.0011 lr: 0.02\n","iteration: 48400 loss: 0.0010 lr: 0.02\n","iteration: 48410 loss: 0.0012 lr: 0.02\n","iteration: 48420 loss: 0.0011 lr: 0.02\n","iteration: 48430 loss: 0.0012 lr: 0.02\n","iteration: 48440 loss: 0.0010 lr: 0.02\n","iteration: 48450 loss: 0.0014 lr: 0.02\n","iteration: 48460 loss: 0.0014 lr: 0.02\n","iteration: 48470 loss: 0.0014 lr: 0.02\n","iteration: 48480 loss: 0.0011 lr: 0.02\n","iteration: 48490 loss: 0.0012 lr: 0.02\n","iteration: 48500 loss: 0.0012 lr: 0.02\n","iteration: 48510 loss: 0.0015 lr: 0.02\n","iteration: 48520 loss: 0.0013 lr: 0.02\n","iteration: 48530 loss: 0.0009 lr: 0.02\n","iteration: 48540 loss: 0.0013 lr: 0.02\n","iteration: 48550 loss: 0.0014 lr: 0.02\n","iteration: 48560 loss: 0.0012 lr: 0.02\n","iteration: 48570 loss: 0.0010 lr: 0.02\n","iteration: 48580 loss: 0.0010 lr: 0.02\n","iteration: 48590 loss: 0.0009 lr: 0.02\n","iteration: 48600 loss: 0.0011 lr: 0.02\n","iteration: 48610 loss: 0.0012 lr: 0.02\n","iteration: 48620 loss: 0.0012 lr: 0.02\n","iteration: 48630 loss: 0.0009 lr: 0.02\n","iteration: 48640 loss: 0.0010 lr: 0.02\n","iteration: 48650 loss: 0.0014 lr: 0.02\n","iteration: 48660 loss: 0.0014 lr: 0.02\n","iteration: 48670 loss: 0.0009 lr: 0.02\n","iteration: 48680 loss: 0.0009 lr: 0.02\n","iteration: 48690 loss: 0.0013 lr: 0.02\n","iteration: 48700 loss: 0.0010 lr: 0.02\n","iteration: 48710 loss: 0.0011 lr: 0.02\n","iteration: 48720 loss: 0.0012 lr: 0.02\n","iteration: 48730 loss: 0.0012 lr: 0.02\n","iteration: 48740 loss: 0.0012 lr: 0.02\n","iteration: 48750 loss: 0.0013 lr: 0.02\n","iteration: 48760 loss: 0.0011 lr: 0.02\n","iteration: 48770 loss: 0.0013 lr: 0.02\n","iteration: 48780 loss: 0.0013 lr: 0.02\n","iteration: 48790 loss: 0.0012 lr: 0.02\n","iteration: 48800 loss: 0.0012 lr: 0.02\n","iteration: 48810 loss: 0.0010 lr: 0.02\n","iteration: 48820 loss: 0.0011 lr: 0.02\n","iteration: 48830 loss: 0.0011 lr: 0.02\n","iteration: 48840 loss: 0.0010 lr: 0.02\n","iteration: 48850 loss: 0.0016 lr: 0.02\n","iteration: 48860 loss: 0.0010 lr: 0.02\n","iteration: 48870 loss: 0.0015 lr: 0.02\n","iteration: 48880 loss: 0.0011 lr: 0.02\n","iteration: 48890 loss: 0.0017 lr: 0.02\n","iteration: 48900 loss: 0.0014 lr: 0.02\n","iteration: 48910 loss: 0.0011 lr: 0.02\n","iteration: 48920 loss: 0.0011 lr: 0.02\n","iteration: 48930 loss: 0.0010 lr: 0.02\n","iteration: 48940 loss: 0.0013 lr: 0.02\n","iteration: 48950 loss: 0.0013 lr: 0.02\n","iteration: 48960 loss: 0.0013 lr: 0.02\n","iteration: 48970 loss: 0.0011 lr: 0.02\n","iteration: 48980 loss: 0.0011 lr: 0.02\n","iteration: 48990 loss: 0.0014 lr: 0.02\n","iteration: 49000 loss: 0.0012 lr: 0.02\n","iteration: 49010 loss: 0.0012 lr: 0.02\n","iteration: 49020 loss: 0.0011 lr: 0.02\n","iteration: 49030 loss: 0.0009 lr: 0.02\n","iteration: 49040 loss: 0.0013 lr: 0.02\n","iteration: 49050 loss: 0.0014 lr: 0.02\n","iteration: 49060 loss: 0.0013 lr: 0.02\n","iteration: 49070 loss: 0.0015 lr: 0.02\n","iteration: 49080 loss: 0.0014 lr: 0.02\n","iteration: 49090 loss: 0.0012 lr: 0.02\n","iteration: 49100 loss: 0.0011 lr: 0.02\n","iteration: 49110 loss: 0.0013 lr: 0.02\n","iteration: 49120 loss: 0.0010 lr: 0.02\n","iteration: 49130 loss: 0.0011 lr: 0.02\n","iteration: 49140 loss: 0.0012 lr: 0.02\n","iteration: 49150 loss: 0.0010 lr: 0.02\n","iteration: 49160 loss: 0.0014 lr: 0.02\n","iteration: 49170 loss: 0.0012 lr: 0.02\n","iteration: 49180 loss: 0.0012 lr: 0.02\n","iteration: 49190 loss: 0.0012 lr: 0.02\n","iteration: 49200 loss: 0.0013 lr: 0.02\n","iteration: 49210 loss: 0.0011 lr: 0.02\n","iteration: 49220 loss: 0.0010 lr: 0.02\n","iteration: 49230 loss: 0.0015 lr: 0.02\n","iteration: 49240 loss: 0.0010 lr: 0.02\n","iteration: 49250 loss: 0.0010 lr: 0.02\n","iteration: 49260 loss: 0.0011 lr: 0.02\n","iteration: 49270 loss: 0.0010 lr: 0.02\n","iteration: 49280 loss: 0.0015 lr: 0.02\n","iteration: 49290 loss: 0.0010 lr: 0.02\n","iteration: 49300 loss: 0.0013 lr: 0.02\n","iteration: 49310 loss: 0.0010 lr: 0.02\n","iteration: 49320 loss: 0.0014 lr: 0.02\n","iteration: 49330 loss: 0.0011 lr: 0.02\n","iteration: 49340 loss: 0.0011 lr: 0.02\n","iteration: 49350 loss: 0.0012 lr: 0.02\n","iteration: 49360 loss: 0.0011 lr: 0.02\n","iteration: 49370 loss: 0.0016 lr: 0.02\n","iteration: 49380 loss: 0.0013 lr: 0.02\n","iteration: 49390 loss: 0.0010 lr: 0.02\n","iteration: 49400 loss: 0.0011 lr: 0.02\n","iteration: 49410 loss: 0.0010 lr: 0.02\n","iteration: 49420 loss: 0.0010 lr: 0.02\n","iteration: 49430 loss: 0.0012 lr: 0.02\n","iteration: 49440 loss: 0.0014 lr: 0.02\n","iteration: 49450 loss: 0.0011 lr: 0.02\n","iteration: 49460 loss: 0.0013 lr: 0.02\n","iteration: 49470 loss: 0.0011 lr: 0.02\n","iteration: 49480 loss: 0.0012 lr: 0.02\n","iteration: 49490 loss: 0.0010 lr: 0.02\n","iteration: 49500 loss: 0.0014 lr: 0.02\n","iteration: 49510 loss: 0.0012 lr: 0.02\n","iteration: 49520 loss: 0.0012 lr: 0.02\n","iteration: 49530 loss: 0.0012 lr: 0.02\n","iteration: 49540 loss: 0.0009 lr: 0.02\n","iteration: 49550 loss: 0.0012 lr: 0.02\n","iteration: 49560 loss: 0.0013 lr: 0.02\n","iteration: 49570 loss: 0.0013 lr: 0.02\n","iteration: 49580 loss: 0.0014 lr: 0.02\n","iteration: 49590 loss: 0.0012 lr: 0.02\n","iteration: 49600 loss: 0.0014 lr: 0.02\n","iteration: 49610 loss: 0.0011 lr: 0.02\n","iteration: 49620 loss: 0.0013 lr: 0.02\n","iteration: 49630 loss: 0.0012 lr: 0.02\n","iteration: 49640 loss: 0.0015 lr: 0.02\n","iteration: 49650 loss: 0.0011 lr: 0.02\n","iteration: 49660 loss: 0.0011 lr: 0.02\n","iteration: 49670 loss: 0.0011 lr: 0.02\n","iteration: 49680 loss: 0.0011 lr: 0.02\n","iteration: 49690 loss: 0.0017 lr: 0.02\n","iteration: 49700 loss: 0.0014 lr: 0.02\n","iteration: 49710 loss: 0.0012 lr: 0.02\n","iteration: 49720 loss: 0.0017 lr: 0.02\n","iteration: 49730 loss: 0.0013 lr: 0.02\n","iteration: 49740 loss: 0.0014 lr: 0.02\n","iteration: 49750 loss: 0.0016 lr: 0.02\n","iteration: 49760 loss: 0.0015 lr: 0.02\n","iteration: 49770 loss: 0.0015 lr: 0.02\n","iteration: 49780 loss: 0.0011 lr: 0.02\n","iteration: 49790 loss: 0.0012 lr: 0.02\n","iteration: 49800 loss: 0.0010 lr: 0.02\n","iteration: 49810 loss: 0.0010 lr: 0.02\n","iteration: 49820 loss: 0.0011 lr: 0.02\n","iteration: 49830 loss: 0.0011 lr: 0.02\n","iteration: 49840 loss: 0.0011 lr: 0.02\n","iteration: 49850 loss: 0.0014 lr: 0.02\n","iteration: 49860 loss: 0.0013 lr: 0.02\n","iteration: 49870 loss: 0.0010 lr: 0.02\n","iteration: 49880 loss: 0.0014 lr: 0.02\n","iteration: 49890 loss: 0.0013 lr: 0.02\n","iteration: 49900 loss: 0.0011 lr: 0.02\n","iteration: 49910 loss: 0.0009 lr: 0.02\n","iteration: 49920 loss: 0.0013 lr: 0.02\n","iteration: 49930 loss: 0.0013 lr: 0.02\n","iteration: 49940 loss: 0.0012 lr: 0.02\n","iteration: 49950 loss: 0.0011 lr: 0.02\n","iteration: 49960 loss: 0.0012 lr: 0.02\n","iteration: 49970 loss: 0.0010 lr: 0.02\n","iteration: 49980 loss: 0.0012 lr: 0.02\n","iteration: 49990 loss: 0.0016 lr: 0.02\n","iteration: 50000 loss: 0.0012 lr: 0.02\n","iteration: 50010 loss: 0.0009 lr: 0.02\n","iteration: 50020 loss: 0.0009 lr: 0.02\n","iteration: 50030 loss: 0.0011 lr: 0.02\n","iteration: 50040 loss: 0.0012 lr: 0.02\n","iteration: 50050 loss: 0.0010 lr: 0.02\n","iteration: 50060 loss: 0.0010 lr: 0.02\n","iteration: 50070 loss: 0.0011 lr: 0.02\n","iteration: 50080 loss: 0.0011 lr: 0.02\n","iteration: 50090 loss: 0.0013 lr: 0.02\n","iteration: 50100 loss: 0.0011 lr: 0.02\n","iteration: 50110 loss: 0.0014 lr: 0.02\n","iteration: 50120 loss: 0.0016 lr: 0.02\n","iteration: 50130 loss: 0.0014 lr: 0.02\n","iteration: 50140 loss: 0.0011 lr: 0.02\n","iteration: 50150 loss: 0.0013 lr: 0.02\n","iteration: 50160 loss: 0.0014 lr: 0.02\n","iteration: 50170 loss: 0.0011 lr: 0.02\n","iteration: 50180 loss: 0.0011 lr: 0.02\n","iteration: 50190 loss: 0.0012 lr: 0.02\n","iteration: 50200 loss: 0.0011 lr: 0.02\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"svK9C-B5gP-X"},"source":["## Restart training:\n","If the training terminates early (e.g. VM disconnects or you CTRL+C out), you can restart the training at a specific checkpoint. Specify the full path of the checkpoint to the variable `init_weights` in the **pose_cfg.yaml** file under the *train* subdirectory. Specifically, change the `init_weights` path from the default (/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt) to, for example, /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2020-12-31/dlc-models/iteration-0/wirelessStimDec31-trainset95shuffle1/train/snapshot-104500. Note that the iteration counter restarts at zero, but it is continuing from the specified snapshot. "]},{"cell_type":"markdown","metadata":{"id":"xZygsb2DoEJc"},"source":["## Evaluate model:\n","This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n","and stores the results as .csv file in a subdirectory under **evaluation-results**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":392},"id":"nv4zlbrnoEJg","executionInfo":{"elapsed":364,"status":"error","timestamp":1609636131267,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"},"user_tz":300},"outputId":"1e885856-08d3-41d1-875c-3bd7cf696ca3"},"source":["%matplotlib notebook\n","deeplabcut.evaluate_network(path_config_file,plotting=True)\n","\n","# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, \n","#so be sure your labels are good! (And you have trained enough ;)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ScannerError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mScannerError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-d4de8d002536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplotting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Here you want to see a low pixel error! Of course, it can only be as good as the labeler,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#so be sure your labels are good! (And you have trained enough ;)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/evaluate.py\u001b[0m in \u001b[0;36mevaluate_network\u001b[0;34m(config, Shuffles, trainingsetindex, plotting, show_errors, comparisonbodyparts, gputouse, rescale, modelprefix, c_engine)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauxiliaryfunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauxiliaryfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multianimalproject\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deeplabcut/utils/auxiliaryfunctions.py\u001b[0m in \u001b[0;36mread_config\u001b[0;34m(configname)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruamelFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0mcurr_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"project_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcurr_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/main.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mconstructor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_constructor_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/constructor.py\u001b[0m in \u001b[0;36mget_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# type: () -> Any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Ensure that the stream contains a single document and construct it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomposer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/composer.py\u001b[0m in \u001b[0;36mget_single_node\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# type: Any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStreamEndEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Ensure that the stream contains no more documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/composer.py\u001b[0m in \u001b[0;36mcompose_document\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Compose the root node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Drop the DOCUMENT-END event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/composer.py\u001b[0m in \u001b[0;36mcompose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_sequence_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMappingStartEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_mapping_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascend_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/composer.py\u001b[0m in \u001b[0;36mcompose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;31m#             start_event.start_mark,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m#             \"found duplicate key\", key_event.start_mark)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mitem_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;31m# node.value[item_key] = item_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/composer.py\u001b[0m in \u001b[0;36mcompose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompose_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# type: (Any, Any) -> Any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAliasEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0malias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/parser.py\u001b[0m in \u001b[0;36mcheck_event\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_event\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_event\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/parser.py\u001b[0m in \u001b[0;36mparse_block_mapping_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscanner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# value token might have post comment move it to e.g. block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscanner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                 \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscanner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/scanner.py\u001b[0m in \u001b[0;36mcheck_token\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneed_more_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_more_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_comments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/scanner.py\u001b[0m in \u001b[0;36m_gather_comments\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0;31m# pull in post comment on e.g. ':'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_more_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/scanner.py\u001b[0m in \u001b[0;36mfetch_more_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Is it the value indicator?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;31m# Is it an alias?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ruamel/yaml/scanner.py\u001b[0m in \u001b[0;36mfetch_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m                         \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                         \u001b[0;34m'mapping values are not allowed here'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                     )\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mScannerError\u001b[0m: mapping values are not allowed here\n  in \"/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/config.yaml\", line 13, column 9"]}]},{"cell_type":"markdown","metadata":{"id":"OVFLSKKfoEJk"},"source":["## Analyze videos: \n","This function performs pose inference on the videos in `videofile_path`. The user can choose the best model from the evaluation results and specify the correct snapshot index in the variable `snapshotindex` in the **config.yaml** file. Otherwise, by default the most recent snapshot is used.\n","\n","The results are stored in hd5 file in the same directory where the video resides. Set `save_as_csv` to `True` if want a separate csv file of pose locations on each frame that can be imported into Matlab or other analysis software."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"id":"Y_LZiS_0oEJl","executionInfo":{"elapsed":1680628,"status":"ok","timestamp":1609781464533,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"},"user_tz":300},"outputId":"6589d295-5f70-4a44-fb09-5c8ba0ada9d2"},"source":["deeplabcut.analyze_videos(path_config_file,videofile_path, videotype=VideoType, save_as_csv=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using snapshot-50000 for model /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/dlc-models/iteration-0/wirelessStimJan1-trainset95shuffle1\n","Initializing ResNet\n","Analyzing all the videos in the directory...\n","Starting to analyze %  /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day9-10W.mp4\n","/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos  already exists!\n","Loading  /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day9-10W.mp4\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/17978 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Duration of video [s]:  599.27 , recorded with  30.0 fps!\n","Overall # of frames:  17978  found with (before cropping) frame dimensions:  1080 1920\n","Starting to extract posture\n"],"name":"stdout"},{"output_type":"stream","text":["18079it [1:35:41,  3.13it/s]                           "],"name":"stderr"},{"output_type":"stream","text":["Detected frames:  17978\n"],"name":"stdout"},{"output_type":"stream","text":["\r18079it [1:36:09,  3.13it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Saving results in /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos...\n","Saving csv poses!\n","Starting to analyze %  /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day15-8W.mp4\n","/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos  already exists!\n","Loading  /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day15-8W.mp4\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/18019 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Duration of video [s]:  600.63 , recorded with  30.0 fps!\n","Overall # of frames:  18019  found with (before cropping) frame dimensions:  1080 1920\n","Starting to extract posture\n"],"name":"stdout"},{"output_type":"stream","text":["18180it [1:36:07,  3.11it/s]                           "],"name":"stderr"},{"output_type":"stream","text":["Detected frames:  18019\n"],"name":"stdout"},{"output_type":"stream","text":["\r18180it [1:36:15,  3.15it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Saving results in /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos...\n","Saving csv poses!\n","The videos are analyzed. Now your research can truly start! \n"," You can create labeled videos with 'create_labeled_video'\n","If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'DLC_resnet50_wirelessStimJan1shuffle1_50000'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"8GTiuJESoEKH"},"source":["## Plot the trajectories of the analyzed videos:\n","This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gX21zZbXoEKJ","executionInfo":{"elapsed":5238,"status":"ok","timestamp":1609783132836,"user":{"displayName":"Drew Richardson","photoUrl":"","userId":"12912776397844713423"},"user_tz":300},"outputId":"5c7a020f-15a0-4d13-922d-8d50d53b799e"},"source":["deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype=VideoType)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Analyzing all the videos in the directory...\n","Loading  /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day15-8W.mp4 and data.\n","Loading  /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day9-10W.mp4 and data.\n","Plots created! Please check the directory \"plot-poses\" within the video directory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pqaCw15v8EmB"},"source":["Now you can look at the plot-poses file and check the \"plot-likelihood.png\" might want to change the \"p-cutoff\" in the config.yaml file so that you have only high confidnece points plotted in the video. i.e. ~0.8 or 0.9. The current default is 0.4. "]},{"cell_type":"markdown","metadata":{"id":"pCrUvQIvoEKD"},"source":["## Create labeled video:\n","This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"6aDF7Q7KoEKE","outputId":"a3241f0f-87e8-4576-e25c-fa4e88a4e8f4"},"source":["deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype=VideoType)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Analyzing all the videos in the directory...\n","/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos  already exists!\n","Starting to process video: /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day15-8W.mp4\n","/content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos  already exists!\n","Loading /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day15-8W.mp4 and data.\n","Starting to process video: /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day9-10W.mp4\n","Loading /content/drive/My Drive/wirelessStim/wirelessStim-Drew-2021-01-01/videos/WS07-Day9-10W.mp4 and data.\n","Duration of video [s]: 600.63, recorded with 30.0 fps!\n","Overall # of frames: 18019 with cropped frame dimensions: 1080 1920\n","Duration of video [s]: 599.27, recorded with 30.0 fps!\n","Generating frames and creating video.\n","Overall # of frames: 17978 with cropped frame dimensions: 1080 1920\n","Generating frames and creating video.\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 17978/17978 [18:15<00:00, 16.42it/s]\n","100%|██████████| 18019/18019 [18:17<00:00, 16.42it/s]\n"],"name":"stderr"}]}]}